
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-8.5.8">
    
    
      
        <title>PN 2206.05826 Appx - Nanxi Gu</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.20d9efc8.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.815d1a91.min.css">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../paper.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#appendix" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Nanxi Gu" class="md-header__button md-logo" aria-label="Nanxi Gu" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Nanxi Gu
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              PN 2206.05826 Appx
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Nanxi Gu" class="md-nav__button md-logo" aria-label="Nanxi Gu" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Nanxi Gu
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        模型
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../Scholars/" class="md-nav__link">
        学者
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../Books/" class="md-nav__link">
        书籍
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/" class="md-nav__link">
        课程
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../Projects/" class="md-nav__link">
        项目
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#appendix" class="md-nav__link">
    Appendix
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-visualization" class="md-nav__link">
    A Visualization
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#b-tasks-and-dataset-descriptions" class="md-nav__link">
    B Tasks and dataset descriptions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#c-difference-between-inter-image-region-word-contrastive-loss-with-other" class="md-nav__link">
    C Difference between inter-image region-word contrastive loss with other
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#region-word-losses" class="md-nav__link">
    "region-word" losses
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#d-training-details-and-hyperparamters" class="md-nav__link">
    D Training details and hyperparamters
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#e-analysis-on-the-effect-of-different-language-encoders-and-pre-trained" class="md-nav__link">
    E Analysis on the effect of different language encoders and pre-trained
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weights" class="md-nav__link">
    weights
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#f-more-analysis-on-pre-training-data" class="md-nav__link">
    F More analysis on pre-training data
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#g-experiments-on-grounded-image-captioning" class="md-nav__link">
    G Experiments on grounded image captioning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#h-inference-speed" class="md-nav__link">
    H Inference speed
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-figure-reference" class="md-nav__link">
    I Figure Reference
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#j-all-results-for-odinw" class="md-nav__link">
    J All results for ODinW
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>PN 2206.05826 Appx</h1>

<h2 id="appendix">Appendix</h2>
<p>The appendix is organized as follows:</p>
<ul>
<li>
<p>In Section A, we provide more visualizations of our model’s predictions on various local-     ization and VL understanding tasks.</p>
</li>
<li>
<p>In Section B, we describe all our evaluated tasks and their dataset in detail.</p>
</li>
<li>
<p>In Section C, we discuss the difference between our additional inter-image region-word     contrastive loss and some other well-known losses that were also applied over a full batch in     multiple works.</p>
</li>
<li>
<p>In Section D, we introduce the training details and hyperparameters used in Section 4 in     the main paper.</p>
</li>
<li>
<p>Section E, we analyze the effect of using different language encoder and their pre-trained     weights in our models.</p>
</li>
<li>
<p>In Section F, we provide more results for all the checkpoints of adding pre-training data     (refer to Section 4 in the main paper).</p>
</li>
<li>
<p>In Section G, we provide a detailed analysis of the experiments of grounded captioning     (mentioned in Section 4 in the main paper).</p>
</li>
<li>
<p>In Section H, we give out a comparison for the model’s inference speed.</p>
</li>
<li>
<p>In Section I, we clearly provide the original sources of the images that are used in our paper.</p>
</li>
<li>
<p>In Section J, we present per-dataset results for all experiments in ODinW.</p>
</li>
</ul>
<p>Bike. Umbrella. Dog...</p>
<p>Agreenumbrella. Apinkstriped umbrella.Aplain whiteumbrella.</p>
<p>Whatistheleft girlholding? [MASK]</p>
<p>Apictureof [MASK]</p>
<p>VL Grounding</p>
<p>VisualQuestion Answering ImageCaption</p>
<p>Image Encoder TextEncoder</p>
<p>DeepFusionBlock</p>
<p>Object Detection</p>
<p>Localization tasks</p>
<p>Answer:umbrella</p>
<p>Car. Umbrella. Bike...</p>
<p>Instance Segmentation</p>
<p>Decode:girlsholding umbrellas.</p>
<p>textinput</p>
<p><cur>GLIPv2</cur> UnifiedOutputs</p>
<p>imageinput</p>
<p>Understanding tasks</p>
<p>Figure 4: <cur>GLIPv2</cur>, a pre-trained grounded VL understanding model, unifies various localization and VL understanding tasks. These two kinds of tasks mutually benefit each other and enable new capabilities such as language-guided detection/segmentation and grounded VQA/captioning.</p>
<h2 id="a-visualization">A Visualization</h2>
<p>We provide a clearer illustration of <cur>GLIPv2</cur> in Figure 4, which elegantly unifies various localization (object detection, instance segmentation) and VL understanding (phrase grounding, VQA and captioning) tasks. More visualizations of the predictions under various tasks from <cur>GLIPv2</cur> are also provided to indicate the model’s strength and capability. Please refer to Figure 5 for OD / Grounding, Figure 6 for Instance / Referring Image Segmentation, and Figure 7 for Grounded VL Understanding.</p>
<hr />
<p>Prompt: person. dog... backpack. umbrella. horse. toothbrush.</p>
<p>Prompt: person. hairdryer...baseball bat.baseballglove. bottle. toothbrush.</p>
<p>Prompt: person. cup.sink... microwave.refrigerator.bear.</p>
<p>Prompt: Mounted officersin bright green jacketssit on their horses wearing helmets.</p>
<p>Prompt: 2 couples are eating dinner on the floorbehind a large plant.</p>
<p>Prompt: A woman figure skaterin a blue costumeholds her legby the blade of her skate</p>
<p>Prompt: fish. jellyfish. penguin. puffin. shark. starfish. stingray</p>
<p>Prompt: dog. person. Prompt: smoke.</p>
<p>Figure 5: Visualization for OD / Grounding. Row 1: Object Detection on COCO. Row 2: Phrase Grounding on Flickr30K. Row 3: Object Detection on ODinW.</p>
<h2 id="b-tasks-and-dataset-descriptions">B Tasks and dataset descriptions</h2>
<p>B.1 (Language-guided) object detection and phrase grounding</p>
<p>COCO.[ 8 ] The Microsoft Common Objects in Context dataset is a medium-scale object detection dataset. It has about 900k bounding box annotations for 80 object categories, with about 7.3 annotations per image. It is one of the most used object detection datasets, and its images are often used within other datasets (including VG and LVIS).</p>
<p>Flickr30k-entities.[ 50 ] Given one or more phrases, which may be interrelated, the phrase grounding task is to provide a set of bounding boxes for each given phrase. We use the Flickr30k-entities dataset for this task, with the train/val/test splits as provided by [ 41 ] and evaluate our performance in terms of Recall. Flickr30K is included in the gold grounding data so we directly evaluate the models after pre-training as in MDETR [30]. We predict use any-box protocol specified in MDETR.</p>
<p>ODinW.We use 13 datasets from Roboflow^2. Roboflow hosts over 30 datasets, and we exclude datasets that are too challenging (e.g., detecting different kinds of chess pieces) or impossible to solve without specific domain knowledge (e.g., understanding sign language). We provide the details of the 13 datasets we use in Table 7. We include the PASCAL VOC 2012 dataset as a reference dataset, as public baselines have been established on this dataset. For PascalVOC, we follow the convention</p>
<p>(^2) <a href="https://public.roboflow.com/object-detection">https://public.roboflow.com/object-detection</a></p>
<hr />
<p>Prompt: person. hairdryer... baseballbat.baseballglove. bottle. toothbrush.</p>
<p>Prompt: person.chair. dining table ... potted plant. vase.</p>
<p>Prompt: person. cup.sink... microwave.refrigerator.bear.</p>
<p>Prompt: green bush Prompt: windowhasaframe Prompt: brownlampshade</p>
<p>Prompt: tissue.jacket....fork. pineapple.dinningtable.</p>
<p>Prompt: donut. wineglass ... banana. pineapple.</p>
<p>Prompt: person. teddybear... lollipop. flower.</p>
<p>Figure 6: Visualization for Instance / Referring Image Segmentation. Row 1: Instance Segmentation on COCO Mask. Row 2: Instance Segmentation on LVIS. Row 3: Referring Image Segmentation on PhraseCut.</p>
<p>Dataset Objects of Interest Train/Val/Test URL PascalVOC Common objects (PascalVOC 2012) 13690/3422/<a href="https://public.roboflow.com/object-detection/pascal-voc-2012">https://public.roboflow.com/object-detection/pascal-voc-2012</a> AerialDrone Boats, cars, etc. from drone images 52/15/7 <a href="https://public.roboflow.com/object-detection/aerial-maritime">https://public.roboflow.com/object-detection/aerial-maritime</a> Aquarium Penguins, starfish, etc. in an aquarium 448/127/63 <a href="https://public.roboflow.com/object-detection/aquarium">https://public.roboflow.com/object-detection/aquarium</a> Rabbits Cottontail rabbits 1980/19/10 <a href="https://public.roboflow.com/object-detection/cottontail-rabbits-video-dataset">https://public.roboflow.com/object-detection/cottontail-rabbits-video-dataset</a> EgoHands Hands in ego-centric images 3840/480/480 <a href="https://public.roboflow.com/object-detection/hands">https://public.roboflow.com/object-detection/hands</a> Mushrooms Two kinds of mushrooms 41/5/5 <a href="https://public.roboflow.com/object-detection/na-mushrooms">https://public.roboflow.com/object-detection/na-mushrooms</a> Packages Delivery packages 19/4/3 <a href="https://public.roboflow.com/object-detection/packages-dataset">https://public.roboflow.com/object-detection/packages-dataset</a> Raccoon Raccoon 150/29/17 <a href="https://public.roboflow.com/object-detection/raccoon">https://public.roboflow.com/object-detection/raccoon</a> Shellfish Shrimp, lobster, and crab 406/116/58 <a href="https://public.roboflow.com/object-detection/shellfish-openimages">https://public.roboflow.com/object-detection/shellfish-openimages</a> Vehicles Car, bus, motorcycle, truck, and ambulance 878/250/126 <a href="https://public.roboflow.com/object-detection/vehicles-openimages">https://public.roboflow.com/object-detection/vehicles-openimages</a> Pistols Pistol 2377/297/297 <a href="https://public.roboflow.com/object-detection/pistols/1">https://public.roboflow.com/object-detection/pistols/1</a> Pothole Potholes on the road 465/133/67 <a href="https://public.roboflow.com/object-detection/pothole">https://public.roboflow.com/object-detection/pothole</a> Thermal Dogs and people in thermal images 142/41/20 <a href="https://public.roboflow.com/object-detection/thermal-dogs-and-people">https://public.roboflow.com/object-detection/thermal-dogs-and-people</a></p>
<p>Table 7: 13 ODinW dataset statistics. We summarize the objects of interest for each dataset and report the image number of each split.</p>
<p>and report on the validation set. For Pistols, there are no official validation or test sets so we split the dataset ourselves.</p>
<p>B.2 (Language-guided) instance segmentation and referring image segmentation</p>
<p>LVIS.[ 23 ] The Large Vocabulary Instance Segmentation dataset has over a thousand object categories, following a long-tail distribution with some categories having only a few examples. Similar to VG, LVIS uses the same images as in COCO, re-annotated with more object categories. In contrast to COCO, LVIS is a federated dataset, which means that only a subset of categories is annotated in each image. Annotations, therefore, include positive and negative object labels for objects that are present</p>
<hr />
<p>Figure 7: Visualization for Grounded VL Understanding. Row 1: Grounded VQA predictions (The model is given the input question and a placeholder token “[MASK]” for the answer. The model can ground not only entities in the question but also the implied answer entity). Row 2: Grounded captioning on COCO (The model can generate high-quality captions and, in the meantime, provide localization results.</p>
<p>and categories that are not present, respectively. In addition, LVIS categories are not pairwise disjoint, such that the same object can belong to several categories.</p>
<p>PhraseCut.[ 63 ] Besides object detection, we show that our <cur>GLIPv2</cur> can be extended to perform segmentation by evaluating the referring expression segmentation task of the recent PhraseCut[ 63 ] which consists of images from VG, annotated with segmentation masks for each referring expression. These expressions comprise a wide vocabulary of objects, attributes and relations, making it a challenging benchmark. Contrary to other referring expression segmentation datasets, in PhraseCut the expression may refer to several objects and the model is expected to find all the corresponding instances.</p>
<p>B.3 VQA and image captioning</p>
<p>VQA.[ 20 ] requires the model to predict an answer given an image and a question. We conduct experiments on the VQA2.0 dataset, which is constructed using images from COCO. It contains 83k images for training, 41k for validation, and 81k for testing. We treat VQA as a classification problem with an answer set of 3,129 candidates following the common practice of this task. For our best models, we report test-dev and test-std scores by submitting to the official evaluation server.^3</p>
<p>COCO image captioning.[ 11 ] The goal of image captioning is to generate a natural language description given an input image. We evaluate <cur>GLIPv2</cur> on COCO Captioning dataset and report BLEU-4, CIDEr, and SPICE scores on the Karparthy test split.</p>
<h2 id="c-difference-between-inter-image-region-word-contrastive-loss-with-other">C Difference between inter-image region-word contrastive loss with other</h2>
<h2 id="region-word-losses">"region-word" losses</h2>
<p>As far as we know, up to the deadline (05/19/2022) for NeurIPS submission, there are only three published papers (VILD [ 21 ], RegionCLIP [ 72 ], and X-VLM [ 69 ]) that have the flavor of "region</p>
<p>(^3) <a href="https://eval.ai/challenge/830/overview">https://eval.ai/challenge/830/overview</a></p>
<hr />
<p>Model Image Text Detection Pre-Train Data Grounding Caption <cur>GLIPv2</cur>-T Swin-T BERT-Base O365 GoldG (no COCO) Cap4M <cur>GLIPv2</cur>-B Swin-B CLIP O365, COCO, OpenImages, VG, ImageNetBoxes GoldG CC15M+ SBU <cur>GLIPv2</cur>-H CoSwin-H [67] CLIP O365, COCO, OpenImages, VG, ImageNetBoxes GoldG CC15M+SBU Mask Head – – LVIS, COCO PhraseCut –</p>
<p>Table 8: A detailed list of <cur>GLIPv2</cur> model variants</p>
<p>word" loss applied over full batch. We discuss the difference between our work and the three aforementioned works in the following:</p>
<ol>
<li>
<p>All these three works use “region-sentence" loss, i.e., the similarity between a region feature     and the [CLS] token of a sentence, instead of true "region-word" loss used in <cur>GLIPv2</cur>. As a     result, none of these three works made use of the phrase grounding data, which may contain     multiple entities in one sentence during their training. It is the most important point in     <cur>GLIPv2</cur> to use phrase grounding data and pseudo grounding data to train a unified grounded     VL understanding model.</p>
</li>
<li>
<p><cur>GLIPv2</cur> has carefully designed the positive label propagation in our inter-image region-word     contrastive loss to mitigate the wrong assumption that "every unpaired region-word pair is     negative". As far as we know, no previous work has mentioned this mechanism of positive     label propagation before.</p>
</li>
<li>
<p>There are some other differences. For example, in VILD, its “region-sentence loss" is     actually not a contrastive loss over full-batch but a classification loss over a fixed vocabulary     per sample (see the definition ofLV iLD−text).</p>
</li>
</ol>
<p>Upon all three points above, we believe that our inter-image region-word contrastive loss is novel and has a significant difference from previous works.</p>
<h2 id="d-training-details-and-hyperparamters">D Training details and hyperparamters</h2>
<p>D.1 Pre-training</p>
<p>Pre-training data. There are three different types of data in pre-training 1) detection data 2) grounding data 3) caption data, as shown in Table 8. The detection data includes Object365 [ 54 ], COCO [ 8 ], OpenImages [ 33 ], Visual Genome [ 34 ], and ImageNetBoxes [ 16 ]. The grounding data includes GoldG, 0.8M human-annotated gold grounding data curated by MDETR [ 30 ] combining Flick30K, VG Caption, and GQA [ 29 ]. The Cap4M is a 4M image-text pairs collected from the web with boxes generated by GLIP-T(C) in [41], and CC (Conceptual Captions) + SBU (with 1M data).</p>
<p>Implementation details. In Section 4 in the main paper, we introduced <cur>GLIPv2</cur>-T, <cur>GLIPv2</cur>-B, <cur>GLIPv2</cur>-H, and we introduce the implementation details in the following.</p>
<p>We pre-train <cur>GLIPv2</cur>-T based on Swin-Tiny models with 32 GPUs and a batch size of 64. We use a base learning rate of 1 × 10 −^5 for the language backbone (BERT-Base) and 1 × 10 −^4 for all other parameters. The learning rate is stepped down by a factor of 0.1 at the 67% and 89% of the total 330,000 training steps. We decay the learning rate when the zero-shot performance on COCO saturates. The max input length is 256 tokens for all models. To optimize the results for object detection, we continue pre-training without the MLM loss for another 300,000 steps.</p>
<p>We pre-train <cur>GLIPv2</cur>-B based on Swin-Base models with 64 GPUs and a batch size of 64. We use a base learning rate of 1 × 10 −^4 for all parameters, including the language backbone (CLIP-type pre-layernorm transformer). The learning rate is stepped down by a factor of 0.1 at the 67% and 89% of the total 1 million training steps. We decay the learning rate when the zero-shot performance on COCO saturates. The max input length is 256 tokens for all models. To optimize the results for object detection, we continue pre-training without the MLM loss for another 500,000 steps.</p>
<p>We pre-train <cur>GLIPv2</cur>-H based on the CoSwin-Huge model from Florence [ 67 ] with 64 GPUs and a batch size of 64. We use a base learning rate of 1 × 10 −^4 for all parameters, including the language backbone (CLIP-type pre-layernorm transformer). The learning rate is stepped down by a factor of</p>
<hr />
<p>...DyHead Module</p>
<p>Fusion</p>
<p>BERTLayer</p>
<p>Image Encoder</p>
<p>Text Encoder</p>
<p>DyHead Module</p>
<p>Fusion</p>
<p>LayerBERT ...</p>
<p>MaskHead</p>
<p>prompt</p>
<p>image</p>
<p>GroundingHead</p>
<p>MLM</p>
<p>(0)Pre-training</p>
<p>...DyHead Module</p>
<p>Fusion</p>
<p>LayerBERT</p>
<p>Image Encoder</p>
<p>EncoderText</p>
<p>DyHead Module</p>
<p>Fusion</p>
<p>prompt BERTLayer ...</p>
<p>image</p>
<p>GroundingHead</p>
<p>(i)OD/Grounding</p>
<p>...DyHead Module</p>
<p>Fusion</p>
<p>BERTLayer</p>
<p>EncoderImage</p>
<p>EncoderText</p>
<p>DyHead Module</p>
<p>Fusion</p>
<p>BERTLayer ...</p>
<p>MaskHead</p>
<p>prompt</p>
<p>image</p>
<p>GroundingHead</p>
<p>(ii)Instance/Referring ImageSegmentation</p>
<p>...DyHead Module</p>
<p>Fusion</p>
<p>LayerBERT</p>
<p>EncoderImage</p>
<p>EncoderText</p>
<p>DyHead Module</p>
<p>Fusion</p>
<p>prompt BERTLayer ...</p>
<p>image</p>
<p>GroundingHead</p>
<p>DecodeAnswerr</p>
<p>(iii)GroundedVisual QuestionAnswering</p>
<p>...DyHead Module</p>
<p>Fusion</p>
<p>BERTLayer</p>
<p>EncoderImage</p>
<p>Text Encoder</p>
<p>DyHead Module</p>
<p>Fusion</p>
<p>LayerBERT ...</p>
<p>image</p>
<p>GroundingHead</p>
<p>(iv)GroundedImageCaptioning (textencoderisuni-directional)</p>
<p>‘a pictureof’ CaptionHead</p>
<p>Figure 8: The model architecture for pre-training (0), and downstream tasks (i) OD / Grounding (ii) Instance / Referring Image Segmentation (iii) Grounded Visual Question Answering (iv) Grounded Image Captioning.</p>
<p>0.1 at the 67% and 89% of the total 1 million training steps. We decay the learning rate when the zero-shot performance on COCO saturates. The max input length is 256 tokens for all models. We found that there isnoneed to continue pre-training without MLM loss for the huge model.</p>
<p>Mask heads of <cur>GLIPv2</cur>-T, <cur>GLIPv2</cur>-B and <cur>GLIPv2</cur>-H are pre-trained COCO, LVIS and PhraseCut, while freezing all the other model parameters. This mask head pre-training uses batch size 64, and goes through COCO for 24 epochs, LVIS for 24 epochs, and PhraseCut for 8 epochs, respectively. <cur>GLIPv2</cur> uses Hourglass network [ 49 ] as instance segmentation head feature extractor, and utilizes the "classification-to-matching" trick to change the instance segmentation head linear prediction layer (outputsK-dimensional logits on each pixel) to a dot product layer between pixel visual features and the word features after VL fusion. <cur>GLIPv2</cur>-T and <cur>GLIPv2</cur>-B use a very basic Hourglass network for segmentation head feature extractor: only 1 scale and 1 layer, with hidden dimension 256. <cur>GLIPv2</cur>-H uses a larger Hourglass network for segmentation head feature extractor: 2 scales and 4 layers, with hidden dimension 384.</p>
<p>D.2 Downstream tasks</p>
<p>OD / Grounding.When fine-tuning on COCO, we use a base learning rate of 1 × 10 −^5 and 24 training epochs for the pre-trained <cur>GLIPv2</cur>-T model, and a base learning rate of 5 × 10 −^6 and 5 training epochs for the pre-trained <cur>GLIPv2</cur>-B and <cur>GLIPv2</cur>-H models.</p>
<p>For direct evaluation on LVIS, since LVIS has over 1,200 categories and they cannot be fit into one text prompt, so we segment them into multiple chunks, fitting 40 categories into one prompt and query the model multiple times with the different prompts. We find that models tend to overfit on LVIS during the course of pre-training so we closely monitor the performance on minival for all models and report the results with the best checkpoints in Table 2 in the main paper.</p>
<p>For direct evaluation on Flickr30K, models may also overfit during the course of pre-training so we monitor the performance on the validation set for all models and report the results with the best checkpoints in Table 2 in the main paper.</p>
<p>Instance segmentation / Referring Image Segmentation.Given the pre-trained model with pretrained mask head, we simply fine-tune theentirenetwork to get the task-specific fine-tuned models.</p>
<p>For fine-tuning on COCO instance segmentation, we use a base learning rate of 1 × 10 −^5 and 24 training epochs for the pre-trained <cur>GLIPv2</cur>-T model, and a base learning rate of 5 × 10 −^6 and 5 training epochs for the pre-trained <cur>GLIPv2</cur>-B and <cur>GLIPv2</cur>-H models.</p>
<p>For fine-tuning on LVIS instance segmentation, we use a base learning rate of 1 × 10 −^5 and 24 training epochs for the pre-trained <cur>GLIPv2</cur>-T model, and a base learning rate of 5 × 10 −^6 and 5 training epochs for the pre-trained <cur>GLIPv2</cur>-B and <cur>GLIPv2</cur>-H models.</p>
<hr />
<p>For fine-tuning on PhraseCut Referring Image segmentation, we use a base learning rate of 1 × 10 −^5 and 12 training epochs for the pre-trained <cur>GLIPv2</cur>-T model, and a base learning rate of 5 × 10 −^6 and 3 training epochs for the pre-trained <cur>GLIPv2</cur>-B and <cur>GLIPv2</cur>-H models.</p>
<p>(Grounded) VQA.To fine-tune <cur>GLIPv2</cur> for VQA, we feed the image and question into the model and then take the output feature sequencePfrom the language side (after the VL fusion) and apply a ‘attention pooling’ layer to obtain a feature vectorPvqa. More specifically, the attention pooling layer applies a linear layer followed by softmax to obtain normalized scaler weights, and then these weights are used to compute a weighted sum to produce the feature vectorpvqa. This feature vector is then fed to a 2-layer MLP with GeLU activation [ 27 ] and a final linear layer to obtain the logits for the 3129-way classification.^4 Following standard practice [ 58 ], we use binary cross entropy loss to take account of different answers from multiple human annotators. Following VinVL [ 71 ], we train on the combination of train2014 + val2014 splits of the VQAv2 dataset, except for the reserved 2k dev split.^5. For the ablation studies we report the accuracy on this 2k dev split.</p>
<p>Other than the conventional VQA setting, we also experimented a new ‘grounded VQA’ setup, which the model is required to not only predict the answer, but also ground the objects (predict bounding boxes in the image) mentioned in the question and answer text, see Figure 8(iii). Note that the language input is the question appended by a[MASK]token, and this[MASK]token should ground to the object if the answer is indeed an object in the image. The total training loss is summing the grounding loss (intra-image region-word contrastive loss) and the VQA loss described previously.</p>
<p>(Grounded) Image Captioning.We fine-tune the pre-trained model on COCO Caption “Karpathy” training split. The training objective is uni-directional Language Modeling (LM), which maximizes the likelihood of the next word at each position given the image and the text sequence before it. To enable autoregressive generation, we use uni-directional attention mask for the text part, and prevent the image part from attending to the text part in the fusion layers. Although the training objective (LM) is different from that in pre-training (i.e., bi-directional MLM), we directly fine-tune the model for image captioning to evaluate its capability of generalizing to VL generation tasks. Our model is trained with cross entropy loss only, without using CIDEr optimization.</p>
<p>For grounded image captioning (Figure 8), we add the grounding loss (intra-image region-word contrastive loss) in training, which is calculated in the same way as in pre-training. We use Flickr30K training split for this task. During inference, for each predicted text token, we get its dot product logits with all the region representations and choose the maximum as the associated bounding box.</p>
<h2 id="e-analysis-on-the-effect-of-different-language-encoders-and-pre-trained">E Analysis on the effect of different language encoders and pre-trained</h2>
<h2 id="weights">weights</h2>
<p>For <cur>GLIPv2</cur>-T, we use the ImageNet pre-trained Swin-Transformer to initialize the image encoder and BERT-base-uncased to initialize the language encoder. For <cur>GLIPv2</cur>-B, we use the pre-trained paired image-language encoder from UniCL (CLIP-like pre-training,<a href="https://github.com/microsoft/">https://github.com/microsoft/</a> UniCL) for initialization. We did an ablation study on the different language encoders (UniCL vs. BERT) and found that their results are nearly the same, as shown in Figure 9. Therefore, UniCL initialization does not skew the good localization performance. The main reason for us to keep the UniCL(CLIP-like) language encoder is due to its Pre-LayerNorm [ 64 ] operation. We find the UniCL(CLIP-like) language encoder with Pre-LayerNorm is more stable during the training compared with BERT, which uses Post-LayerNorm.</p>
<h2 id="f-more-analysis-on-pre-training-data">F More analysis on pre-training data</h2>
<p>Table 5 in the main paper reports the last checkpoint results on <cur>GLIPv2</cur> when we do the scaling up of pre-training data. As more weak image-text pair data (Cap) is involved in our training, it benefits both standard/in-domain (i.e., COCO, Flickr30K) and large-domain gap (i.e., ODinW, LVIS) tasks. Further adding the inter-image region-word contrastive helps when we are fixing the data at the same scale. For large-domain gap tasks, adding the inter-image region-word contrastive loss will further</p>
<p>(^4) We experimented simpler pooling methods such as average pooling and[CLS]pooling [ 17 ] in the early experiments and found the attention pooling described above works better. (^5) 2000 images sampled from the val2014 split (and their corresponding question-answer pairs).</p>
<hr />
<p>Figure 9: GLIP-B with image encoder initialized from UniCL pre-trained image encoder, but with different language encoder initialization. Blue: language encoder initialized by Bert-Base, thus un-paired image-language pre-trained encoders. Yellow: language encoder initialized from UniCL pre-trained language encoder, thus paired UniCL pre-trained image-language encoders. From the results, we can see that the COCO zero-shot transfer results from two initializations are nearly the same. Similar results have been observed for other metrics, i.e., LVIS zero-shot AP, ODinW benchmark, and Flickr30k grounding performance.</p>
<p>boost the model to learn better representation. To learn more scaling-up effects on various tasks under all the checkpoints for GLIP and <cur>GLIPv2</cur>, see Figure 10. Given the considerable amount of improvement of <cur>GLIPv2</cur> when the number of caption data increases from 0M to 12M, we hypothesize that it has potential to further grow by training on even larger-scale web image-text pairs.</p>
<h2 id="g-experiments-on-grounded-image-captioning">G Experiments on grounded image captioning</h2>
<p>The grounded captioning task requires the model to generate an image caption and also ground predicted phrases to object regions. The final predictions consist of (1) the text captions (2) predicted object regions, and (3) the grounding correspondence between the phrases and regions. Following the established benchmarks [ 48 , 74 ], we evaluate the caption metrics on COCO Captions and report the grounding metrics on Flick30K, as shown in Table 9.</p>
<p>Model B@4COCO CaptionCIDEr SPICE R@1Flickr30K GroundingR@5 R@10</p>
<p>No Pretrain 35.4 115.3 21.2 77.0 92.9 95.7 +Lmlm 33.4 107.6 19.9 70.9 90.0 93.2 +Lloc+Lintra+Linter 36.6 120.3 21.6 80.8 94.9 96.7 <cur>GLIPv2</cur>-T 36.5 119.8 21.6 80.8 94.4 96.5 <cur>GLIPv2</cur>-B 37.4 123.0 21.9 81.0 94.5 96.5</p>
<p>Table 9: Grounded image captioning results on the COCO Caption, and Flickr30K Entities. We report BLEU@4, CIDer, and SPICE metrics for caption evaluation, and we use R@1, R@5, R@10 for grounding evaluation.</p>
<h2 id="h-inference-speed">H Inference speed</h2>
<p>We test the inference speed for <cur>GLIPv2</cur> on V100 with batch size 1 and show its comparison to MDETR, as shown in Table 10.</p>
<hr />
<p>+0M +Cap4M +Cap1 2 M +Cap48M</p>
<p>+0M +Cap 4 M +Cap12M</p>
<p>+0M +Cap4M +Cap1 2 M +Cap48M</p>
<p>+0M +Cap 4 M +Cap12M</p>
<p>+0M +Cap4M +Cap1 2 M +Cap48M</p>
<p>+0M +Cap 4 M +Cap12M</p>
<p>+0M +Cap4M +Cap1 2 M +Cap48M</p>
<p>+0M +Cap 4 M +Cap12M</p>
<p>Figure 10: Pre-train data scale up on Base-scale model. Left: GLIP, Right: <cur>GLIPv2</cur>; Row 1: COCO minival, Row 2: ODinW test split, Row 3: LVIS minival, Row 4: Flick30K test.</p>
<h2 id="i-figure-reference">I Figure Reference</h2>
<p>We provided the original sources of the images that are used in our paper in the following. All datasets above were collected by the creators (cited) and consent for any personally identifiable information (PII) was ascertained by the authors where necessary.</p>
<p>Figure 1 in the main paper The top left and the bottom middle figures are the 281759.jpg in COCO val set; The left right images are (from top to down: (1) 2588.jpg in ODinW Aquarium test set. (2) 13923.jpg in LVIS val set. (3) 132690.jpg in VQA2.0 val set (question id is 132690002). (4) 462565.jpg in COCO Caption val set.</p>
<hr />
<p>Model Object Detection (COCO) Phrase Grounding (Flick30K) Referring Expression Segmentation (PhraseCut) MDETR R101 [30] – 9.31 3.80 MDETR EffB3 [30] – 11.20 3.98 MDETR EffB5 [30] – 9.15 – <cur>GLIPv2</cur>-T 4.12 3.74 2.26 <cur>GLIPv2</cur>-B 3.01 3.23 2.39 <cur>GLIPv2</cur>-H 1.21 1.13 0.89</p>
<p>Table 10: Model inference speed on various tasks. We report FPS, which is the number of images processed per second per GPU (higher is better).</p>
<p>Figure 2 in the main paper The top left figure is the 209297.jpg in COCO train set; The bottom left figure is the 9378.jpg in COCO val set.</p>
<p>Figure 4 in the Appendix Same as Figure 1. The top left and the bottom middle figures are the 281759.jpg in COCO val set.</p>
<p>Figure 5 in the Appendix Row 1 (from left to right): (1) 439715.jpg in COCO val set. (2) 6471.jpg in COCO val set. (3) 13923.jpg in COCO val set; Row 2: (1) 5521996.jpg in Flickr30K val set. (2) 764507.jpg in Flickr30K val set. (3) 7520721.jpg in Flick30K val set; Row 3: (1) 2588.jpg in ODinW Aquarium test set. (2) 143.jpg in Thermal val set. (3) ck0l9j6n6oqjo0848ps5blk3b.jpg in WildFire val set.</p>
<p>Figure 6 in the Appendix Row 1 (from left to right): (1) 13923.jpg in COCO val set. (2) 6471.jpg in COCO val set. (3) 7574.jpg in COCO val set; Row 2: (1) 117320.jpg in LVIS val set. (2) 2587.jpg in LVIS val set. (3) 211120.jpg in LVIS val set; Row 3: (1) 4744.jpg in PhraseCut test set. (2) 4744.jpg in PhraseCut val set. (3) 567.jpg in PhraseCut train set.</p>
<p>Figure 7 in the Appendix Row 1 (from left to right): (1) 486.jpg in VQA2.0 val set (question id is 486002). (2) 262746.jpg in VQA2.0 val set (question id is 262746002). (3) 132690.jpg in VQA2.0 val set (question id is 132690002); Row 2: (1) 391895.jpg in COCO Caption val set. (2) 462565.jpg in COCO Caption val set. (3) 579056.jpg in COCO Caption val set.</p>
<h2 id="j-all-results-for-odinw">J All results for ODinW</h2>
<p>We report the per-dataset performance under 0,1,3,5,10-shot and full data as well as prompt tuning, and full-model tuning in Table 11 and Table 12 (on the next page).</p>
<p>Model PascalVOC AerialDrone Aquarium Rabbits EgoHands Mushrooms Packages Raccoon Shellfish Vehicles Pistols Pothole Thermal Avg GLIP-T 56.2 12.5 18.4 70.2 50.0 73.8 72.3 57.8 26.3 56.0 49.6 17.7 44.1 46.5 GLIP-L 61.7 7.1 26.9 75.0 45.5 49.0 62.8 63.3 68.9 57.3 68.6 25.7 66.0 52.1 <cur>GLIPv2</cur>-T 57.6 10.5 18.4 71.4 52.7 77.7 67.7 58.8 27.8 55.6 60.1 20.0 52.4 48.5 <cur>GLIPv2</cur>-B 62.8 8.6 18.9 73.7 50.3 83.0 68.6 61.6 56.0 53.8 67.8 32.6 53.8 54.2 <cur>GLIPv2</cur>-H 66.3 10.9 30.4 74.6 55.1 52.1 71.3 63.8 66.2 57.2 66.4 33.8 73.3 55.5</p>
<p>Table 11: Zero-shot performance on 13 ODinW datasets.</p>
<hr />
<p>Model Shot Tune PascalVOC AerialDrone Aquarium Rabbits EgoHands Mushrooms PackagesRaccoon Shellfish Vehicles Pistols Pothole Thermal Avg DyHeadO365 1 Full 25.8±3.0 16.5±1.8 15.9±2.7 55.7±6.044.0±3.6 66.9±3.9 54.2±5.7 50.7±7.714.1±3.633.0±11.0 11.0±6.58.2±4.1 43.2±10.033.8±3.5 DyHeadO365 3 Full 40.4±1.0 20.5±4.0 26.5±1.3 57.9±2.053.9±2.5 76.5±2.3 62.6±13.352.5±5.022.4±1.747.4±2.0 30.1±6.919.7±1.557.0±2.3 43.6±1.0 DyHeadO365 5 Full 43.5±1.0 25.3±1.8 35.8±0.5 63.0±1.056.2±3.9 76.8±5.9 62.5±8.7 46.6±3.128.8±2.251.2±2.2 38.7±4.121.0±1.453.4±5.2 46.4±1.1 DyHeadDyHeadO365O365 All 10 FullFull 46.653.3±0.3 29.028.4±2.8 41.749.5±1.0 65.273.5±2.562.577.9±0.8 85.484.0±2.2 67.969.2±4.5 47.956.2±2.228.643.6±5.053.859.2±1.0 39.268.9±4.927.953.7±2.364.173.7±2.6 50.860.8±1.3 GLIP-T 1 Prompt 54.4±0.9 15.2±1.4 32.5±1.0 68.0±3.260.0±0.7 75.8±1.2 72.3±0.0 54.5±3.924.1±3.059.2±0.9 57.4±0.618.9±1.856.9±2.7 49.9±0.6 GLIP-TGLIP-T 35 PromptPrompt 56.858.5±±0.80.5 18.918.2±±3.60.1 37.641.0±±1.61.2 72.471.8±±0.52.462.865.7±±1.30.7 85.487.5±±2.82.2 64.572.3±±4.60.0 69.160.6±±1.82.222.031.4±±0.94.262.761.0±±1.11.8 56.154.4±±0.60.625.932.6±±0.71.463.866.3±±4.82.8 53.755.5±±1.30.5 GLIP-T 10 Prompt 59.7±0.7 19.8±1.6 44.8±0.9 72.1±2.065.9±0.6 87.4±1.1 72.3±0.0 57.5±1.230.0±1.462.1±1.4 57.8±0.933.5±0.173.1±1.4 56.6±0.2 GLIP-T All Prompt 66.4 27.6 50.9 70.6 73.3 88.1 67.7 64.0 40.3 65.4 68.3 50.7 78.5 62.4 GLIP-T 1 Full 54.8±2.0 18.4±1.0 33.8±1.1 70.1±2.964.2±1.8 83.7±3.0 70.8±2.1 56.2±1.822.9±0.256.6±0.5 59.9±0.418.9±1.354.5±2.7 51.1±0.1 GLIP-T 3 Full 58.1±0.5 22.9±1.3 40.8±0.9 65.7±1.666.0±0.2 84.7±0.5 65.7±2.8 62.6±1.427.2±2.761.9±1.8 60.7±0.227.1±1.270.4±2.5 54.9±0.2 GLIP-T 5 Full 59.5±0.4 23.8±0.9 43.6±1.4 68.7±1.366.1±0.6 85.4±0.4 72.3±0.0 62.1±2.027.3±1.261.0±1.8 62.7±1.634.5±0.566.6±2.3 56.4±0.4 GLIP-TGLIP-T All 10 FullFull 59.162.3±1.3 26.331.2±1.1 46.352.5±1.6 67.370.8±1.567.178.7±0.7 87.888.1±0.5 72.375.6±0.0 57.761.4±1.734.651.4±1.765.465.3±1.4 61.671.2±1.039.358.7±1.074.776.7±2.3 58.464.9±0.2 GLIP-L 1 Prompt 62.8±0.4 18.0±1.8 37.4±0.3 71.9±2.468.9±0.1 81.8±3.4 65.0±2.8 63.9±0.470.2±1.267.0±0.4 69.3±0.127.6±0.469.8±0.6 59.5±0.4 GLIP-L 3 Prompt 65.0±0.5 21.4±1.0 43.6±1.1 72.9±0.770.4±0.1 91.4±0.7 57.7±3.7 70.7±1.169.7±0.962.6±0.8 67.7±0.436.2±1.168.8±1.5 61.4±0.3 GLIP-LGLIP-L 105 PromptPrompt 65.665.9±±0.30.2 19.923.4±±1.62.6 47.750.3±±0.70.7 73.773.6±±0.70.770.671.8±±0.30.3 86.886.5±±0.50.3 64.670.5±±0.71.1 69.469.0±±3.30.568.069.4±±1.32.467.870.8±±1.51.2 68.368.8±±0.30.636.639.3±±1.60.971.974.9±±0.62.1 62.464.2±±0.50.4 GLIP-L All Prompt 72.9 23.0 51.8 72.0 75.8 88.1 75.2 69.5 73.6 72.1 73.7 53.5 81.4 67.9±0.0 GLIP-L 1 Full 64.8±0.6 18.7±0.6 39.5±1.2 70.0±1.570.5±0.2 69.8±18.0 70.6±4.0 68.4±1.271.0±1.365.4±1.1 68.1±0.228.9±2.972.9±4.7 59.9±1.4 GLIP-L 3 Full 65.6±0.6 22.3±1.1 45.2±0.4 72.3±1.470.4±0.4 81.6±13.3 71.8±0.3 65.3±1.667.6±1.066.7±0.9 68.1±0.337.0±1.973.1±3.3 62.1±0.7 GLIP-L 5 Full 66.6±0.4 26.4±2.5 49.5±1.1 70.7±0.271.9±0.2 88.1±0.0 71.1±0.6 68.8±1.268.5±1.770.0±0.9 68.3±0.539.9±1.475.2±2.7 64.2±0.3 GLIP-LGLIP-L All 10 FullFull 66.469.6±0.7 32.032.6±1.4 52.356.6±1.1 70.676.4±0.772.479.4±0.3 88.188.1±0.0 67.167.1±3.6 64.769.4±3.169.465.8±1.471.571.6±0.8 68.475.7±0.744.360.3±0.676.383.1±1.1 64.968.9±0.7 <cur>GLIPv2</cur>-T 1 Prompt 51.2±0.3 17.7±1.2 34.2±0.1 68.7±1.267.3±0.9 83.7±2.1 68.1±1.7 53.4±0.230.0±0.959.0±0.1 60.0±0.321.9±0.266.5±0.7 52.4±0.5 <cur>GLIPv2</cur>-T 3 Prompt 66.6±0.2 11.5±0.7 37.2±1.0 71.7±0.370.1±0.4 45.7±0.1 57.7±1.2 69.7±1.542.7±0.467.5±0.9 65.6±1.036.7±1.269.2±1.2 55.6±0.4 <cur>GLIPv2</cur>-T<cur>GLIPv2</cur>-T 105 PromptPrompt 58.959.9±±1.20.4 17.421.6±±0.62.0 42.843.7±±0.40.3 72.674.3±±0.50.466.168.2±±0.20.7 84.988.1±±0.80.1 69.772.0±±0.60.9 65.560.0±±2.10.435.635.6±±0.81.262.866.1±±0.90.6 59.861.0±±0.20.335.542.8±±0.90.474.470.9±±0.23.2 57.458.8±±0.40.5 <cur>GLIPv2</cur>-T All Prompt 67.4 22.3 50.5 74.3 73.4 85.5 74.7 65.8 53.7 67.4 68.9 52.3 83.7 64.8±0.0 <cur>GLIPv2</cur>-T 1 Full 64.8±0.6 18.7±0.6 39.5±1.2 70.0±1.570.5±0.2 69.8±18.0 70.6±4.0 68.4±1.271.0±1.365.4±1.1 68.1±0.228.9±2.972.9±4.7 52.8±1.4 <cur>GLIPv2</cur>-T 3 Full 53.9±0.1 17.8±0.7 42.7±1.1 73.1±1.065.9±0.2 84.7±3.4 69.7±0.8 60.7±1.328.8±0.861.7±1.3 60.6±0.235.5±0.468.3±1.7 55.6±0.7 <cur>GLIPv2</cur>-T 5 Full 58.9±0.2 17.4±1.1 42.8±1.3 72.6±0.766.1±0.6 84.9±0.9 69.7±0.3 65.5±1.035.6±0.962.8±0.3 59.8±0.235.5±1.274.4±2.1 57.4±0.4 <cur>GLIPv2</cur>-T<cur>GLIPv2</cur>-T All 10 FullFull 57.666.4±1.0 27.630.2±1.2 49.152.5±1.0 70.474.8±0.569.280.0±0.2 88.188.1±0.0 73.174.3±2.3 58.063.7±2.842.954.4±1.264.863.0±0.2 62.173.0±0.939.960.1±0.471.683.5±0.8 59.766.5±0.3 <cur>GLIPv2</cur>-B 1 Prompt 68.7±0.1 19.9±0.3 38.4±0.8 68.5±1.068.6±0.8 87.7±3.0 69.3±1.7 68.5±0.455.2±0.365.7±0.7 67.2±0.134.8±0.869.6±0.4 60.4±0.3 <cur>GLIPv2</cur>-B 3 Prompt 67.2±0.6 22.2±0.3 46.5±0.9 71.2±0.870.9±0.1 86.9±0.2 67.7±1.8 63.7±2.346.9±0.868.1±0.4 67.4±0.947.9±1.078.9±1.7 62.0±0.5 <cur>GLIPv2</cur>-B<cur>GLIPv2</cur>-B 105 PromptPrompt 68.969.4±±1.00.7 25.721.8±±0.41.3 50.548.7±±0.90.2 73.871.3±±1.50.269.771.0±±0.60.7 84.988.1±±0.30.4 69.368.6±±0.70.7 65.873.5±±1.60.365.761.5±±1.01.969.269.3±±0.30.2 67.568.6±±0.70.734.041.3±±0.20.273.175.2±±0.61.3 62.963.8±±0.40.3 <cur>GLIPv2</cur>-B All Prompt 71.9 26.1 50.6 74.5 73.5 86.9 74.9 71.0 71.6 71.0 72.4 50.2 80.5 67.3±0.0 <cur>GLIPv2</cur>-B 1 Full 67.8±0.4 18.7±0.3 44.2±0.9 71.4±0.370.4±1.2 87.9±7.3 66.1±2.4 68.9±1.160.6±1.668.1±0.6 69.0±0.735.1±0.968.9±2.1 61.2±0.6 <cur>GLIPv2</cur>-B 3 Full 68.1±0.2 25.7±0.4 46.4±1.6 69.8±1.371.3±1.2 88.0±3.4 68.6±0.9 69.8±1.760.1±0.368.4±1.9 68.5±0.639.8±0.871.4±2.1 62.8±0.8 <cur>GLIPv2</cur>-B 5 Full 68.6±1.0 21.6±0.6 46.7±0.7 70.9±0.971.0±1.2 88.1±3.7 69.1±0.2 71.8±1.061.5±0.768.7±0.2 69.3±0.840.2±1.074.8±2.8 63.3±0.6 <cur>GLIPv2</cur>-B<cur>GLIPv2</cur>-B All 10 FullFull 67.471.1±1.3 22.332.6±1.1 50.557.5±0.7 74.373.6±0.473.480.0±0.4 85.588.1±0.1 74.774.9±0.9 65.868.2±2.453.770.6±1.167.471.2±0.9 68.976.5±0.752.358.7±0.683.779.6±3.2 64.669.4±0.3 <cur>GLIPv2</cur>-H 1 Prompt 68.3±0.6 16.4±0.6 45.8±0.3 72.0±0.567.9±0.9 89.3±3.2 69.3±1.7 67.9±0.866.3±1.968.0±0.7 66.8±0.333.9±0.470.7±1.5 61.4±0.5 <cur>GLIPv2</cur>-H 3 Prompt 69.5±0.7 25.9±0.2 50.0±1.2 75.4±1.470.1±0.9 85.9±2.5 69.3±0.7 70.8±1.266.4±0.868.0±1.2 68.8±0.934.0±0.372.7±1.6 63.6±0.6 <cur>GLIPv2</cur>-H<cur>GLIPv2</cur>-H 105 PromptPrompt 69.466.0±±0.70.7 22.027.5±±0.61.3 49.153.8±±0.10.2 70.774.6±±1.00.273.080.1±±0.50.7 88.187.4±±0.80.4 70.369.3±±0.40.7 71.266.0±±1.80.362.951.2±±1.41.970.167.2±±0.30.2 68.372.8±±0.60.742.758.3±±0.60.274.376.5±±0.51.3 63.965.5±±0.70.6 <cur>GLIPv2</cur>-H All Prompt 71.2 31.1 57.1 75.0 79.8 88.1 68.6 68.3 59.6 70.9 73.6 61.4 78.6 69.1±0.0 <cur>GLIPv2</cur>-H 1 Full 67.8±0.6 17.3±0.6 50.7±0.3 63.8±0.567.3±0.9 89.4±3.2 69.3±1.7 68.2±0.866.6±1.966.8±0.7 67.0±0.334.0±0.475.0±1.5 61.7±0.5 <cur>GLIPv2</cur>-H 3 Full 62.3±0.2 29.1±0.4 53.8±1.6 72.7±1.378.4±1.2 85.8±3.4 68.6±0.9 60.7±1.743.6±0.365.9±1.9 72.2±0.655.9±0.881.1±2.1 64.1±0.8 <cur>GLIPv2</cur>-H 5 Full 66.4±1.0 23.4±0.6 50.7±0.7 73.9±0.971.8±1.2 84.2±3.7 71.2±0.2 68.1±1.067.4±0.770.8±0.2 65.8±0.854.6±1.075.6±2.8 64.4±0.6 <cur>GLIPv2</cur>-H<cur>GLIPv2</cur>-H All 10 FullFull 67.374.4±1.3 31.636.3±1.1 52.458.7±0.7 71.377.1±0.480.079.3±0.4 88.188.1±0.1 72.974.3±0.9 56.973.1±2.452.270.0±1.165.472.2±0.9 73.972.5±0.761.058.3±0.684.081.4±3.2 65.970.4±0.3</p>
<p>Table 12: Per-dataset performance of DyHead, GLIP-T, GLIP-L, and <cur>GLIPv2</cur>-T, <cur>GLIPv2</cur>-B and <cur>GLIPv2</cur>-H. For PascalVOC, we report the mAP (IoU=0.50:0.95) using the COCO evaluation script, to be consistent with other 12 datasets. “Prompt” denotes prompt tuning. “Full” denotes full-model tuning.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.16e2a7d4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.d6c3db9e.min.js"></script>
      
        <script src="../../../javascripts/mathjac.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>