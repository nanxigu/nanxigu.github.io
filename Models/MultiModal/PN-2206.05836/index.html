
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-8.5.8">
    
    
      
        <title>GLIPv2: Unifying Localization and VL Understanding - Nanxi Gu</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.20d9efc8.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.815d1a91.min.css">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../paper.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#glipv2-unifying-localization-and-vl-understanding" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Nanxi Gu" class="md-header__button md-logo" aria-label="Nanxi Gu" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Nanxi Gu
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              GLIPv2: Unifying Localization and VL Understanding
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Nanxi Gu" class="md-nav__button md-logo" aria-label="Nanxi Gu" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Nanxi Gu
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        模型
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../Scholars/" class="md-nav__link">
        学者
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../Books/" class="md-nav__link">
        书籍
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/" class="md-nav__link">
        课程
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../Projects/" class="md-nav__link">
        项目
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract 摘要
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    1. Introduction 介绍
  </a>
  
    <nav class="md-nav" aria-label="1. Introduction 介绍">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#localization-vl-understanding-grounded-vl-understanding" class="md-nav__link">
    Localization + VL understanding = grounded VL understanding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-stronger-vl-grounding-task-inter-image-region-word-contrastive-learning" class="md-nav__link">
    A stronger VL grounding task inter-image region-word contrastive learning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glipv2-achieves-mutual-benefit-between-localization-and-vl-understanding" class="md-nav__link">
    GLIPv2 achieves mutual benefit between localization and VL understanding
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-related-work" class="md-nav__link">
    2. Related Work 相关工作
  </a>
  
    <nav class="md-nav" aria-label="2. Related Work 相关工作">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#localization-models" class="md-nav__link">
    Localization Models 定位模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vision-language-understanding-models" class="md-nav__link">
    Vision-Language Understanding Models 视觉语言理解模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unifying-localization-and-understanding" class="md-nav__link">
    Unifying localization and understanding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glipv2-vs-glip" class="md-nav__link">
    GLIPv2 vs GLIP
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-glipv2-unifying-localization-and-vl-understanding" class="md-nav__link">
    3. GLIPv2: Unifying Localization and VL Understanding
  </a>
  
    <nav class="md-nav" aria-label="3. GLIPv2: Unifying Localization and VL Understanding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-a-unified-vl-formulation-and-architecture" class="md-nav__link">
    3.1. A Unified VL Formulation and Architecture
  </a>
  
    <nav class="md-nav" aria-label="3.1. A Unified VL Formulation and Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vision-language-understanding-tasks" class="md-nav__link">
    Vision-Language understanding tasks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#language-guided-object-detection-and-phrase-grounding" class="md-nav__link">
    (Language-guided) object detection and phrase grounding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#language-guided-instance-segmentation-and-referring-image-segmentation" class="md-nav__link">
    (Language-guided) instance segmentation and referring image segmentation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-glipv2-pre-training" class="md-nav__link">
    3.2. GLIPv2 Pre-training
  </a>
  
    <nav class="md-nav" aria-label="3.2. GLIPv2 Pre-training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intra-image-region-word-alignment-loss" class="md-nav__link">
    Intra-image region-word alignment loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pre-training-with-both-detection-and-paired-image-text-data" class="md-nav__link">
    Pre-training with both detection and paired-image-text data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#second-stage-pre-training-of-the-segmentation-head" class="md-nav__link">
    Second-stage pre-training of the segmentation head
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-transfer-glipv2-to-localization-and-vl-tasks" class="md-nav__link">
    3.3. Transfer GLIPv2 to Localization and VL Tasks
  </a>
  
    <nav class="md-nav" aria-label="3.3. Transfer GLIPv2 to Localization and VL Tasks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#one-model-architecture-for-all" class="md-nav__link">
    One model architecture for all
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#one-set-of-weights-for-all" class="md-nav__link">
    One set of weights for all
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grounded-vl-understanding" class="md-nav__link">
    Grounded VL understanding
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-experiments" class="md-nav__link">
    4. Experiments 实验
  </a>
  
    <nav class="md-nav" aria-label="4. Experiments 实验">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-one-model-architecture-for-all" class="md-nav__link">
    4.1. One Model Architecture for All
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-one-set-of-model-parameters-for-all" class="md-nav__link">
    4.2. One Set of Model Parameters for All
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-glipv2-as-a-strong-few-shot-learner" class="md-nav__link">
    4.3. GLIPv2 as a Strong Few-Shot Learner
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-analysis" class="md-nav__link">
    4.4. Analysis
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-conclusion-and-social-impacts" class="md-nav__link">
    5. Conclusion and Social Impacts
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References 参考文献
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="glipv2-unifying-localization-and-vl-understanding">GLIPv2: Unifying Localization and VL Understanding</h1>
<ul>
<li>作者: Haotian Zhang | Pengchuan Zhang | Xiaowei Hu | Yen-Chun Chen | Liunian Harold Li | Xiyang Dai | Lijuan Wang | Lu Yuan | Jenq-Neng Hwang | Jianfeng Gao</li>
<li>机构: 华盛顿大学 | Meta AI | 微软 | UCLA</li>
<li>时间: 2022-06-12 -&gt; 2022-10-11</li>
<li>发表: NeurIPS 2022</li>
<li>预印: <a href="https://arxiv.org/abs/2206.05836">arXiv:2206.05836v2</a></li>
<li>领域: #计算机视觉</li>
<li>标签: #开源</li>
<li>引用: 77 篇</li>
<li>代码: <a href="https://github.com/microsoft/GLIP">Github</a></li>
</ul>
<style>
    cur{
        color:red;
    }
    term{
        color:blue;
    }
    abbv{
        color:orange;
    }
</style>

<h2 id="abstract">Abstract 摘要</h2>
<blockquote>
<p>We present <cur>GLIPv2</cur>, a grounded VL understanding model, that serves both localization tasks (e.g., object detection, instance segmentation) and <term>Vision-Language (VL)</term> understanding tasks (e.g., VQA, image captioning). <cur>GLIPv2</cur> elegantly unifies localization pre-training and <term>Vision-Language Pre-training (VLP)</term> with three pre-training tasks: phrase grounding as a VL reformulation of the detection task, region-word contrastive learning as a novel region-word level contrastive learning task, and the masked language modeling. This unification not only simplifies the previous multi-stage VLP procedure but also achieves mutual benefits between localization and understanding tasks. Experimental results show that a single <cur>GLIPv2</cur> model (all model weights are shared) achieves near SoTA performance on various localization and understanding tasks. The model also shows (1) strong zero-shot and few-shot adaption performance on open-vocabulary object detection tasks and (2) superior grounding capability on VL understanding tasks. Code is released at Github.</p>
</blockquote>
<h2 id="1-introduction">1. Introduction 介绍</h2>
<blockquote>
<p>Recently, a general interest arises in building general-purpose vision systems [ 24 , 28 , 66 , 47 ], also called vision foundation models [ 6 , 67 ], that solve various vision tasks simultaneously, such as image classification [ 35 ], object detection [ 44 ], and Visual-Language (VL) understanding [ 3 , 11 , 32 ]. Of particular interest, is the unification betweenlocalizationtasks (e.g., object detection [ 44 ] and segmentation [ 8 , 23 ]) and VLunderstandingtasks (e.g., VQA [ 3 ] and image captioning [ 11 ]). Localization pre-training benefits VL tasks [ 1 , 70 ], and the “localization-&gt;VLP” two-stage pretraining procedure [ 46 , 57 , 13 , 56 , 39 , 37 , 75 , 42 , 40 ] is the common practice in VL community. A long-standing challenge is the unification of localization and understanding, which aims atmutual benefit between these two kinds of tasks, simplified pre-training procedure, and reduced pre-training cost. However, these two kinds of tasks appear to be dramatically different: localization tasks are visiononly and require fine-grained output (e.g., bounding boxes or pixel masks), while VL understanding tasks emphasize fusion between two modalities and require high-level semantic outputs (e.g., answers or captions). [ 24 , 28 , 66 ] have made early attempts at unifying these tasks in a straightforward multi-task manner, where a low-level visual encoder is shared across tasks, and two separate high-level branches are designed for localization and VL understanding, respectively. The localization tasks are still vision-only and do not benefit from the rich semantics in vision-language data. As a result, such unified models see the marginal mutual benefit or even performance degradation [ 28 ] compared with task-specific models.</p>
<p>In this paper, we identify “VL grounding” as a “meta”-capability for localization and understanding capabilities. VL grounding involves not onlyunderstandingan input sentence but alsolocalizingthe mentioned entities in the image (see an example in Figure 1). We build agrounded VL understandingmodel (<cur>GLIPv2</cur>) as a unified model for localization and VL understanding tasks.</p>
</blockquote>
<!-- Figure 1: Left: <cur>GLIPv2</cur>, a pre-trained grounded VL understanding model, unifies various localization and VL understanding tasks. These two kinds of tasks mutually benefit each other, and enables new capabilities such as language-guided detection/segmentation and grounded VQA/captioning. Right: Additional examples from ODinW (detection), LVIS (segmentation), VQA, COCO Captioning. -->

<h3 id="localization-vl-understanding-grounded-vl-understanding">Localization + VL understanding = grounded VL understanding</h3>
<blockquote>
<p>Localization tasks involve both localization and semantic classification, where classification can be cast as a VL understanding problem using theclassification-to-matchingtrick (Section 3.1). Therefore, we reformulate localization tasks as VL grounding tasks, in which the language input is a synthesized sentence as the concatenation of category names [ 41 ]. Localization data are turned into VL grounding data, accordingly. The massive VL understanding data (image-text pairs) can be easily turned into VL grounding data in a self-training manner [ 41 ]. Therefore, <cur>GLIPv2</cur> has a unified pre-training process: all task data are turned into grounding data and <cur>GLIPv2</cur> is pre-trained to perform grounded VL understanding.</p>
</blockquote>
<h3 id="a-stronger-vl-grounding-task-inter-image-region-word-contrastive-learning">A stronger VL grounding task inter-image region-word contrastive learning</h3>
<blockquote>
<p>GLIP [ 41 ] proposes the phrase grounding task as its pre-training task, which we argue is an easy task and does not fully utilize data information. For example, in the VL grounding task in Figure 1, the phrase grounding task only requires the model to match a given image region to one of the three phrases in the text input, i.e., “green, pink striped, or plain white umbrella?”. This 1-in-3 choice is very easy, only requires color understanding, but loses lots of information in this grounding data: the umbrellas are not any other colors, like black, yellow, etc; objects in those regions are umbrellas but not any other categories, like car, bike, etc. From a contrastive learning view, this phrase grounding task only has two negatives. More negatives can be created from this annotation and thus enable stronger contrastive learning. In <cur>GLIPv2</cur>, we introduce the novel inter-image region-word contrastive learning task, which leverages phrases from other sentences in the same batch as potential negatives, as another much stronger VL grounding task. This new region-word contrastive loss enables <cur>GLIPv2</cur> to learn more discriminative region-word features and demonstrates improvements over all downstream tasks.</p>
</blockquote>
<h3 id="glipv2-achieves-mutual-benefit-between-localization-and-vl-understanding"><cur>GLIPv2</cur> achieves mutual benefit between localization and VL understanding</h3>
<blockquote>
<p>1) Experimental results (Table 2) show that a single <cur>GLIPv2</cur> model (all model weights are shared) achieves near SoTA performance on various localization and understanding tasks.
2) Thanks to semantic-rich annotations from the image-text data, <cur>GLIPv2</cur> shows superior zero-shot and few-shot transfer learning ability to open-world object detection and instance segmentation tasks, evaluated on the LVIS dataset and the "Object Detection in the Wild (ODinW)" benchmark.
3) <cur>GLIPv2</cur> enables language-guided detection and segmentation ability, and achieves new SoTA performance on the Flick30K-entities phrase grounding and PhraseCut referring image segmentation tasks.
4) Inherently a grounding model, <cur>GLIPv2</cur> leads to VL understanding models with strong grounding ability, which are self-explainable and easy to debug. For example, <cur>GLIPv2</cur>, when <cur>GLIPv2</cur> is finetuned on VQA, it can answer questions while localizing mentioned entities (see Figure 1 and Section 4.4).</p>
</blockquote>
<h2 id="2-related-work">2. Related Work 相关工作</h2>
<h3 id="localization-models">Localization Models 定位模型</h3>
<blockquote>
<p>Traditionally, localization tasks such as object detection and segmentation are single-modality and output bounding boxes or pixel masks [ 52 , 43 , 26 , 14 , 53 , 10 , 9 ]. One challenge of these single-modality models lies in generalization to rare and novel concepts: it is hard to collect localization data that cover many rare categories [ 23 ]. A long line of research focuses on this generalization problem, under the name of zero-shot [ 4 , 76 , 7 , 77 ], weakly-supervised [ 19 , 5 , 61 ], or open-vocabulary [ 68 , 22 ] localization.</p>
</blockquote>
<p>传统的定位任务如目标检测和分割是单模态并输出边界框或像素掩膜. 这些单模态模型的一个挑战在于对稀有的和新颖概念的推广: 很难收集涵盖许多稀有类别的定位数据. 一长串的研究集中在这个泛化问题上, 以零样本, 弱监督或开放词汇表定位为名.</p>
<blockquote>
<p>Built upon MDETR[^30] and GLIP[^41], <cur>GLIPv2</cur> converts localization tasks into a grounded vision-language task using the classification-to-matching trick (Section 3). Thus <cur>GLIPv2</cur> can learn from the semantic-rich vision-language data and shows strong performance on open-vocabulary localization tasks.</p>
</blockquote>
<p>基于 MDETR 和 GLIP, GLIPv2 使用分类匹配技巧 (第3节) 将定位任务转换为 Grounded 视觉语言任务. 因此, GLIPv2 可以从语义丰富的视觉语言数据中学习, 并在开放词汇表定位任务中表现出强大的性能.</p>
<h3 id="vision-language-understanding-models">Vision-Language Understanding Models 视觉语言理解模型</h3>
<blockquote>
<p>Vision-language (VL) understanding tasks such as VQA[^3], image captioning [ 11 ], and image-text retrieval [ 31 ] involve understanding visual semantics and how they are expressed in natural language. Many VL models (e.g., BUTD) [ 2 , 70 ] rely on a pre-trained localization model as their visual encoder; the downside is the pro-longed “localization-&gt;VLP” pre-training pipeline [ 46 , 57 , 13 , 56 , 39 , 37 , 75 , 42 , 40 ]. In contrast, <cur>GLIPv2</cur> simplifies the pre-training pipeline and enables grounded VL understanding for better interpretability (Section 4.4).</p>
</blockquote>
<p>视觉语言理解模型例如 VQA, image captioning 和 image-text retrieval 涉及到理解视觉语义和如何以自然语言表示它们. 许多视觉语言模型如 BUTD 依赖于预训练定位模型作为其视觉编码器, 底侧是长期的 "定位→VLP" 预训练过程. 相比之下, GLIPv2 简化了预训练过程并使得 grounded 视觉语言理解有了更强的解释性.</p>
<h3 id="unifying-localization-and-understanding">Unifying localization and understanding</h3>
<blockquote>
<p>[24, 28, 66] made pioneering efforts in unifying localization and understanding. However, localization tasks are still treated as single-modality tasks, while VL tasks involve two modalities. The unification is achieved via straightforward multi-tasking: a low-level visual encoder is shared across tasks and two separate branches are designed for localization and VL understanding. Such unified models do not bring evident mutual benefit and often underperform task-specific models. In contrast, <cur>GLIPv2</cur> identifies grounded VL understanding as a meta-task for localization and understanding. The task unification brings architecture unification: the unified grounded VL understanding model empowers a localization branch with VL capacity, arriving at a unified branch that excels at both tasks.</p>
</blockquote>
<h3 id="glipv2-vs-glip"><cur>GLIPv2</cur> vs GLIP</h3>
<blockquote>
<ol>
<li>GLIP shows that grounded pre-training improves localization. <cur>GLIPv2</cur> further shows grounded pre-training improves VL understanding and thus leads to a unified model for localization and VL understanding.</li>
<li><cur>GLIPv2</cur> introduces the inter-image region-word contrastive loss, which is another and stronger grounding task than the pre-training task in GLIP. The proposed loss can be viewed as a region-word level generalization of the prevalent image-level contrastive learning [38, 51, 65].</li>
<li><cur>GLIPv2</cur> outperforms GLIP on all benchmarks with the same pre-training data.</li>
</ol>
</blockquote>
<h2 id="3-glipv2-unifying-localization-and-vl-understanding">3. <cur>GLIPv2</cur>: Unifying Localization and VL Understanding</h2>
<p>Based on the reformulation of object detection as a generalized phrase grounding task in GLIP [ 41 ], we unify both localization and VL understanding tasks as grounded vision-language tasks. A grounded vision-language task takes both image and text as inputs, and outputs region-level understanding results (e.g., detection, segmentation) and/or image-level understanding results with associated grounding/localization information (e.g., VQA, image captioning). We will present the unified grounded VL formulation and architecture in Section 3.1, the pre-training losses in Section 3.2, and transfer to downstream tasks in Section 3.3.</p>
<h3 id="31-a-unified-vl-formulation-and-architecture">3.1. A Unified VL Formulation and Architecture</h3>
<p>At the center of <cur>GLIPv2</cur>’s unified formulation is theclassification-to-matchingtrick, which reformulates anytask-specific fixed-vocab classification problem as an task-agnostic open-vocabulary vision-language matchingproblem. The best example is the reformulation of image classification as image-text matching in CLIP [ 51 ], which enables the model to learn from raw image-text data directly, and achieves strong zero-shot results on open-vocabulary classification tasks. In <cur>GLIPv2</cur>, we replace every semantic classification linear layer in traditional single-modality vision models with a vision-language matching dot-product layer.</p>
<p>As illustrated in Figure 1, <cur>GLIPv2</cur>’s unified VL architecture is based on the generic architecture we term ArchitectureΠ. It consists of a dual encoder, denoted asEncVandEncL, and a fusion encoder, denoted asEncV L. The model takes an image-text pair(Img,Text)as input, and extract visual and text features as below:</p>
<p>O ̊=EncV(Img), P ̊=EncL(Text), O, P=EncV L(O, ̊P ̊), (1)</p>
<p>where(O ̊,P ̊)and(O, P)denote the image/text featuresbeforeandafterVL fusion, respectively.</p>
<h4 id="vision-language-understanding-tasks">Vision-Language understanding tasks</h4>
<p>ArchΠis the most popular model architecture for VL understanding tasks. Given the cross-modality fused representationsOandP, it is straightforward to add lightweight task-specific heads for various VL tasks. For example, <cur>GLIPv2</cur> adds a two-layer MLP on top of text featuresPas the masked language modeling (MLM) head, to perform the MLM pre-training. We provide model details of VQA and image captioning in Section 3.3.</p>
<h4 id="language-guided-object-detection-and-phrase-grounding">(Language-guided) object detection and phrase grounding</h4>
<p>Following GLIP [ 41 ], <cur>GLIPv2</cur> uses the classification-to-matching trick to unify detection and grounding. More specifically, for detection, we simply replace the class logitsScls=OWT, whereWis the weight matrix of the box classifier, with a task-agnostic region-word similarity logitsSground=OPT, where text featuresPare label embeddings from a task-agnostic language encoder. As shown in Figure 1, object detection and phrase grounding share the same input/output format and model architecture. See GLIP [ 41 ] for more details. Their only difference is the input text format: (1) for object detection, the text input is a string of concatenated candidate object labels; (2) for phrase grounding, the text input is a natural language sentence. We refer to GLIP [41] for more details.</p>
<h4 id="language-guided-instance-segmentation-and-referring-image-segmentation">(Language-guided) instance segmentation and referring image segmentation</h4>
<p>Given the object detection results, an instance segmentation head is added to classify each pixel within the box into a semantic class. Again, <cur>GLIPv2</cur> uses the classification-to-matching trick to produce a unified instance segmentation head for the standard instance segmentation tasks and the referring image segmentation tasks and leverage both types of data for its pre-training. This classification-to-matching trick can also apply to many other semantic classification heads in single modality CV models (e.g., semantic segmentation) and thus transfers them to language-guided CV models.</p>
<h3 id="32-glipv2-pre-training">3.2. <cur>GLIPv2</cur> Pre-training</h3>
<p>The <cur>GLIPv2</cur> is pre-trained with three pre-training losses: phrase grounding lossLgroundfrom a vision-language reformulation of the object detection task, region-word contrastive lossLinterfrom a novel region-word level contrastive learning task, and the standard masked language modeling loss Lmlmproposed in BERT [17].</p>
<p>L<cur>GLIPv2</cur>=Lloc+Lintra ︸ ︷︷ ︸ Lground</p>
<p>+Linter+Lmlm (2)</p>
<p>Similar to losses in detection tasks, the grounding lossLgroundhas two parts: the localization lossLloc trains localization heads with bounding-box supervision, e.g., RPN loss, box regression loss and/or centerness loss [ 59 ]; the intra-image region-word alignment lossLintrais essentially the semantic classification/retrieval loss for each region.</p>
<h4 id="intra-image-region-word-alignment-loss">Intra-image region-word alignment loss</h4>
<p>Given one image-text pair(Img,Text), we obtain the image and text featuresaftercross-modality fusionOandP. The Intra-image region-word alignment loss is computed by Lintra=loss(OPT;T), (3)</p>
<p>whereOPTis the similarity score between image regions and word tokens, andTis the target affinity matrix determined by the ground-truth annotations. The loss functionlossis typically a cross-entropy loss for two-stage detectors [53] and a focal loss [43] for one-stage detectors.</p>
<p>However, as discussed in Section 1, this intra-image region-word contrastive learning is rather weak in the sense of contrastive learning, due to the limited number of phrases that can one caption can contain. GLIP [ 41 ] alleviates this problem by appending a few negative sentences to form a longer text input with more (negative) phrases. However, constrained by the maximal length of text tokens (256 in GLIP and <cur>GLIPv2</cur>), only a few negative sentences can be added and the number of negative phrases remains in the order of 10’s. This small-negative-example problem also exists in detection data [41] when the input text cannot include all class names in a detection dataset, e.g., Objects365. Inter-image region-word contrastive loss.In <cur>GLIPv2</cur>, we propose using phrases from other imagetext pairs in the same batch as negative examples, which effectively increases the number of negative examples to the order of 1000’s, with nearly negligible additional computational cost. As in(1), given a batch of image-text pairs(Imgi,Texti)Bi=1and their ground-truth annotations (Ti)Bi=1, the model produces the image and text featuresbeforeandafterVL fusion, denoted as</p>
<p>(O ̊i,P ̊i)Bi=1and(Oi, Pi)Bi=1, respectively. Then as illustrated in Figure 2 (Left), a batch-wise similarity matrixSbatchgroundand a batch-wise target affinity matrixTbatchare constructed by considering all the image regions and text phrases across this batch. Their(i, j)’th blocks are obtained as below:</p>
<p>Sbatchground[i, j] =O ̊i(P ̊j)T, Tbatch[i, j] =</p>
<p>Ti, ifi=j obtained by label propagation, otherwise.</p>
<p>The inter-image region-word contrastive loss is then defined as the standard bi-directional contrastive loss applied on all image regions and phrases in this batch:</p>
<p>Linter=cross_entropy_loss(Sbatchground, Tbatch,axis= 0)+cross_entropy_loss(Sbatchground, Tbatch,axis= 1). (5) Compared with that in the inter-image contrastive loss(3), the number of negatives is multiplied by batch sizeBin this inter-image contrastive loss(5). We elaborate two important details in(4). (1) <cur>GLIPv2</cur> uses the image text features(O ̊i,P ̊i)Bi=1before VL fusion,not(Oi, Pi)Bi=1after VL fusion, to compute the batch-wise similarity matrix in the inter-image contrastive loss(4). Otherwise, the image and text features after VL fusion would have seen the paired information(1), and thus the model can easily rule out the negatives from misaligned images/texts. (2) We cannot simply assign all regions and texts from unpaired image-text as negative pairs, as done in the standard contrastive loss in CLIP [ 51 ]. Instead, we determine the off-diagonal blocks in the target affinity matrixTbatchby label propagation. For example, as illustrated in Figure 2 (Left), if a region is annotated as “person”, it should be a positive pair with all “person” phrases in detection-type texts. We do not propagate positives to grounding-type texts (natural sentences) because phrases in sentences carry contexts that are unique to that image-sentence pair.</p>
<h4 id="pre-training-with-both-detection-and-paired-image-text-data">Pre-training with both detection and paired-image-text data</h4>
<p><cur>GLIPv2</cur> pre-training data is in the image-text-target triplet format(Img,Text, T), where the target affinity matrixTcontains the box-label localization annotations. We also use massive image-text pair data(Img,Text)to pre-train <cur>GLIPv2</cur>, by generating grounding boxesTˆfor phrases in the text with the GLIP pre-trained model from [ 41 ]. The human-annotated OD/grounding data provides high-fidelity localization supervision, while the massive image-text data greatly improves the concept diversity for <cur>GLIPv2</cur>.</p>
<h4 id="second-stage-pre-training-of-the-segmentation-head">Second-stage pre-training of the segmentation head</h4>
<p><cur>GLIPv2</cur> performs a second-stage pre-training of the language-guided segmentation head on both instance segmentation and image referring segmentation data, while fixing all other parts of the model.</p>
<h3 id="33-transfer-glipv2-to-localization-and-vl-tasks">3.3. Transfer <cur>GLIPv2</cur> to Localization and VL Tasks</h3>
<p>We introduce two ways to easily transfer <cur>GLIPv2</cur> to various downstream tasks. In addition, <cur>GLIPv2</cur> can perform conventional VL tasks (e.g., VQA) along with localization, effectively making every task we consider a “grounded VL understanding” task.</p>
<h4 id="one-model-architecture-for-all">One model architecture for all</h4>
<p><cur>GLIPv2</cur> can be transferred to downstream tasks by fine-tuning the model with an (optional) task-specific head. 1) Fordetection and segmentationtasks, no task-specific head is needed as the pre-training architecture can inherently perform detection and segmentation. 2) ForVLtasks: for VQA, a classification head is added on top of the hidden representation of the start-of-sequence token; for caption generation, we train with a unidirectional language modeling loss, which maximizes the likelihood of the next word given context. We use a unidirectional attention mask and prevent the image part from attending to the text in the fusion layers.</p>
<!-- Figure 2: <cur>GLIPv2</cur> pre-training losses: the intra-image alignment lossLintra(right) takes features after VL fusion and compute loss over region-word pairs within each image-text pair; the inter-image contrastive loss (left)Lintertakes features before VL fusion and computes loss over all region-word pairs across a batch of image-text pairs. Label propagation is used to determine the off-diagonal blocks of theLintertarget matrix (4).  -->

<h4 id="one-set-of-weights-for-all">One set of weights for all</h4>
<p>There is a growing interest in developing models that can be transferred to various tasks while only changing the least amount of parameters to save training time and storage cost [ 55 , 36 ]. Following GLIP, <cur>GLIPv2</cur> can be transferred to localization tasks in azero-shotor aprompt-tuningsetting (Section 4.2). One single <cur>GLIPv2</cur> model can serve various tasks, where each task only keeps few or no parameters. Of particular interest is the prompt tuning setting. For a certain localization task, the text prompt is the same for all input images; thus, we could directly tuneP ̊, a small prompt embedding matrix, to adapt <cur>GLIPv2</cur> to new tasks. Prompt tuning in a deep-fused model such as <cur>GLIPv2</cur> is different from the conventional linear probing/prompt tuning setting [ 62 , 51 , 73 ] in shallow-interacting vision models such as CLIP. The latter can also be viewed as only tuning a small prompt/softmax embeddingP; however, tuningPonly affects the very last layer of the model while the visual representation is still frozen. In contrast, GLIP/<cur>GLIPv2</cur>’s visual representation is conditioned on the prompt embeddingP ̊; tuningP ̊changes the text, visual, as well as fused embeddings. As a result, prompt tuning in <cur>GLIPv2</cur> is highly effective, often matching the performance of fine-tuning (see Table 2). This is in contrast to the common observation in CV that linear probing lags behind fine-tuning by a large gap [25].</p>
<h4 id="grounded-vl-understanding">Grounded VL understanding</h4>
<p><cur>GLIPv2</cur> also enables grounded VL understanding, where we retain the ability to perform grounding when fine-tuning the model to a downstream VL task. This increases the interpretability of the model. Specifically, we first turn the VL data of the downstream task into grounded VL data using a pre-trained GLIP model. Then we train the model with both the downstream task head and grounding head. For VQA, the model is trained to predict the answer and ground entities in the question as well as the implied entity in the answer; for captioning, the model is trained to predict the next word given the context and ground the current decoded word. By tuning localization tasks into a grounded VL task and augmenting VL tasks with grounding ability, we effectively turn every task into a grounded VL understanding task (see examples in Figure 1).</p>
<h2 id="4-experiments">4. Experiments 实验</h2>
<p>In this section, we show that <cur>GLIPv2</cur> serves as a performant and easy-to-deploy general-purpose vision system. 1)One Model Architecture for All(Section 4.1). <cur>GLIPv2</cur> can be directly fine-tuned to both localization and VL understanding tasks with minimal architecture change. It achieves performance on par with SOTA models with specialized architectures. 2)One Model Weight for All(Section 4.2). <cur>GLIPv2</cur> can be transferred to localization tasks in a zero-shot manner with zero</p>
<hr />
<p>Model Model Type COCO-Det ODinW(test-dev) (test) (minival)LVIS COCO-Mask(test-dev) Flickr30K PhraseCut(test) (test) (test-dev / test-std) (Karpathy-test)VQA Captioning Mask R-CNN [26] Localization</p>
<p>39.8 33.3 / / 37.1 DETR [9] 42.0 17.8 / DyHead-T [15] 49.7 60.8 DyHead-L [15] 60.3* VisualBERT [39] Understanding</p>
<pre><code>- - - - 71.33 - 70.8 / 71.0 - UNITER [12] - - - - - - 73.8 / 74.0 - VinVL [70] - - - - - - 76.5 / 76.6 130.8 GPV [24] Localization &amp; Understanding

    - - - - - - 62.5 / - 102.3 UniT [28] 42.3 - - - - - 67.6 / - - MDETR [30] - - 24.2 / - - 84.3 53.7 70.6 / 70.6 - Unicorn [66] - - - - 80.4 - 69.2 / 69.4 119.1 GLIP-T [41] Localization &amp; Understanding
</code></pre>
<p>55.2 64.9 85.7 GLIP-L [41] 61.5* 68.9 87.1 <cur>GLIPv2</cur>-T (Ours) Localization &amp; Understanding</p>
<p>55.5 66.5 50.6 / 41.4 53.5 / 42.0 86.5 59.4 71.6 / 71.8 122.1 <cur>GLIPv2</cur>-B (Ours) 58.8 69.4 57.3 / 46.2 59.0 / 45.8 87.5 61.3 73.1 / 73.3 128.5 <cur>GLIPv2</cur>-H (Ours) 60.6 (62.4*) 70.4 59.8 / 48.8 59.8 / 48.9 87.7 61.3 74.6 / 74.8 131.0</p>
<p>Table 1: One model architecture results. For COCO-Det test-dev, * indicates multi-scale evaluation. For LVIS, we report the numbers for bothbboxandsegmon minival to avoid data contamination due to the pre-training. For Flickr30K test, we report the metric underR@1. For COCO-Mask, we also report bothbboxandsegmon test-dev.</p>
<p>parameter update; with prompt tuning, a single <cur>GLIPv2</cur> model can achieve comparable performance with fully fine-tuned settings on both localization and understanding tasks.</p>
<p>Following GLIP [41], we adopt Swin Transformer [45] as the image encoder EncV, text transformers [ 60 , 51 ] as the text encoderEncL, Dynamic Head [ 15 ] with language-aware deep fusion [ 41 ] as the fusion encoderEncV L, and Hourglass network [ 49 ] as instance segmentation head feature extractor. We train <cur>GLIPv2</cur> at three scales: <cur>GLIPv2</cur>-T, <cur>GLIPv2</cur>-B, and <cur>GLIPv2</cur>-H.</p>
<p><cur>GLIPv2</cur>-Thas the same model config and initialization as GLIP-T: Swin-Tiny and BERT-Base as the dual encoder. The model is pre-trained on the following data: 1) O365, 2) GoldG as in GLIP-T (C), and 3) Cap4M, 4M image-text pairs collected from the web with boxes generated by GLIP-T [ 41 ].<cur>GLIPv2</cur>-B/<cur>GLIPv2</cur>-Hare based on Swin-Base/Swin-Huge and the pre-layernorm text transformer [ 18 ] as dual encoder, and are initialized from the UniCL [ 65 ] checkpoints. We observe much stabler training with GPT-type pre-layernorm transformer [ 18 ] than BERT-type post-layernorm transformer. The training data contain: 1) FiveODs (2.78M data)^1 ; 2) GoldG as in MDETR [ 30 ]; and 3) CC15M+SBU, 16M public image-text data with generated boxes by GLIP-L [ 41 ].Segmentation headsof <cur>GLIPv2</cur> models are pre-trained on COCO, LVIS [ 23 ] and PhraseCut [ 63 ], with all other model parameters are frozen.</p>
<p>NoteAll datasets above were collected by the creators (cited) and consent for any personally identifiable information (PII) was ascertained by the authors where necessary. Due to limited space, we refer to supplementary for details of training recipes and hyper-parameters.</p>
<h3 id="41-one-model-architecture-for-all">4.1. One Model Architecture for All</h3>
<p>We compare <cur>GLIPv2</cur> to existing object detection and vision-language pre-training methods on a wide range of tasks. We fine-tune the model on 8 different downstream tasks and report the performance in Table 1. We make the following observations.</p>
<p><cur>GLIPv2</cur> v.s. specialized Localization methods.<cur>GLIPv2</cur> outperforms previous localization models on generalization to both common and rare classes and domainswith a single model architecture and pre-training stage.1) OD on common categories (COCO-Det), <cur>GLIPv2</cur>-T achieves 5.8 improvement compared to the standard DyHead-T trained on O365 (55.5 v.s. 49.7). <cur>GLIPv2</cur>-H reaches 62.4 AP on test-dev, and surpass the performance of the previous SoTA model GLIP-L.2) OD on rare / unseen categories (LVIS), <cur>GLIPv2</cur>-T outperforms a supervised MDETR on thebboxby a great margin (59.8 v.s. 24.2).3) Generalization to diverse real-word tasks (ODinw), <cur>GLIPv2</cur>-T (55.5) performs better than original GLIP-T (64.9) on the average of 13 public datasets; <cur>GLIPv2</cur>-B outperforms GLIP-L by 0.5 AP.4) Instance segmentation (COCO-Mask &amp; PhraseCut), for traditional instance segmentation</p>
<p>(^1) Besides O365, it combines with 4 additional OD datasets including COCO [44], OpenImages [33], Visual Genome [34], and ImageNetBoxes [35]</p>
<hr />
<p>Model</p>
<p>Direct Evaluation Prompt Tuning COCO-Mask ODinW LVIS-Det Flickr30K COCO-Det ODinW LVIS COCO-Mask PhraseCut (minival) (test) (minival) (minival) (test-dev) (test) (minival) (test-dev) (test) GLIP-T 46.6/– 46.5 26.0 85.7 – 46.5 GLIP-L 49.8/– 52.1 37.3 87.1 58.8 67.9 <cur>GLIPv2</cur>-T 47.3/35.7 48.5 29.0 86.0 53.4(-2.1) 64.8(-1.7) 49.3 / 34.8(-1.3 / -6.6) 53.2 / 41.2(-0.3 / -0.8) 49.4 <cur>GLIPv2</cur>-B 61.9†/43.4 54.2 48.5 87.2 59.0(+0.2) 67.3(-2.1) 56.8 / 41.7(-0.5 / -4.5) 58.8 / 44.9(-0.2 / -0.9) 55.9 <cur>GLIPv2</cur>-H 64.1†/47.4 55.5 50.1 87.7 60.2 / 61.9*(-0.4 / -0.5) 69.1(-1.3) 59.2 / 43.2(-0.6 / -5.7) 59.8 / 47.2(-0.0 / -1.7) 56.1</p>
<p>Table 2: One set of weights results v.s. Original GLIP. * indicates multi-scale evaluation. Numbers in red clearly points out the difference between the prompt tuning and full fine-tuning results (see Table 1). Numbers in gray mean that they are not inzero-shotmanner.†: these two numbers are artificially high due to some overlap between COCO-minival and VisualGenome-train.</p>
<p>FullTuning-Model Prompt Tuning <cur>GLIPv2</cur>-H GLIP<cur>GLIPv2</cur>v2--BT</p>
<p>GLIP-T</p>
<p>DyHead-T</p>
<p>Figure 3: Data efficiency of <cur>GLIPv2</cur> on ODinW. The X-axis is the amount of task-specific data, from zero-shot to all data. Y-axis is the average AP across 13 datasets.</p>
<p>Model Zero-Shot 0 1 Prompt Tuning / Fine Tuning 3 5 10 All DyHead-TO365[41] 33.843.646.450.860.8Lloc+Lintra(GLIP-T) 46.5 49.951.3 53.754.9 55.556.4 56.658.4 62.464.9 Lloc+Lintra+Linter 48.4 52.151.4 55.655.3 56.756.6 58.359.5 62.966.3 Lloc+Lintra+Linter+Lmlm 48.5 52.452.8 55.655.6 57.457.4 58.859.7 64.866.5</p>
<p>Table 3: Zero-shot, prompt tuning, and full finetuning performance on ODinW. <cur>GLIPv2</cur> models exhibit superior data efficiency.</p>
<p>(i.e., COCO-Mask), <cur>GLIPv2</cur>-H outperforms the well-known Mask R-CNN by a great margin onsegm. For language-guided segmentation (i.e., PhraseCut), compared to MDETR, <cur>GLIPv2</cur>-T achieves an improvement of 5.7 mask AP.</p>
<p><cur>GLIPv2</cur> v.s. specialized VL Understanding methods. <cur>GLIPv2</cur> rivals with SoTA specialized models for VL tasks.1) For VQA, <cur>GLIPv2</cur> outperforms VisualBERT and UNITER and approaches the previous SoTA model VinVL.2) For Captioning, the best <cur>GLIPv2</cur> even surpasses VinVL (VinVL and <cur>GLIPv2</cur> are not trained with CIDEr optimization).</p>
<p><cur>GLIPv2</cur> v.s. localization and VL models.Prior works such GPV, UniT and Unicorn have also explored unifying localization and VL models (see a discussion in Section 2). <cur>GLIPv2</cur> outperforms all previous systems on both localization and VL tasks. For the best <cur>GLIPv2</cur>-H, it outperforms the UniT by a great margin (18.3 AP) on COCO object detection tasks. Meanwhile, it also surpasses UniT’s performance on VQA by 6.9 points and GPV’s peformance on Image Captioning as well.</p>
<p>Takeaway.Most notably, <cur>GLIPv2</cur> outperforms previous “unified” models (GPV, UniT, MDETR, Unicorn) by a large margin. This is the first time that a single model architecture could achieve near SoTA performance on both localization and understanding. In contrast, in prior work, there exists certain trade-off between localization and understanding: models that aim to achieve high understanding performance tend to have lower localization performance (e.g., UNiT’s detection performance is limited to the DETR [ 9 ] architecture), as it is not trivial to merge a SoTA localization branch and a SoTA VL branch into a single model.</p>
<h3 id="42-one-set-of-model-parameters-for-all">4.2. One Set of Model Parameters for All</h3>
<p><cur>GLIPv2</cur> is pre-trained to perform grounding; thus it can be transferred to various localization tasks with changing zero or few parameters. We evaluate <cur>GLIPv2</cur> under two such settings: 1) direct evaluation, where we transfer the model “as is” without any parameter change, and 2) prompt tuning, where only the prompt embedding is tuned for specific tasks (Section 3.3).</p>
<p>Direct evaluation. The pre-trained <cur>GLIPv2</cur> can be directly evaluated on any object detection task (by concatenating the object categories into a text prompt) and visual grounding task without any further tuning. We evaluate the models on four localization tasks: COCO, ODinW, LVIS, and Flickr30, and their results are presented in Table 2. Note that for <cur>GLIPv2</cur>-B and <cur>GLIPv2</cur>-H, the training sets of Flick30K and LVIS are present in the pre-training data. Thus, reported numbers on these metrics are notzero-shotevaluation (we have marked them gray). For all other evaluation results, the models are evaluated inzero-shotsettings without any further tuning.</p>
<p><cur>GLIPv2</cur> can be effortlessly transferred to different localization tasks without further tuning.1) For COCO, <cur>GLIPv2</cur>-T achieves a zero-shot performance of 47.3 without seeing any COCO training images. This surpasses well-established supervised systems (e.g., Mask R-CNN) and also outperforms GLIP-T by 0.7 AP. 2) ForODinW, <cur>GLIPv2</cur> also shows strong zero-shot performance. <cur>GLIPv2</cur>-T (48.5) surpasses the GLIP-T (46.5). Meanwhile, the zero-shot performance of <cur>GLIPv2</cur>-B and <cur>GLIPv2</cur>H even surpasses the 10-shot tuning performance of DyHead-T (to be introduced in Figure 3). 3) ForLVIS, <cur>GLIPv2</cur>-T achieves a 3 AP improvement performance compared to the GLIP-T. 4) For Flickr30K, <cur>GLIPv2</cur>-B achieves even higher number (87.2) compared to original GLIP-L (87.1).</p>
<p>Prompt Tuning. Following GLIP, <cur>GLIPv2</cur> supports efficient prompt tuning: the visual representation is heavily conditioned on the text representation due to the deep fusion block (Section 3.3); thus we could fine-tune only the prompt embedding for each task but still maintain high performance. Prompt tuning <cur>GLIPv2</cur> achieves similar performance as full fine-tuning. When comparing the performance of each task in Table 1 and 2 at the same time, for <cur>GLIPv2</cur>, prompt tuning performance almost matches the one model architecture results on localization tasks, without changing any of the grounding model parameters.</p>
<h3 id="43-glipv2-as-a-strong-few-shot-learner">4.3. <cur>GLIPv2</cur> as a Strong Few-Shot Learner</h3>
<p>We demonstrate <cur>GLIPv2</cur>’s performance on ODinW datasets with respect to different amounts of training data in Figure 3. The performance improvement between <cur>GLIPv2</cur>-T and GLIP-T exhibits more superior data efficiency for prompt tuning. We compare with the SoTA detector DyHead-T, pre-trained on Objects365 in Table 3. It can be seen that a zero-shot <cur>GLIPv2</cur>-T (48.5) outperforms a outperforms 5-shot DyHead-T (46.4) while the performance of one-shot <cur>GLIPv2</cur>-H (61.3) surpasses a all-shot fully supervised DyHead-T (60.8).</p>
<h3 id="44-analysis">4.4. Analysis</h3>
<p>Pre-training lossesTable 4 shows the performance of the downstream tasks with different variants of our method. Compared to the GLIP pre-training tasks with only intra-image region-word contrastive loss (Row 3), adding inter-image word-region loss (Row 5) substantially improves the pre-trained model performance across all the object detection tasks (COCO, ODinW, and LVIS) on both zero-shot and fine-tuned manner. Consistent with common observations from most VL understanding methods, adding MLM loss (Row4) benefits for learning the representation for understanding tasks (Flick30k, VQA, and Captioning). Furthermore, using all three losses together at the 1st stage pre-training and doing the 2nd stage pre-training without MLM on OD and GoldG data, <cur>GLIPv2</cur> (Row6) can perform well on both the localization and VL understanding tasks.</p>
<p>An additional stage of pre-training is applied for small models (<cur>GLIPv2</cur>-T and <cur>GLIPv2</cur>-B) due to limited model capacity. In order to achieve higher performance on both localization and understanding tasks, we find that including all data (even with some noise) and MLM loss in the first stage of pre-training will benefit the model for learning a better representation of both localization and understanding capability. Since the OD tasks require the model with more accurate localization ability, in our 2nd stage of pre-training, we decide to eliminate the MLM loss. The large model (<cur>GLIPv2</cur>-H) does not need this additional stage because it has enough capacity to learn both wordregion alignment and MLM together in a single stage. Pre-training dataTable 5 reports the last checkpoint results on <cur>GLIPv2</cur> when we do the scaling up of pre-training data. As more weak image-text pair data (Cap) is involved in our training, it benefits both standard/in-domain (i.e., COCO, Flickr30K) and large-domain gap (i.e., ODinW, LVIS) tasks. We also show that by adding the inter-image region-word contrastive helps when we are fixing the data at the same scale. For large-domain gap tasks, adding the inter-image region-word contrastive</p>
<hr />
<p>Row Model COCO ODinW LVISFlickr30K VQA Captioning 1 No pre-train –/50.6 –/60.8 – – 64.6 111.5 2 +Lmlm –/48.5 –/37.4 – – 64.6 110.9 3 +Lloc+Lintra 46.6/55.2 46.5/64.9 26.0 85.7 69.4 119.7 4 +Lloc+Lintra+Lmlm 47.0/55.2 47.6/66.2 28.5 86.5 69.8 120.7 5 +Lloc+Lintra+Linter 47.1/55.4 48.4/66.3 28.6 85.8 68.7 120.4 6 +Lloc+Lintra+Linter+Lmlm47.3/55.5 48.5/66.5 29.0 86.3 70.7 122.1</p>
<p>Table 4: Pre-training losses on Tiny-scale model. Involving intra-image region-word alignment lossLintra, inter-image region-word contrastive lossLinterand MLM lossLmlmwill benefit both localization and understanding tasks.</p>
<p>Linter Pre-train Data COCO ODinW LVIS Flick30K 7 O365, GoldG 48.06 43.14 25.6 84.36 3 O365, GoldG 48.59 42.64 26.9 83.90 7 O365, GoldG, Cap4M 48.21 51.35 34.2 85.56 3 O365, GoldG, Cap4M 48.79 52.70 35.0 85.50 7 O365, GoldG, Cap12M 48.50 49.32 35.5 85.79 3 O365, GoldG, Cap12M 49.26 53.15 36.6 85.84</p>
<p>Table 5: Pre-train data scale up on Base-scale model. Results are reported at the last checkpoint. See supplementary for results at all checkpoints.</p>
<p>Model B4 CIDEr SPICECOCO Caption Flickr30K GroundingR@1 R@5 R@10</p>
<p><cur>GLIPv2</cur>-T 36.5 119.8 21.6 80.8 94.4 96.5 <cur>GLIPv2</cur>-B37.4 123.0 21.9 81.0 94.5 96.5</p>
<p>Table 6: <cur>GLIPv2</cur> can perform captioning and grounding at the same time (a.k.a., grounded VL understanding).</p>
<p>loss will further boost the model to learn better representation. For more detailed scaling-up effects on various tasks under all the checkpoints for GLIP and <cur>GLIPv2</cur>, refer to Appendix.</p>
<p>Note that the(Img,Text, T)data used in <cur>GLIPv2</cur> pre-training can be just human-annotated data (Row1&amp;2 in Table 5), with which <cur>GLIPv2</cur> pre-training does not involve any pseudo data from a pre-trained grounding/localization model. In order to achieve the best performance, <cur>GLIPv2</cur> uses image-text pair data with pseudo boxes (Cap) from a pre-trained GLIP model (Row3-6 in Table 4), which is trained with the same "grounded VL understanding" task but just with smaller data.</p>
<p>Grounded Vision-Language Understanding<cur>GLIPv2</cur> can be trained to perform a VL task and grounding at the same time (Section 3.3). We denote such an ability as grounded VL understanding. In Figure 1, we showcase grounded predictions of <cur>GLIPv2</cur> on VQA and COCO captions. We also conduct quantitative evaluations (Table 6). The model achieves strong performance for both VL understanding (on COCO Caption) and localization (on Flickr30K Grounding). Such an ability to produce high-level semantic outputs (i.e., answers and captions) and supporting localization results is another appealing trait of <cur>GLIPv2</cur>, as potential users can have a better understanding of the model behaviour. See more detailed analysis and qualitative examples in the Appendix.</p>
<h2 id="5-conclusion-and-social-impacts">5. Conclusion and Social Impacts</h2>
<p>This paper proposes <cur>GLIPv2</cur>, a unified framework for VL representation learning that serves both localization tasks and VL understanding tasks. We experimentally verify the effectiveness of the unified model and the novel region-word contrastive learning. Compared to existing methods, <cur>GLIPv2</cur> achieves competitive near SoTA performance on various localization and understanding tasks. However, additional analysis of the data and the model is necessary before deploying it in practice since large-scale web data may contain unintended private information, unsuitable images/text, or some bias leakage. Further investigation may be needed for web data due to the above issues.</p>
<h2 id="references">References 参考文献</h2>
<p>[1] Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and visual question answering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 6077–6086. IEEE (2018)
[2] Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and visual question answering. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 6077–6086 (2018)
[3] Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.: VQA: Visual Question Answering. In: International Conference on Computer Vision (ICCV) (2015) \
[4] Bansal, A., Sikka, K., Sharma, G., Chellappa, R., Divakaran, A.: Zero-shot object detection. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 384–400 (2018)
[5] Bilen, H., Vedaldi, A.: Weakly supervised deep detection networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2846–2854 (2016)
[6] Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., et al.: On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021)
[7] Bucher, M., Vu, T.H., Cord, M., Pérez, P.: Zero-shot semantic segmentation. Advances in Neural Information Processing Systems 32 (2019)
[8] Caesar, H., Uijlings, J., Ferrari, V.: Coco-stuff: Thing and stuff classes in context. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1209–1218 (2018)
[9] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end object detection with transformers. In: European Conference on Computer Vision. pp. 213–229. Springer (2020)
[10] Chen, K., Pang, J., Wang, J., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Shi, J., Ouyang, W., et al.: Hybrid task cascade for instance segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4974–4983 (2019)
[11] Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollár, P., Zitnick, C.L.: Microsoft COCO captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015)
[12] Chen, Y.C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z., Cheng, Y., Liu, J.: UNITER: Universal image-text representation learning. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 104–120. Springer (2020)
[13] Chen, Y.C., Li, L., Yu, L., Kholy, A.E., Ahmed, F., Gan, Z., Cheng, Y., Liu, J.: Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 (2019)</p>
<p>[14]Dai, J., Li, Y., He, K., Sun, J.: R-fcn: Object detection via region-based fully convolutional networks. In: Advances in neural information processing systems. pp. 379–387 (2016)</p>
<p>[15]Dai, X., Chen, Y., Xiao, B., Chen, D., Liu, M., Yuan, L., Zhang, L.: Dynamic head: Unifying object detection heads with attentions. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7373–7382 (2021)</p>
<p>[16]Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-Scale Hierarchical Image Database. In: Proceedings of the IEEE conference on computer vision and pattern recognition (2009)</p>
<p>[17]Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)</p>
<p>[18]Gao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y., Li, H., Qiao, Y.: Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544 (2021)</p>
<p>[19]Gokberk Cinbis, R., Verbeek, J., Schmid, C.: Multi-fold mil training for weakly supervised object localization. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2409–2416 (2014)</p>
<hr />
<p>[20]Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 6904–6913 (2017)</p>
<p>[21]Gu, X., Lin, T.Y., Kuo, W., Cui, Y.: Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921 (2021)</p>
<p>[22]Gu, X., Lin, T.Y., Kuo, W., Cui, Y.: Zero-shot detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921 (2021)</p>
<p>[23]Gupta, A., Dollar, P., Girshick, R.: Lvis: A dataset for large vocabulary instance segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5356–5364 (2019)</p>
<p>[24]Gupta, T., Kamath, A., Kembhavi, A., Hoiem, D.: Towards general purpose vision systems. arXiv preprint arXiv:2104.00743 (2021)</p>
<p>[25]He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377 (2021)</p>
<p>[26]He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask r-cnn. In: Proceedings of the IEEE international conference on computer vision. pp. 2961–2969 (2017)</p>
<p>[27]Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016)</p>
<p>[28]Hu, R., Singh, A.: Unit: Multimodal multitask learning with a unified transformer. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1439–1449 (2021)</p>
<p>[29]Hudson, D.A., Manning, C.D.: Gqa: A new dataset for real-world visual reasoning and compositional question answering. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 6700–6709 (2019)</p>
<p>[30]Kamath, A., Singh, M., LeCun, Y., Synnaeve, G., Misra, I., Carion, N.: Mdetr-modulated detection for end-to-end multi-modal understanding. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1780–1790 (2021)</p>
<p>[31]Karpathy, A., Joulin, A., Fei-Fei, L.F.: Deep fragment embeddings for bidirectional image sentence mapping. Advances in neural information processing systems 27 (2014)</p>
<p>[32]Kiros, R., Salakhutdinov, R., Zemel, R.S.: Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539 (2014)</p>
<p>[33]Krasin, I., Duerig, T., Alldrin, N., Ferrari, V., Abu-El-Haija, S., Kuznetsova, A., Rom, H., Uijlings, J., Popov, S., Veit, A., et al.: Openimages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from <a href="https://github">https://github</a>. com/openimages 2 (3), 18 (2017)</p>
<p>[34]Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual Genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision (IJCV) 123 (1), 32–73 (2017)</p>
<p>[35]Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems 25 , 1097–1105 (2012)</p>
<p>[36]Lester, B., Al-Rfou, R., Constant, N.: The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 (2021)</p>
<p>[37]Li, G., Duan, N., Fang, Y., Jiang, D., Zhou, M.: Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training. arXiv preprint arXiv:1908.06066 (2019)</p>
<p>[38]Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H.: Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems 34 (2021)</p>
<p>[39]Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 (2019)</p>
<p>[40]Li, L.H., You, H., Wang, Z., Zareian, A., Chang, S.F., Chang, K.W.: Unsupervised vision-andlanguage pre-training without parallel images and captions. arXiv preprint arXiv:2010.12831 (2020)</p>
<hr />
<p>[41]Li, L.H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L., Zhang, L., Hwang, J.N., et al.: Grounded language-image pre-training. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10965–10975 (2022)</p>
<p>[42]Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., et al.: Oscar: Object-semantics aligned pre-training for vision-language tasks. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 121–137. Springer (2020)</p>
<p>[43]Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P.: Focal loss for dense object detection. In: Proceedings of the IEEE international conference on computer vision. pp. 2980–2988 (2017)</p>
<p>[44]Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference on computer vision. pp. 740–755. Springer (2014)</p>
<p>[45]Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021)</p>
<p>[46]Lu, J., Batra, D., Parikh, D., Lee, S.: ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In: Advances in Neural Information Processing Systems (NeurIPS). pp. 13–23 (2019)</p>
<p>[47]Lu, J., Clark, C., Zellers, R., Mottaghi, R., Kembhavi, A.: Unified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916 (2022)</p>
<p>[48]Ma, C.Y., Kalantidis, Y., AlRegib, G., Vajda, P., Rohrbach, M., Kira, Z.: Learning to generate grounded visual captions without localization supervision. In: European Conference on Computer Vision. pp. 353–370. Springer (2020)</p>
<p>[49]Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose estimation. In: European conference on computer vision. pp. 483–499. Springer (2016)</p>
<p>[50]Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., Lazebnik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In: Proceedings of the IEEE international conference on computer vision. pp. 2641– 2649 (2015)</p>
<p>[51]Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning (ICML) (2021)</p>
<p>[52]Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 779–788 (2016)</p>
<p>[53]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems 28 , 91–99 (2015)</p>
<p>[54]Shao, S., Li, Z., Zhang, T., Peng, C., Yu, G., Zhang, X., Li, J., Sun, J.: Objects365: A large-scale, high-quality dataset for object detection. In: Proceedings of the IEEE international conference on computer vision. pp. 8430–8439 (2019)</p>
<p>[55]Shin, T., Razeghi, Y., Logan IV, R.L., Wallace, E., Singh, S.: Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980 (2020)</p>
<p>[56]Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.: VL-BERT: Pre-training of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530 (2019)</p>
<p>[57]Tan, H., Bansal, M.: Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490 (2019)</p>
<p>[58]Teney, D., Anderson, P., He, X., Van Den Hengel, A.: Tips and tricks for visual question answering: Learnings from the 2017 challenge. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4223–4232 (2018)</p>
<p>[59]Tian, Z., Shen, C., Chen, H., He, T.: Fcos: Fully convolutional one-stage object detection. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 9627–9636 (2019)</p>
<hr />
<p>[60]Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. In: Advances in neural information processing systems. pp. 5998–6008 (2017)</p>
<p>[61]Wang, P., Cai, Z., Yang, H., Swaminathan, G., Vasconcelos, N., Schiele, B., Soatto, S.: Omnidetr: Omni-supervised object detection with transformers. arXiv preprint arXiv:2203.16089 (2022)</p>
<p>[62]Wang, X., Huang, T.E., Darrell, T., Gonzalez, J.E., Yu, F.: Frustratingly simple few-shot object detection. arXiv preprint arXiv:2003.06957 (2020)</p>
<p>[63]Wu, C., Lin, Z., Cohen, S., Bui, T., Maji, S.: Phrasecut: Language-based image segmentation in the wild. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10216–10225 (2020)</p>
<p>[64]Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., Liu, T.: On layer normalization in the transformer architecture. In: International Conference on Machine Learning. pp. 10524–10533. PMLR (2020)</p>
<p>[65]Yang, J., Li, C., Zhang, P., Xiao, B., Liu, C., Yuan, L., Gao, J.: Unified contrastive learning in image-text-label space. arXiv preprint arXiv:2204.03610 (2022)</p>
<p>[66]Yang, Z., Gan, Z., Wang, J., Hu, X., Ahmed, F., Liu, Z., Lu, Y., Wang, L.: Crossing the format boundary of text and boxes: Towards unified vision-language modeling. arXiv preprint arXiv:2111.12085 (2021)</p>
<p>[67]Yuan, L., Chen, D., Chen, Y.L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., et al.: Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432 (2021)</p>
<p>[68]Zareian, A., Rosa, K.D., Hu, D.H., Chang, S.F.: Open-vocabulary object detection using captions. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14393–14402 (2021)</p>
<p>[69]Zeng, Y., Zhang, X., Li, H.: Multi-grained vision language pre-training: Aligning texts with visual concepts. arXiv preprint arXiv:2111.08276 (2021)</p>
<p>[70]Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., Gao, J.: Vinvl: Revisiting visual representations in vision-language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5579–5588 (2021)</p>
<p>[71]Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., Gao, J.: Vinvl: Revisiting visual representations in vision-language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 5579–5588 (June 2021)</p>
<p>[72]Zhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li, L.H., Zhou, L., Dai, X., Yuan, L., Li, Y., et al.: Regionclip: Region-based language-image pretraining. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16793–16803 (2022)</p>
<p>[73]Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language models. arXiv preprint arXiv:2109.01134 (2021)</p>
<p>[74]Zhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J., Gao, J.: Unified vision-language pre-training for image captioning and vqa. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 13041–13049 (2020)</p>
<p>[75]Zhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J.J., Gao, J.: Unified vision-language pretraining for image captioning and VQA. AAAI (2020)</p>
<p>[76]Zhu, P., Wang, H., Saligrama, V.: Zero shot detection. IEEE Transactions on Circuits and Systems for Video Technology 30 (4), 998–1010 (2019)</p>
<p>[77]Zhu, P., Wang, H., Saligrama, V.: Don’t even look once: Synthesizing features for zero-shot detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11693–11702 (2020)</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.16e2a7d4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.d6c3db9e.min.js"></script>
      
        <script src="../../../javascripts/mathjac.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>