
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-8.5.8">
    
    
      
        <title>Adaptive Self-Supervision Algorithms for Physics-Informed Neural Networks 物理信息神经网络的自适应自监督算法 - Nanxi Gu</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.20d9efc8.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.815d1a91.min.css">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../paper.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#adaptive-self-supervision-algorithms-for-physics-informed-neural-networks" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Nanxi Gu" class="md-header__button md-logo" aria-label="Nanxi Gu" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Nanxi Gu
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Adaptive Self-Supervision Algorithms for Physics-Informed Neural Networks <br> 物理信息神经网络的自适应自监督算法
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Nanxi Gu" class="md-nav__button md-logo" aria-label="Nanxi Gu" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Nanxi Gu
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        模型
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../Scholars/" class="md-nav__link">
        学者
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../Books/" class="md-nav__link">
        书籍
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../Courses/" class="md-nav__link">
        课程
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../Projects/" class="md-nav__link">
        项目
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract 摘要
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    1. Introduction 介绍 
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-related-work" class="md-nav__link">
    2. Related Work 相关工作
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-methods" class="md-nav__link">
    3. Methods 方法
  </a>
  
    <nav class="md-nav" aria-label="3. Methods 方法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#resampling-collocation-points" class="md-nav__link">
    Resampling Collocation Points 重采样配置点
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptive-sampling" class="md-nav__link">
    Adaptive Sampling 自适应采样
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#progressive-adaptive-sampling" class="md-nav__link">
    Progressive Adaptive Sampling 渐进自适应采样
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-experiments" class="md-nav__link">
    4. Experiments 实验
  </a>
  
    <nav class="md-nav" aria-label="4. Experiments 实验">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-2d-poissons-equation" class="md-nav__link">
    4.1. 2D Poisson's Equation 二维泊松方程
  </a>
  
    <nav class="md-nav" aria-label="4.1. 2D Poisson's Equation 二维泊松方程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-formulation" class="md-nav__link">
    Problem Formulation 问题形式化
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-2d-diffusion-advection-equation" class="md-nav__link">
    4.2. 2D Diffusion-Advection Equation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-conclusions" class="md-nav__link">
    5. Conclusions 结论
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    Reference 参考文献
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="adaptive-self-supervision-algorithms-for-physics-informed-neural-networks">Adaptive Self-Supervision Algorithms for Physics-Informed Neural Networks <br> 物理信息神经网络的自适应自监督算法</h1>
<ul>
<li>作者: Shashank Subramanian | Robert M. Kirby | Michael W. Mahoney | Amir Gholami</li>
<li>机构:</li>
<li>时间: 2022-07-08</li>
<li>预印: <a href="http://arxiv.org/abs/2207.04084">arXiv:2207.04084v1</a></li>
<li>领域:</li>
<li>标签: #PINN #开源</li>
<li>引用: 29 篇 (自引 1 篇)</li>
<li>代码: <a href="https://github.com/ShashankSubramanian/adaptive-selfsupervision-pinns">Github</a></li>
</ul>
<style>
    cur{
        color:red;
    }
    model{
        text-decoration: underline;
        text-decoration-color: orange;
    }
    term{
        text-decoration: underline;
        text-decoration-color: purple;
    }
</style>

<h2 id="abstract">Abstract 摘要</h2>
<blockquote>
<p><model>Physics-informed neural networks (PINNs)</model> incorporate physical knowledge from the problem domain as a soft constraint on the loss function, but recent work has shown that this can lead to optimization difficulties. Here, we study the impact of the location of the collocation points on the trainability of these models. We find that the <model>vanilla PINN</model> performance can be significantly boosted by adapting the location of the collocation points as training proceeds. Specifically, we propose a novel adaptive collocation scheme which progressively allocates more collocation points (without increasing their number) to areas where the model is making higher errors (based on the gradient of the loss function in the domain). This, coupled with a judicious restarting of the training during any optimization stalls (by simply resampling the collocation points in order to adjust the loss landscape) leads to better estimates for the prediction error. We present results for several problems, including a <term>2D Poisson</term> and <term>diffusion-advection</term> system with different forcing functions. We find that training <model>vanilla PINNs</model> for these problems can result in up to 70% prediction error in the solution, especially in the regime of low collocation points. In contrast, our adaptive schemes can achieve up to an order of magnitude smaller error, with similar computational complexity as the baseline. Furthermore, we find that the adaptive methods consistently perform on-par or slightly better than <model>vanilla PINN</model> method, even for large collocation point regimes. The code for all the experiments has been open sourced and available at Github.</p>
</blockquote>
<h2 id="1-introduction">1. Introduction 介绍 <span id='sec1'></span></h2>
<p>A key aspect that distinguishes scientific ML (SciML) [4,10,14,16,23,28] from other ML tasks is that scientists typically know a great deal about the underlying physical processes that generate their data. For example, while the Ordinary Differential Equations (ODEs) or Partial Differential Equations (PDEs) used to simulate the physical phenomena may not capture every detail of a physical system,they often provide a reasonably good approximation. In some cases, we know that physical systems have to obey conservation laws (mass, energy, momentum, etc.). In other cases, we can learn these constraints, either exactly or approximately, from the data. In either case, the main challenge in SciML lies in combining such scientific prior domain-driven knowledge with large-scale data-driven methods from ML in a principled manner.
One popular method to incorporate scientific prior knowledge is to incorporate them as soft-constraints throughout training, as proposed with Physics Informed Neural Networks (PINNs) [10,12,17]. These models use penalty method techniques from optimization [3] and formulate the solution of the PDE as an unconstrained optimization problem that minimizes a self-supervision loss function that incorporates the domain physics (PDEs) as a penalty (regularization) term. Formulating the problem as a soft-constraint makes it very easy to use existing auto-differentiation frameworks for SciML tasks. However, training PINNs this way can be very difficult, and it is often difficult to solve the optimization problem [6,11,25]. This could be partly because the self-supervision term typically contains complex terms such as (higher-order) derivatives of spatial functions and other nonlinearities that cause the loss term to become ill-conditioned [11]. This is very different than unit‘pball or other such convex functions, more commonly used as regularization terms in ML. Several solutions such as loss scaling [25], curriculum or sequence-to-sequence learning [11], tuning of loss function weights [13], and novel network architectures [19] have been proposed to address this problem.
One overlooked, but very important, parameter of the training process is the way that the self-supervision is performed in PINNs, and in particular which data points in the domain are used for enforcing the physical constraints (commonly referred to as collocation points in numerical analysis). In the original work of [17],the collocation points are randomly sampled in the beginning of the training and kept constant throughout the learning process. However, we find that this is sub-optimal, and instead we propose adaptive collocation schemes. We show that these schemes can result in an order of magnitude better performance, with similar computational overhead.</p>
<p>Background. We focus on scientific systems that have a PDE constraint of the following form:(1.1)F(u(x)) = 0,x ∈ Ω ⊂ Rd,whereFis a differential operator representing the PDE,u(x) is the state variable (i.e., physical quantity of interest), Ω is the physical domain, andxrepresents spatial domain (2D in all of our results). To ensure existence and uniqueness of an analytical solution, there are additional constraints specified on the boundary,dΩ, as well (such as periodic or Dirichlet boundary conditions). One possible approach to learn a representation for the solution is to incorporate the PDE as a hard constraint, and formulate a loss function that measures the prediction error on the boundary (where data points are available):(1.2)minθL(u)s.t.F(u) = 0,whereL(u) is typically a data mismatch term (this includes initial/boundary conditions but can also include observational data points), and whereFis a constraint on the residual of the PDE system (i.e.,F(u) is the residual) under consideration. Since constrained optimization is typically more difficult than unconstrained optimization [3], this constraint is typically relaxed and added as a penalty term to the loss function. This yields the following unconstrained optimization problem, namely the PINNs (soft constrained) optimization problem:(1.3)minθL(u) + λFLF.
In this problem formulation, the regularization parameter,λF, controls the weight given to the PDE constraints, as compared to the data misfit term;θdenotes the parameters of the model that predictsu(x),which is often taken to be a neural network; and the PDE loss functional term,LF, can be considered as a self-supervised loss, as all the information comes from the PDE system we are interested in simulating,instead of using direct observations. Typically a Euclidean loss function is used to measure the residual of the PDE for this loss function,kF(u)k22, where the‘2norm is computed at discrete points (collocation points) that are randomly sampled from Ω. This loss term is often the source of the training difficulty with PINNs [6, 11], which is the focus of our paper.
Main contributions. Unlike other work in the literature, which has focused on changing the training method or the neural network (NN) architecture, here we focus on the self-supervision component of PINNs,and specifically on the selection of the collocation points. In particular, we make the following contributions:•We study the role of the collocation points for two PDE systems: steady state diffusion (Poisson);and diffusion-advection. We find that keeping the collocation points constant throughout training is a sub-optimal strategy and is an important source of the training difficulty with PINNs. This is particularly true for cases where the PDE problem exhibits local behaviour (e.g., in presence of sharp, or very localized, features).•We propose an alternative strategy of resampling the collocation points when training stalls. Although this strategy is simple, it can lead to significantly better reconstruction (see Tab. 1 and Fig. 2). Im-portantly, this approach does not increase the computational complexity, and it is easy to implement.•We propose to improve the basic resampling scheme with a gradient-based adaptive scheme. This adaptive scheme is designed to help to relocate the collocation points to areas with higher loss gradient,without increasing the total number of points (see Algorithm 1 for the algorithm). In particular,we progressively relocate the points to areas of high gradient as training proceeds. This is done through a cosine-annealing that gradually changes the sampling of collocation points from uniform to adaptive through training. We find that this scheme consistently achieves better performance than the basic resampling method, and it can lead to more than 10x improvements in the prediction error(see Tab. 1, Fig. 2).•We extensively test our adaptive schemes for the two PDE systems of Poisson and diffusion-advection while varying the number of collocation points for problems with both smooth or sharp features.
While the resampling and adaptive schemes perform similarly in the large collocation point regime,the adaptive approach shows significant improvement when the number of collocation points is small and the forcing function is sharp (see Tab. 2).</p>
<h2 id="2-related-work">2. Related Work 相关工作</h2>
<blockquote>
<p>There has been a large body of work studying <model>PINNs</model> [5,7,9,18,20,21,30] and the challenges associated with their training [6,11,24,25,26,27]. The work of [25] notes these challenges and proposes a loss scaling method to resolve the training difficulty. Similar to this approach, some works have treated the problem as a multi-objective optimization and tune the weights of the different loss terms[2,29]. A more formal approach was suggested in [13] where the weights are learned by solving a minimax optimization problem that ascends in the loss weight space and descends in the model parameter space. This approach was extended in [15] to shift the focus of the weights from the loss terms to the training data points instead, and the minimax forces the optimization to pay attention to specific regions of the domain. However, minimax optimization problems are known to be hard to optimize and introduce additional complexity and computational expense. Furthermore, it has been shown that using curriculum or sequence-to-sequence learning can ameliorate the training difficulty with <model>PINNs</model> [11]. More recently, the work of [24] shows that incorporating causality in time can help training for time-dependent PDEs.</p>
<p>There is also recent work that studies the role of the collocation points. For instance, [22] refines the collocation point set without learnable weights. They propose an auxiliary NN that acts as a generative model to sample new collocation points that mimic the PDE residual. However, the auxiliary network also has to be trained in tandem with the PINN. The work of [14] proposes an adaptive collocation scheme where the points are densely sampled uniformly and trained for some number of iterations. Then the set is extended by adding points in increasing rank order of PDE residuals to refine in certain locations (of sharp fronts, for example) and the model is retrained. However, this method can increase the computational overhead, as the number of collocation points is progressively increased. Furthermore, in <sup id="fnref:8"><a class="footnote-ref" href="#fn:8">1</a></sup> the authors show that the latter approach can lead to excessive clustering of points throughout training. To address this, instead they propose to add points based on an underlying density function defined by the PDE residual. Both these schemes keep the original collocation set (the low residual points) and increase the training dataset sizes as the optimization proceeds.</p>
<p>Unlike the work of [8,14], we focus on using gradient of the loss function, instead of the nominal loss value, as the proxy to guide the adaptive resampling of the collocation points. We show that this approach leads to better localization of the collocation points, especially for problems with sharp features. Furthermore, we incorporate a novel cosine-annealing scheme, which progressively incorporates adaptive sampling as training proceeds. Importantly, we also keep the number of collocation points the same. Not only does this not increase the computational overhead, but this is also easier to implement as well．</p>
</blockquote>
<h2 id="3-methods">3. Methods 方法</h2>
<blockquote>
<p>In <model>PINNs</model>, we use a feedforward NN, denoted <span class="arithmatex">\(NN(\pmb{x};\theta)\)</span>, that is parameterized by weights and biases, <span class="arithmatex">\(\theta\)</span>, takes as input values for coordinate points, <span class="arithmatex">\(\pmb{x}\)</span>, and outputs the solution value <span class="arithmatex">\(u(\pmb{x})\in\mathbb{R}\)</span> at these points. As described in <a href="#sec1">Section 1</a>, the model parameters <span class="arithmatex">\(\theta\)</span> are optimized through the loss function:</p>
</blockquote>
<p>在 PINNs 中, 我们使用一个有权重和偏差 <span class="arithmatex">\(\theta\)</span> 参数化的前馈神经网络 <span class="arithmatex">\(NN(\pmb{x};\theta)\)</span>, 输入值为配置点 <span class="arithmatex">\(\pmb{x}\)</span> 并输出解在这些点上的值 <span class="arithmatex">\(u(\pmb{x})\in\mathbb{R}\)</span>. 正如第一节所描述的, 模型参数 <span class="arithmatex">\(\theta\)</span> 通过以下损失函数进行优化:</p>
<div class="arithmatex">\[
    \min_{\theta}\mathcal{L}_{\mathcal{B}} + \lambda_{\mathcal{F}}\mathcal{L}_{\mathcal{F}}. \tag{3.1}
\]</div>
<blockquote>
<p>We focus on boundary-value (steady state) problems and define the two loss terms as: <span id='eq3.2'></span></p>
</blockquote>
<p>我们主要关注边值 (稳态) 问题并定义相应的两个损失项如下:</p>
<div class="arithmatex">\[
    \mathcal{L}_{\mathcal{B}} =\frac{1}{n_b} \sum_{i=1}^{n_b}\|u(\pmb{x}_b^i)-\hat{u}(\pmb{x}_b^i)\|_2^2, \tag{3.2a}
\]</div>
<div class="arithmatex">\[
    \mathcal{L}_{\mathcal{F}} =\frac{1}{n_c} \sum_{i=1}^{n_c}\|\mathcal{F}(u(\pmb{x}_c^i))\|_2^2, \tag{3.2b}
\]</div>
<blockquote>
<p>where <span class="arithmatex">\(u\)</span> is the model predicted solution, <span class="arithmatex">\(\hat{u}\)</span> is the true solution or data, <span class="arithmatex">\(\pmb{x}_b^i\)</span> are points on the boundary, and <span class="arithmatex">\(\pmb{x}_c^i\)</span> are collocation points uniformly sampled from the domain <span class="arithmatex">\(\Omega\)</span>. Here, <span class="arithmatex">\(n_b\)</span> and <span class="arithmatex">\(n_c\)</span> are the number of boundary and collocation points, respectively; and the boundary loss term <span class="arithmatex">\(\mathcal{L}_{\mathcal{B}}\)</span> implements a Dirichlet boundary condition, where we assume that the solution values are known on the boundary <span class="arithmatex">\(d\Omega\)</span>.</p>
</blockquote>
<p>其中 <span class="arithmatex">\(u\)</span> 是模型预测解, <span class="arithmatex">\(\hat{u}\)</span> 是真解或数据, <span class="arithmatex">\(\pmb{x}_b^i\)</span> 是边界上的点, <span class="arithmatex">\(\pmb{x}_c^i\)</span> 是从定义域 <span class="arithmatex">\(\Omega\)</span> 中均匀采样的配置点.式子中的 <span class="arithmatex">\(n_b\)</span> 和 <span class="arithmatex">\(n_c\)</span> 分别表示边界点和配置点的数量, 并且边界损失项 <span class="arithmatex">\(\mathcal{L}_{\mathcal{B}}\)</span> 使用了 Dirichlet 边界条件, 即我们假设解在边界 <span class="arithmatex">\(d\Omega\)</span> 上的值已知.</p>
<blockquote>
<p>In <model>PINNs</model><sup id="fnref:17"><a class="footnote-ref" href="#fn:17">2</a></sup>, the collocation points used in <a href="#eq3.2">Eq.3.2b</a> are randomly sampled with a uniform probability over the entire space <span class="arithmatex">\(\Omega\)</span> in the beginning of training and then kept constant afterwards (we refer to this approach as Baseline). While a uniformly distributed collocation point set may be sufficient for simple <term>PDEs</term> with smooth features, we find them to be sub-optimal when the problem exhibits sharp/local features, or even fail to train. To address this, we propose the following schemes.</p>
</blockquote>
<p>在 PINNs 中, 上述公式使用的配置点是在训练开始是在整个空间 <span class="arithmatex">\(\Omega\)</span> 上根据均匀分布进行随机采样的, 然后在之后保持不变 (我们将这一方法作为基线). 虽然对于有光滑性质的简单偏微分方程来说, 一个均匀分布的配置点集可能已经足够了, 我们发现当问题出现 sharp/局部性时, 可能会出现次优解甚至训练失败. 为了解决这一问题, 我们提出如下方案.</p>
<h3 id="resampling-collocation-points">Resampling Collocation Points 重采样配置点</h3>
<blockquote>
<p>Current <model>PINNs</model> are typically optimized with <term>LBFGS</term>. In our experiments, we found that in the baseline approach <term>LBFGS</term> often fails to find a descent direction and training stalls, even after hyperparamter tuning. This agrees with other results reported in the literature [6,11,25]. We find that this is partially due to the fact that the collocation points are kept constant and not changed. The simplest approach to address this is to resample the collocation points when <term>LBFGS</term> stalls. We refer to this approach as Resampling. As we will discuss in the next section, we find this approach to be helpful for cases with moderate to large number of collocation points.</p>
</blockquote>
<h3 id="adaptive-sampling">Adaptive Sampling 自适应采样</h3>
<blockquote>
<p>While the Resampling method is effective for large number of collocation points, we found it to be sub-optimal in the small collocation point regime, especially for problems with sharp/localized features. In this case, the Resampling method still uses a uniform distribution with which to sample the new the collocation points; and, in the presence of a small number of collocation points and/or sharp/localized features, this is not an optimal allocation of the points. Ideally, we want to find a probability distribution, as a replacement for the uniform distribution, that can improve trainability of PINNs for a given number of collocation points and/or computational budget. There are several possibilities to define this distribution. The first intuitive approach would be to use the value of the PDE residual (Eq. 3.2b), and normalize it as a probability distribution. This could then be used to sample collocation points based on the loss values in the domain. That is, more points would be sampled in areas with with higher PDE residual, and vice versa. We refer to this approach as <cur>ADAPTIVE-R</cur> sampling (withRreferring to the PDE residual). An alternative is to use the gradient of the loss function (PDE loss termLF) w.r.t. the spatial grid, using that as the probability distribution with which to sample the collocation points. We refer to this approach as <cur>ADAPTIVE-G</cur> (withG referring to gradient of the loss). As we will show in the next section, when combined with a cosine annealing scheme (discussed next), both of these adaptive schemes consistently perform better than the Resampling scheme.</p>
</blockquote>
<h3 id="progressive-adaptive-sampling">Progressive Adaptive Sampling 渐进自适应采样</h3>
<blockquote>
<p>Given the non-convex nature of the problem, it might be important to not overly constrain the NN to be too locally focused, as the optimization procedure could get stuck in a local minima. However, this can be addressed by progressively incorporating adaptive sampling as training proceeds. In particular, we use a cosine annealing strategy. This allows the NN to periodically alternate between focusing on regions of high error as well as uniformly covering larger parts of the sample space, over a period of iterations. Specifically, in each annealing period, we start with a full uniform sampling, and progressively incorporate adaptively sampled points using a cosine schedule rule of <span class="arithmatex">\(\eta=\dfrac{1}{2}(1+\cos\pi\dfrac{T_c}{T})\)</span>, where <span class="arithmatex">\(\eta\)</span> is the fraction of points uniformly sampled, <span class="arithmatex">\(T_c\)</span> is the number of epochs since the last restart, and <span class="arithmatex">\(T\)</span> is the length of the cosine schedule. We use this schedule to resample the collocation every <span class="arithmatex">\(e\)</span> epochs. Given the periodic nature of the cosine annealing, that approach balances local adaptivity without losing global information. We outline the adaptive sampling <a href="#alg1">Algorithm 1</a>.</p>
</blockquote>
<table>
<thead>
<tr>
<th><span id='alg1'>Algorithm 1. Adaptive Sampling for Self-Supervision in PINNs</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Require</strong>: Loss <span class="arithmatex">\(\mathcal{L}\)</span>, NN model, number of collocation points <span class="arithmatex">\(n_c\)</span>, PDE regularization <span class="arithmatex">\(\lambda_{\mathcal{F}}\)</span>, <span class="arithmatex">\(T\)</span>, <span class="arithmatex">\(s_w\)</span>, momentum <span class="arithmatex">\(\gamma\)</span>, max epochs <span class="arithmatex">\(i_{max}\)</span></td>
</tr>
<tr>
<td>1: <span class="arithmatex">\(i\leftarrow 0\)</span><br>2: <strong>while</strong> <span class="arithmatex">\(i\leq i_{max}\)</span> <strong>do</strong><br>3: <span class="arithmatex">\(\qquad\)</span>Compute proxy function as the loss gradient (<cur>ADAPTIVE-G</cur>) or PDE residual (<cur>ADAPTIVE-R</cur>)<br>4: <span class="arithmatex">\(\qquad\)</span>Current proxy <span class="arithmatex">\(\mathcal{P}_i\leftarrow\mathcal{P} + \gamma\mathcal{P}_{i-1}\)</span><br>5: <span class="arithmatex">\(\qquad\)</span><span class="arithmatex">\(T_c\leftarrow i \mod T\)</span><br>6: <span class="arithmatex">\(\qquad\)</span><span class="arithmatex">\(\eta \leftarrow\)</span> cosine-schedule<span class="arithmatex">\((T_c, T)\)</span><br>7: <span class="arithmatex">\(\qquad\)</span><strong>if</strong> <span class="arithmatex">\(i \mod e\)</span> is true <strong>then</strong><br>8: <span class="arithmatex">\(\qquad\qquad\)</span>Sample <span class="arithmatex">\(\eta n_c\)</span> points <span class="arithmatex">\(\pmb{x}_u\)</span> uniformly<br>9: <span class="arithmatex">\(\qquad\qquad\)</span>Sample <span class="arithmatex">\((1 - \eta)n_c\)</span> points <span class="arithmatex">\(\pmb{x}_a\)</span> using proxy function <span class="arithmatex">\(\mathcal{P}_i\)</span><br>10:<span class="arithmatex">\(\qquad\)</span><strong>end if</strong><br>11:<span class="arithmatex">\(\qquad\)</span><span class="arithmatex">\(\pmb{x}_c\leftarrow \pmb{x}_u\cup \pmb{x}_a\)</span><br>12:<span class="arithmatex">\(\qquad\)</span>Input <span class="arithmatex">\(\pmb{x}\leftarrow\pmb{x}_b\cup\pmb{x}_c\)</span> where <span class="arithmatex">\(\pmb{x}_b\)</span> are boundary points<br>13:<span class="arithmatex">\(\qquad\)</span><span class="arithmatex">\(u \leftarrow NN(\pmb{x}; \theta)\)</span><br>14:<span class="arithmatex">\(\qquad\)</span><span class="arithmatex">\(\mathcal{L} \leftarrow \mathcal{L}_\mathcal{B}+ \lambda_\mathcal{F}\mathcal{L}_\mathcal{F}\)</span><br>15:<span class="arithmatex">\(\qquad\)</span><span class="arithmatex">\(\theta \leftarrow\)</span> optimizer-update<span class="arithmatex">\((\theta, \mathcal{L})\)</span><br>16:<span class="arithmatex">\(\qquad\)</span><strong>if</strong> stopping-criterion is true <strong>then</strong><br>17:<span class="arithmatex">\(\qquad\qquad\)</span>reset cosine scheduler <span class="arithmatex">\(T_c\leftarrow 0\)</span><br>18:<span class="arithmatex">\(\qquad\)</span><strong>end if</strong><br>19:<span class="arithmatex">\(\qquad\)</span><span class="arithmatex">\(i \leftarrow i + 1\)</span><br>20: <strong>end while</strong></td>
</tr>
</tbody>
</table>
<h2 id="4-experiments">4. Experiments 实验</h2>
<p>In this section, we show examples that highlight the prediction error improvement using our adaptive self-supervision method on two PDE systems: Poisson’s equation §4.1 and (steady state)diffusion-advection §4.2.Problem setup. We use the same problem set up as in [11,17,25] for the NN model, which is a feed forward model with hyperbolic tangent activation function, trained with LBFGS optimizer. We focus on2D spatial domains in Ω = [0,1]2. The training data set contains points that are randomly sampled from a uniform 2562mesh on Ω. The testing data set is the set of all 2562points, along with the true solution of the PDEˆu(x) at these points. Furthermore, we use a constant regularization parameter ofλF= 1e-4 for all the runs, which we found to be a good balance between the boundary term and the PDE residual. To ensure a fair comparison, we tune the rest of the hyperparameters both for the baseline model as well as for the adaptive schemes. For tuning the parameters, we use the validation loss computed by calculating the total loss over randomly sampled points in the domain (30K points for all the experiments). Note that we do not use any signal from the analytical solution, and instead only use the loss in Eq. 3.1.Metrics. We train the optimizer for 5000 epochs with a stall criterion when the loss does not change for 10 epochs. We keep track of the minimum validation loss in order to get the final model parameters for testing. We compute our prediction error using two metrics: the‘2relative error,µ1=ku − ˆuk2/kˆukand the‘1errorµ2=ku − ˆuk1, whereuis the model prediction solution from the best validation loss epoch andˆuis the true solution. All runs are trained on an A100 NVIDIA GPU on the Perlmutter supercomputer.</p>
<h3 id="41-2d-poissons-equation">4.1. 2D Poisson's Equation 二维泊松方程</h3>
<h4 id="problem-formulation">Problem Formulation 问题形式化</h4>
<blockquote>
<p>We consider a prototypical elliptic system defined by the Poisson’s equation with source function f(x). This system represents a steady state diffusion equation:</p>
</blockquote>
<div class="arithmatex">\[
(4.1)− div K∇u = f(x),x ∈ Ω,
\]</div>
<p>whereKdenotes the diffusion tensor. For homogeneous diffusion tensors, the solution of the poisson’s equation with doubly-periodic boundary conditions can be computed using the fast Fourier transform on a discrete grid as:(4.2)ˆu = F−1�−1−(k2xk11+ k2yk22+ 2kxkyk12)F(f(x))�,whereFis the Fourier transform,kx, kyare the frequencies in the Fourier domain, andk11, k22, k12are the diagonal and off-diagonal coefficients of the diffusion tensorK. We enforce the boundary conditions as Dirichlet boundary conditions using the true solution to circumvent the ill-posedness of the doubly-periodic Poisson’s equation.1We use a Gaussian function as the source with standard deviationσfand consider the diffusion tensork11= 1, k22= 8, k12= 4 to make the problem anisotropic. We consider two test-cases:(i) TC1: smooth source function withσf= 0.1 (ii) TC2: sharp source function withσf= 0.01. We show these source functions and the corresponding target solutions in Fig. 1. For any experiment, we sweep over all the hyperparameters and select the ones that show the lowest/best validation loss for the final model.
Observations. We start by examining the performance of the difference methods for the relatively small number of collocation points ofnc= 1,000 (which corresponds to 1.5% of the 256×256 domain Ω). We report the errorsµ1andµ2for the different schemes in Tab. 1. We also visualize the predicted solution for each method in Fig. 2, along with the location of the collocation points overlayed for the final model. We can clearly see that the baseline model does not converge (despite hyperparameter tuning) due to the optimizer stalls. However, Resampling achieves significantly better errors, especially for the smooth source function setup (TC1). However, for the sharp source experiment (TC2), the resampling does not help and shows an error of about 50%.Overall, the adaptive schemes show much better (or comparable) prediction errors for both test-cases.
In particular, ADAPTIVE-R and ADAPTIVE-G achieve about 2–5% relative error (µ1) for both TC1 and TC2. The visual reconstruction shown in Fig. 2 also shows the clearly improved prediction with the adaptive schemes (last two columns), as compared to the baseline with/without resampling (first two columns). Note that the ADAPTIVE-G method assigns more collocation points around the sharp features. This is due to the fact that it uses the gradient information for its probability distribution, instead of the residual of the PDE which is used in ADAPTIVE-R method. To show the effect of the scheduler, we show an ablation study with the cosine-annealing scheduler in the appendix (see Fig. A.4).We then repeat the same experiment, but now varying the number of collocation points,nc, from 500to 8K, and we report the relative error (µ1) in Tab. 2. We observe a consistent trend, where the Baseline(PINN training without resampling) does not converge to a good solution, whereas ADAPTIVE-G consistently achieves up to an order of magnitude better error. Also, note that the performance of the Resampling method significantly improves and becomes on par with ADAPTIVE-G as we increasenc. This is somewhat expected since at large number of collocation points resampling will have the chance to sample points near the areas with sharp features. In Fig. A.1, we show the errors for every test-case as a function of number of collocation points using 10 different random seed values to quantify the variance in the different methods.
We observe that the adaptive schemes additionally show smaller variances across seeds, especially for the test-cases with sharp sources.</p>
<h3 id="42-2d-diffusion-advection-equation">4.2. 2D Diffusion-Advection Equation</h3>
<p>We next look at a steady-state 2D diffusion-advection equation
with source function f(x), diffusion tensor K, and velocity vector v:</p>
<p>For homogeneous diffusion tensors and velocity vectors, the solution of the advection-diffusion equation with
doubly-periodic boundary conditions can also be computed using the fast Fourier transform on a discrete
grid as:
ˆu = F−1�−1
g1− g2
F(f(x))�,
g1= −(k2xk11+ k2yk22+ 2kxkyk12),
g2= ikxv1+ ikyv2,
(4.4)
whereFis the Fourier transform,i=√−1,kx, kyare the frequencies in the Fourier domain,k11, k22, k12are
the diagonal and off-diagonal coefficients of the diffusion tensorK, andv1, v2are the velocity components.</p>
<p>As before, we enforce the boundary conditions as Dirichlet boundary conditions using the true solution,and we consider a Gaussian function as the source with standard deviationσf. We use a diffusion tensor k11= 1, k22= 8, k12= 4 and velocity vectorv1= 40, v2= 10 to simulate sufficient advection. We consider two test-cases as before: (i) TC3: smooth source function withσf= 0.1 (ii) TC4: sharp source function with σf= 0.01. We show the source functions and the target solutions in Fig. 3.Observations. We observe a very similar behaviour for the reconstruction errors as in the previous experiments. As before, we start withnc= 1,000 collocation points, and we report the results in Tab. 3and Fig. 4. Here, the baseline achieves slightly better performance for TC3 (14%) but completely fails (100%error) for the TC4 test case, which includes a sharper forcing function. However, the Resampling achieves better results for both cases. Furthermore, the best performance is achieved by the adaptive methods.
Finally, in Tab. 4 we report the relative errors for the baseline and adaptive schemes with various numbers of collocation pointsnc. Similar to the Poisson system, larger values ofncshow good performance, but the baselines underperform in the low data regime for sharp sources. The resampling achieves better errors, while the adaptive methods once again consistently achieve the best performance for both data regimes.</p>
<h2 id="5-conclusions">5. Conclusions 结论</h2>
<blockquote>
<p>We studied the impact of the location of the collocation points in training SciML models, focusing on the popular class of PINN models, and we showed that the vanilla PINN strategy of keeping the collocation points fixed throughout training often results in suboptimal solutions. This is particularly the case for PDE systems with sharp (or very localized) features. We showed that a simple strategy of resampling collocation points during optimization stalls can significantly improve the reconstruction error, especially for moderately large number of collocation points. We also proposed adaptive collocation schemes to obtain a better allocation of the collocation points. This is done by constructing a probability distribution derived from either the PDE residual (<cur>ADAPTIVE-R</cur>) or its gradient w.r.t. the input (<cur>ADAPTIVE-G</cur>). We found that by progressively incorporating the adaptive schemes, we can achieve up to an order of magnitude better solutions, as compared to the baseline, especially for the regime of a small number of collocation points and with problems that exhibit sharp (or very localized) features. Some limitations of this current work include the following: we did not change the NN architecture (it was fixed as a feed forward NN) or tune hyperparameters relating to the architecture (which can be a significant factor in any analysis); and we only focused on 2D spatial systems (it is known that time-dependent or 3D or higher-dimensional systems can show different behaviours and may benefit from different kinds of adaptivity in space and time). We leave these directions to future work. However, we expect that techniques such as those we used here that aim to combine in a more principled way domain-driven scientific methods and data-driven ML methods will help in these cases as well.</p>
</blockquote>
<h2 id="reference">Reference 参考文献</h2>
<p>[2] Rafael Bischof and Michael Kraus. Multi-Objective Loss Balancing for Physics-Informed Deep Learning.
arXiv preprint arXiv:2110.09813, 2021.</p>
<p>[3] (Book) Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge university press, 2004.</p>
<p>[4] Steven L Brunton, Bernd R Noack, and Petros Koumoutsakos. Machine learning for fluid mechanics.
Annual Review of Fluid Mechanics, 52:477–508, 2020.</p>
<p>[5] Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Optics express, 28(8):11618–11633, 2020.</p>
<p>[6] C. Edwards. Neural networks learn to speed up simulations. Communications of the ACM, 65(5):27–29,2022.</p>
<p>[7] Nicholas Geneva and Nicholas Zabaras. Modeling the dynamics of pde systems with physics-constrained deep auto-regressive networks. Journal of Computational Physics, 403:109056, 2020.</p>
<p>[9] Xiaowei Jin, Shengze Cai, Hui Li, and George Em Karniadakis. Nsfnets (navier-stokes flow nets): Physics-informed neural networks for the incompressible navier-stokes equations. Journal of Computational Physics, 426:109951, 2021.</p>
<p>[10] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang.
Physics-informed machine learning. Nature Reviews Physics, 3(6):422–440, 2021.</p>
<p>[11] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks. NeurIPS 2021.</p>
<p>[12] Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. IEEE transactions on neural networks, 9(5):987–1000, 1998.</p>
<p>[13] Dehao Liu and Yan Wang. A dual-dimer method for training physics-constrained neural networks with minimax architecture. Neural Networks, 136:112–125, 2021.</p>
<p>[14] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library for solving differential equations. SIAM Review, 63(1):208–228, 2021.</p>
<p>[15] Levi McClenny and Ulisses Braga-Neto. Self-adaptive physics-informed neural networks using a soft attention mechanism. arXiv preprint arXiv:2009.04544, 2020.</p>
<p>[16] Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar, Dominic Skinner, Ali Ramadhan, and Alan Edelman. Universal differential equations for scientific machine learning. arXiv preprint arXiv:2001.04385, 2020.</p>
<p>[18] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science, 367(6481):1026–1030, 2020.</p>
<p>[19] Amuthan A Ramabathiran and Prabhu Ramachandran. Spinn: Sparse, physics-based, and partially interpretable neural networks for pdes. Journal of Computational Physics, 445:110600, 2021.</p>
<p>[20] Francisco Sahli Costabal, Yibo Yang, Paris Perdikaris, Daniel E Hurtado, and Ellen Kuhl. Physics-informed neural networks for cardiac activation mapping. Frontiers in Physics, 8:42, 2020.</p>
<p>[21] Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial differential equations. Journal of computational physics, 375:1339–1364, 2018.</p>
<p>[22] Kejun Tang, Xiaoliang Wan, and Chao Yang. Das: A deep adaptive sampling method for solving partial differential equations. arXiv preprint arXiv:2112.14038, 2021.</p>
<p>[23] Laura von Rueden, Sebastian Mayer, Katharina Beckh, Bogdan Georgiev, Sven Giesselbach, Raoul Heese, Birgit Kirsch, Julius Pfrommer, Annika Pick, Rajkumar Ramamurthy, et al. Informed machine learning–a taxonomy and survey of integrating knowledge into learning systems. arXiv preprint arXiv:1903.12394,2019.</p>
<p>[24] Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality is all you need for training physics-informed neural networks. arXiv preprint arXiv:2203.07404, 2022. Adaptive Self-supervision Algorithms for Physics-informed Neural Networks1.</p>
<p>[25] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient pathologies in physics-informed neural networks. arXiv preprint arXiv:2001.04536, 2020.</p>
<p>[26] Sifan Wang, Hanwen Wang, and Paris Perdikaris. On the eigenvector bias of fourier feature networks:From regression to solving multi-scale pdes with physics-informed neural networks. arXiv preprint arXiv:2012.10047, 2020.</p>
<p>[27] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel perspective. arXiv preprint arXiv:2007.14527, 2020.</p>
<p>[28] Jared Willard, Xiaowei Jia, Shaoming Xu, Michael Steinbach, and Vipin Kumar. Integrating physics-based modeling with machine learning: A survey. arXiv preprint arXiv:2003.04919, 2020.</p>
<p>[29] Zixue Xiang, Wei Peng, Xiaohu Zheng, Xiaoyu Zhao, and Wen Yao. Self-adaptive loss balanced physics-informed neural networks for the incompressible navier-stokes equations. arXiv preprint arXiv:2104.06217,2021.</p>
<p>[30] Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Journal of Computational Physics, 394:56–81, 2019.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:8">
<p>John Hanna, Jose V Aguado, Sebastien Comas-Cardona, Ramzi Askri, and Domenico Borzacchiello.
Residual-based adaptivity for two-phase flow simulation in porous media using physics-informed neural networks. arXiv preprint arXiv:2109.14290, 2021.&#160;<a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:17">
<p>Maziar Raissi, Paris Perdikaris, and George E Karniadakis. <a href="../PN-JCP201810045/">Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations</a>. JCP 2019.&#160;<a class="footnote-backref" href="#fnref:17" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.16e2a7d4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.d6c3db9e.min.js"></script>
      
        <script src="../../../javascripts/mathjac.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>