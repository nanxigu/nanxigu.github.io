{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gu Nanxi","title":"\u9996\u9875"},{"location":"#gu-nanxi","text":"","title":"Gu Nanxi"},{"location":"Models/","text":"\u6a21\u578b PINNs \u7cfb\u5217 2022.07 Adaptive Self-Supervision Algorithms for PINNs A Comprehensive Study of Non-Adaptive and Residual-Based Adaptive Sampling for PINNs 2022.10 Failure-Informed Adaptive Sampling for PINNs A Novel Adaptive Causal Sampling Method for PINNs","title":"\u6a21\u578b"},{"location":"Models/#_1","text":"","title":"\u6a21\u578b"},{"location":"Models/#pinns","text":"","title":"PINNs \u7cfb\u5217"},{"location":"Models/#202207","text":"Adaptive Self-Supervision Algorithms for PINNs A Comprehensive Study of Non-Adaptive and Residual-Based Adaptive Sampling for PINNs","title":"2022.07"},{"location":"Models/#202210","text":"Failure-Informed Adaptive Sampling for PINNs A Novel Adaptive Causal Sampling Method for PINNs","title":"2022.10"},{"location":"Songs/","text":"\u6b4c\u8bcd\u8bb0\u5f55 \u5e7e\u7530\u308a\u3089 - \u30ed\u30de\u30f3\u30b9\u306e\u7d04\u675f [ChiliChill - \u534a\u9192]","title":"\u6b4c\u8bcd"},{"location":"Songs/#_1","text":"\u5e7e\u7530\u308a\u3089 - \u30ed\u30de\u30f3\u30b9\u306e\u7d04\u675f [ChiliChill - \u534a\u9192]","title":"\u6b4c\u8bcd\u8bb0\u5f55"},{"location":"Concepts/JL%20Lemma/","text":"Johnson-Lindenstrauss Lemma","title":"Johnson-Lindenstrauss Lemma"},{"location":"Concepts/JL%20Lemma/#johnson-lindenstrauss-lemma","text":"","title":"Johnson-Lindenstrauss Lemma"},{"location":"Concepts/Space/","text":"\u7a7a\u95f4 \u5411\u91cf\u7a7a\u95f4 \u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4","title":"\u7a7a\u95f4"},{"location":"Concepts/Space/#_1","text":"","title":"\u7a7a\u95f4"},{"location":"Concepts/Space/#_2","text":"","title":"\u5411\u91cf\u7a7a\u95f4"},{"location":"Concepts/Space/#_3","text":"","title":"\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4"},{"location":"Models/PINNs/PN-2207.04084/","text":"Adaptive Self-Supervision Algorithms for Physics-Informed Neural Networks \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u9002\u5e94\u81ea\u76d1\u7763\u7b97\u6cd5 \u4f5c\u8005: Shashank Subramanian | Robert M. Kirby | Michael W. Mahoney | Amir Gholami \u673a\u6784: \u65f6\u95f4: 2022-07-08 \u9884\u5370: arXiv:2207.04084v1 \u9886\u57df: \u6807\u7b7e: #PINN #\u5f00\u6e90 \u5f15\u7528: 29 \u7bc7 \u4ee3\u7801: Github Abstract \u6458\u8981 Physics-informed neural networks (PINNs) incorporate physical knowledge from the problem domain as a soft constraint on the loss function, but recent work has shown that this can lead to optimization difficulties. Here, we study the impact of the location of the collocation points on the trainability of these models. We find that the vanilla PINN performance can be significantly boosted by adapting the location of the collocation points as training proceeds. Specifically, we propose a novel adaptive collocation scheme which progressively allocates more collocation points (without increasing their number) to areas where the model is making higher errors (based on the gradient of the loss function in the domain). This, coupled with a judicious restarting of the training during any optimization stalls (by simply resampling the collocation points in order to adjust the loss landscape) leads to better estimates for the prediction error. We present results for several problems, including a 2D Poisson and diffusion-advection system with different forcing functions. We find that training vanilla PINNs for these problems can result in up to 70% prediction error in the solution, especially in the regime of low collocation points. In contrast, our adaptive schemes can achieve up to an order of magnitude smaller error, with similar computational complexity as the baseline. Furthermore, we find that the adaptive methods consistently perform on-par or slightly better than vanilla PINN method, even for large collocation point regimes. The code for all the experiments has been open sourced and available at Github. 1. Introduction \u4ecb\u7ecd 2. Related Work \u76f8\u5173\u5de5\u4f5c There has been a large body of work studying PINNs [5,7,9,18,20,21,30] and the challenges associated with their training [6,11,24\u201327]. The work of [25] notes these challenges and proposes a loss scaling method to resolve the training difficulty. Similar to this approach, some works have treated the problem as a multi-objective optimization and tune the weights of the different loss terms[2,29]. A more formal approach was suggested in [13] where the weights are learned by solving a minimax\u3000optimization problem that ascends in the loss weight space and descends in the model parameter space. This approach was extended in [15] to shift the focus of the weights from the loss terms to the training data points instead, and the minimax forces the optimization to pay attention to specific regions of the domain. However,minimax optimization problems are known to be hard to optimize and introduce additional complexity and computational expense. Furthermore, it has been shown that using curriculum or sequence-to-sequence learning can ameliorate the training difficulty with PINNs [11]. More recently, the work of [24] shows that incorporating causality in time can help training for time-dependent PDEs. There is also recent work that studies the role of the collocation points. For instance, [22] refines the collocation point set without learnable weights. They propose an auxiliary NN that acts as a generative model to sample new collocation points that mimic the PDE residual. However, the auxiliary network also has to be trained in tandem with the PINN. The work of [14] proposes an adaptive collocation scheme where the points are densely sampled uniformly and trained for some number of iterations. Then the set is extended by adding points in increasing rank order of PDE residuals to refine in certain locations (of sharp fronts,for example) and the model is retrained. However, this method can increase the computational overhead,as the number of collocation points is progressively increased. Furthermore, in 1 the authors show that the latter approach can lead to excessive clustering of points throughout training. To address this, instead they propose to add points based on an underlying density function defined by the PDE residual. Both these schemes keep the original collocation set (the low residual points) and increase the training dataset sizes as the optimization proceeds. Unlike the work of [8,14], we focus on using gradient of the loss function, instead of the nominal loss value,as the proxy to guide the adaptive resampling of the collocation points. We show that this approach leads to better localization of the collocation points, especially for problems with sharp features. Furthermore, we incorporate a novel cosine-annealing scheme, which progressively incorporates adaptive sampling as training proceeds. Importantly, we also keep the number of collocation points the same. Not only does this not increase the computational overhead, but this is also easier to implement as well\uff0e 3. Methods \u65b9\u6cd5 In PINNs, we use a feedforward NN, denoted \\(NN(\\pmb{x};\\theta)\\) , that is parameterized by weights and biases, \\(\\theta\\) , takes as input values for coordinate points, \\(\\pmb{x}\\) , and outputs the solution value \\(u(\\pmb{x})\\in\\mathbb{R}\\) at these points. As described in Section 1, the model parameters \\(\\theta\\) are optimized through the loss function: \\[ \\min_{\\theta}\\mathcal{L}_{\\mathcal{B}} + \\lambda_{\\mathcal{F}}\\mathcal{L}_{\\mathcal{F}}. \\tag{3.1} \\] 4. Experiments \u5b9e\u9a8c 5. Conclusions \u7ed3\u8bba Reference \u53c2\u8003\u6587\u732e Rafael Bischof and Michael Kraus. Multi-objective loss balancing for physics-informed deep learning. arXiv preprint arXiv:2110.09813, 2021. [3] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004. [4] Steven L Brunton, Bernd R Noack, and Petros Koumoutsakos. Machine learning for fluid mechanics. Annual Review of Fluid Mechanics, 52:477\u2013508, 2020. [5] Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Optics express, 28(8):11618\u201311633, 2020. [6] C. Edwards. Neural networks learn to speed up simulations. Communications of the ACM, 65(5):27\u201329,2022. [7] Nicholas Geneva and Nicholas Zabaras. Modeling the dynamics of pde systems with physics-constrained deep auto-regressive networks. Journal of Computational Physics, 403:109056, 2020. [9] Xiaowei Jin, Shengze Cai, Hui Li, and George Em Karniadakis. Nsfnets (navier-stokes flow nets): Physics-informed neural networks for the incompressible navier-stokes equations. Journal of Computational Physics, 426:109951, 2021. [10] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422\u2013440, 2021. [11] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Charac-terizing possible failure modes in physics-informed neural networks. Advances in Neural Information Processing Systems, 34, 2021. [12] Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. IEEE transactions on neural networks, 9(5):987\u20131000, 1998. [13] Dehao Liu and Yan Wang. A dual-dimer method for training physics-constrained neural networks with minimax architecture. Neural Networks, 136:112\u2013125, 2021. [14] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library for solving differential equations. SIAM Review, 63(1):208\u2013228, 2021. [15] Levi McClenny and Ulisses Braga-Neto. Self-adaptive physics-informed neural networks using a soft attention mechanism. arXiv preprint arXiv:2009.04544, 2020. [16] Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar,Dominic Skinner, Ali Ramadhan, and Alan Edelman. Universal differential equations for scientific machine learning. arXiv preprint arXiv:2001.04385, 2020. [17] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686\u2013707, 2019. [18] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science, 367(6481):1026\u20131030, 2020. [19] Amuthan A Ramabathiran and Prabhu Ramachandran. Spinn: Sparse, physics-based, and partially interpretable neural networks for pdes. Journal of Computational Physics, 445:110600, 2021. [20] Francisco Sahli Costabal, Yibo Yang, Paris Perdikaris, Daniel E Hurtado, and Ellen Kuhl. Physics-informed neural networks for cardiac activation mapping. Frontiers in Physics, 8:42, 2020. [21] Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial differential equations. Journal of computational physics, 375:1339\u20131364, 2018. [22] Kejun Tang, Xiaoliang Wan, and Chao Yang. Das: A deep adaptive sampling method for solving partial differential equations. arXiv preprint arXiv:2112.14038, 2021. [23] Laura von Rueden, Sebastian Mayer, Katharina Beckh, Bogdan Georgiev, Sven Giesselbach, Raoul Heese,Birgit Kirsch, Julius Pfrommer, Annika Pick, Rajkumar Ramamurthy, et al. Informed machine learning\u2013a taxonomy and survey of integrating knowledge into learning systems. arXiv preprint arXiv:1903.12394,2019. [24] Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality is all you need for training physics-informed neural networks. arXiv preprint arXiv:2203.07404, 2022.Adaptive Self-supervision Algorithms for Physics-informed Neural Networks1. [25] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient pathologies in physics-informed neural networks. arXiv preprint arXiv:2001.04536, 2020. [26] Sifan Wang, Hanwen Wang, and Paris Perdikaris. On the eigenvector bias of fourier feature networks:From regression to solving multi-scale pdes with physics-informed neural networks. arXiv preprint arXiv:2012.10047, 2020. [27] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel perspective. arXiv preprint arXiv:2007.14527, 2020. [28] Jared Willard, Xiaowei Jia, Shaoming Xu, Michael Steinbach, and Vipin Kumar. Integrating physics-based modeling with machine learning: A survey. arXiv preprint arXiv:2003.04919, 2020. [29] Zixue Xiang, Wei Peng, Xiaohu Zheng, Xiaoyu Zhao, and Wen Yao. Self-adaptive loss balanced physics-informed neural networks for the incompressible navier-stokes equations. arXiv preprint arXiv:2104.06217,2021. [30] Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Journal of Computational Physics, 394:56\u201381, 2019. John Hanna, Jose V Aguado, Sebastien Comas-Cardona, Ramzi Askri, and Domenico Borzacchiello. Residual-based adaptivity for two-phase flow simulation in porous media using physics-informed neural networks. arXiv preprint arXiv:2109.14290, 2021. \u21a9","title":"Adaptive Self-Supervision Algorithms for Physics-Informed Neural Networks <br> \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u9002\u5e94\u81ea\u76d1\u7763\u7b97\u6cd5"},{"location":"Models/PINNs/PN-2207.04084/#adaptive-self-supervision-algorithms-for-physics-informed-neural-networks","text":"\u4f5c\u8005: Shashank Subramanian | Robert M. Kirby | Michael W. Mahoney | Amir Gholami \u673a\u6784: \u65f6\u95f4: 2022-07-08 \u9884\u5370: arXiv:2207.04084v1 \u9886\u57df: \u6807\u7b7e: #PINN #\u5f00\u6e90 \u5f15\u7528: 29 \u7bc7 \u4ee3\u7801: Github","title":"Adaptive Self-Supervision Algorithms for Physics-Informed Neural Networks  \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u9002\u5e94\u81ea\u76d1\u7763\u7b97\u6cd5"},{"location":"Models/PINNs/PN-2207.04084/#abstract","text":"Physics-informed neural networks (PINNs) incorporate physical knowledge from the problem domain as a soft constraint on the loss function, but recent work has shown that this can lead to optimization difficulties. Here, we study the impact of the location of the collocation points on the trainability of these models. We find that the vanilla PINN performance can be significantly boosted by adapting the location of the collocation points as training proceeds. Specifically, we propose a novel adaptive collocation scheme which progressively allocates more collocation points (without increasing their number) to areas where the model is making higher errors (based on the gradient of the loss function in the domain). This, coupled with a judicious restarting of the training during any optimization stalls (by simply resampling the collocation points in order to adjust the loss landscape) leads to better estimates for the prediction error. We present results for several problems, including a 2D Poisson and diffusion-advection system with different forcing functions. We find that training vanilla PINNs for these problems can result in up to 70% prediction error in the solution, especially in the regime of low collocation points. In contrast, our adaptive schemes can achieve up to an order of magnitude smaller error, with similar computational complexity as the baseline. Furthermore, we find that the adaptive methods consistently perform on-par or slightly better than vanilla PINN method, even for large collocation point regimes. The code for all the experiments has been open sourced and available at Github.","title":"Abstract \u6458\u8981"},{"location":"Models/PINNs/PN-2207.04084/#1-introduction","text":"","title":"1. Introduction \u4ecb\u7ecd"},{"location":"Models/PINNs/PN-2207.04084/#2-related-work","text":"There has been a large body of work studying PINNs [5,7,9,18,20,21,30] and the challenges associated with their training [6,11,24\u201327]. The work of [25] notes these challenges and proposes a loss scaling method to resolve the training difficulty. Similar to this approach, some works have treated the problem as a multi-objective optimization and tune the weights of the different loss terms[2,29]. A more formal approach was suggested in [13] where the weights are learned by solving a minimax\u3000optimization problem that ascends in the loss weight space and descends in the model parameter space. This approach was extended in [15] to shift the focus of the weights from the loss terms to the training data points instead, and the minimax forces the optimization to pay attention to specific regions of the domain. However,minimax optimization problems are known to be hard to optimize and introduce additional complexity and computational expense. Furthermore, it has been shown that using curriculum or sequence-to-sequence learning can ameliorate the training difficulty with PINNs [11]. More recently, the work of [24] shows that incorporating causality in time can help training for time-dependent PDEs. There is also recent work that studies the role of the collocation points. For instance, [22] refines the collocation point set without learnable weights. They propose an auxiliary NN that acts as a generative model to sample new collocation points that mimic the PDE residual. However, the auxiliary network also has to be trained in tandem with the PINN. The work of [14] proposes an adaptive collocation scheme where the points are densely sampled uniformly and trained for some number of iterations. Then the set is extended by adding points in increasing rank order of PDE residuals to refine in certain locations (of sharp fronts,for example) and the model is retrained. However, this method can increase the computational overhead,as the number of collocation points is progressively increased. Furthermore, in 1 the authors show that the latter approach can lead to excessive clustering of points throughout training. To address this, instead they propose to add points based on an underlying density function defined by the PDE residual. Both these schemes keep the original collocation set (the low residual points) and increase the training dataset sizes as the optimization proceeds. Unlike the work of [8,14], we focus on using gradient of the loss function, instead of the nominal loss value,as the proxy to guide the adaptive resampling of the collocation points. We show that this approach leads to better localization of the collocation points, especially for problems with sharp features. Furthermore, we incorporate a novel cosine-annealing scheme, which progressively incorporates adaptive sampling as training proceeds. Importantly, we also keep the number of collocation points the same. Not only does this not increase the computational overhead, but this is also easier to implement as well\uff0e","title":"2. Related Work \u76f8\u5173\u5de5\u4f5c"},{"location":"Models/PINNs/PN-2207.04084/#3-methods","text":"In PINNs, we use a feedforward NN, denoted \\(NN(\\pmb{x};\\theta)\\) , that is parameterized by weights and biases, \\(\\theta\\) , takes as input values for coordinate points, \\(\\pmb{x}\\) , and outputs the solution value \\(u(\\pmb{x})\\in\\mathbb{R}\\) at these points. As described in Section 1, the model parameters \\(\\theta\\) are optimized through the loss function: \\[ \\min_{\\theta}\\mathcal{L}_{\\mathcal{B}} + \\lambda_{\\mathcal{F}}\\mathcal{L}_{\\mathcal{F}}. \\tag{3.1} \\]","title":"3. Methods \u65b9\u6cd5"},{"location":"Models/PINNs/PN-2207.04084/#4-experiments","text":"","title":"4. Experiments \u5b9e\u9a8c"},{"location":"Models/PINNs/PN-2207.04084/#5-conclusions","text":"","title":"5. Conclusions \u7ed3\u8bba"},{"location":"Models/PINNs/PN-2207.04084/#reference","text":"Rafael Bischof and Michael Kraus. Multi-objective loss balancing for physics-informed deep learning. arXiv preprint arXiv:2110.09813, 2021. [3] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004. [4] Steven L Brunton, Bernd R Noack, and Petros Koumoutsakos. Machine learning for fluid mechanics. Annual Review of Fluid Mechanics, 52:477\u2013508, 2020. [5] Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Optics express, 28(8):11618\u201311633, 2020. [6] C. Edwards. Neural networks learn to speed up simulations. Communications of the ACM, 65(5):27\u201329,2022. [7] Nicholas Geneva and Nicholas Zabaras. Modeling the dynamics of pde systems with physics-constrained deep auto-regressive networks. Journal of Computational Physics, 403:109056, 2020. [9] Xiaowei Jin, Shengze Cai, Hui Li, and George Em Karniadakis. Nsfnets (navier-stokes flow nets): Physics-informed neural networks for the incompressible navier-stokes equations. Journal of Computational Physics, 426:109951, 2021. [10] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422\u2013440, 2021. [11] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Charac-terizing possible failure modes in physics-informed neural networks. Advances in Neural Information Processing Systems, 34, 2021. [12] Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. IEEE transactions on neural networks, 9(5):987\u20131000, 1998. [13] Dehao Liu and Yan Wang. A dual-dimer method for training physics-constrained neural networks with minimax architecture. Neural Networks, 136:112\u2013125, 2021. [14] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library for solving differential equations. SIAM Review, 63(1):208\u2013228, 2021. [15] Levi McClenny and Ulisses Braga-Neto. Self-adaptive physics-informed neural networks using a soft attention mechanism. arXiv preprint arXiv:2009.04544, 2020. [16] Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar,Dominic Skinner, Ali Ramadhan, and Alan Edelman. Universal differential equations for scientific machine learning. arXiv preprint arXiv:2001.04385, 2020. [17] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686\u2013707, 2019. [18] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science, 367(6481):1026\u20131030, 2020. [19] Amuthan A Ramabathiran and Prabhu Ramachandran. Spinn: Sparse, physics-based, and partially interpretable neural networks for pdes. Journal of Computational Physics, 445:110600, 2021. [20] Francisco Sahli Costabal, Yibo Yang, Paris Perdikaris, Daniel E Hurtado, and Ellen Kuhl. Physics-informed neural networks for cardiac activation mapping. Frontiers in Physics, 8:42, 2020. [21] Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial differential equations. Journal of computational physics, 375:1339\u20131364, 2018. [22] Kejun Tang, Xiaoliang Wan, and Chao Yang. Das: A deep adaptive sampling method for solving partial differential equations. arXiv preprint arXiv:2112.14038, 2021. [23] Laura von Rueden, Sebastian Mayer, Katharina Beckh, Bogdan Georgiev, Sven Giesselbach, Raoul Heese,Birgit Kirsch, Julius Pfrommer, Annika Pick, Rajkumar Ramamurthy, et al. Informed machine learning\u2013a taxonomy and survey of integrating knowledge into learning systems. arXiv preprint arXiv:1903.12394,2019. [24] Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality is all you need for training physics-informed neural networks. arXiv preprint arXiv:2203.07404, 2022.Adaptive Self-supervision Algorithms for Physics-informed Neural Networks1. [25] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient pathologies in physics-informed neural networks. arXiv preprint arXiv:2001.04536, 2020. [26] Sifan Wang, Hanwen Wang, and Paris Perdikaris. On the eigenvector bias of fourier feature networks:From regression to solving multi-scale pdes with physics-informed neural networks. arXiv preprint arXiv:2012.10047, 2020. [27] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel perspective. arXiv preprint arXiv:2007.14527, 2020. [28] Jared Willard, Xiaowei Jia, Shaoming Xu, Michael Steinbach, and Vipin Kumar. Integrating physics-based modeling with machine learning: A survey. arXiv preprint arXiv:2003.04919, 2020. [29] Zixue Xiang, Wei Peng, Xiaohu Zheng, Xiaoyu Zhao, and Wen Yao. Self-adaptive loss balanced physics-informed neural networks for the incompressible navier-stokes equations. arXiv preprint arXiv:2104.06217,2021. [30] Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Journal of Computational Physics, 394:56\u201381, 2019. John Hanna, Jose V Aguado, Sebastien Comas-Cardona, Ramzi Askri, and Domenico Borzacchiello. Residual-based adaptivity for two-phase flow simulation in porous media using physics-informed neural networks. arXiv preprint arXiv:2109.14290, 2021. \u21a9","title":"Reference \u53c2\u8003\u6587\u732e"},{"location":"Models/PINNs/PN-2207.10289/","text":"A Comprehensive Study of Non-Adaptive and Residual-Based Adaptive Sampling for Physics-Informed Neural Networks \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u81ea\u9002\u5e94\u91c7\u6837\u548c\u57fa\u4e8e\u6b8b\u5dee\u7684\u81ea\u9002\u5e94\u91c7\u6837\u7684\u7efc\u5408\u7814\u7a76 \u4f5c\u8005: Chenxi Wu1,\u2020, Min Zhu1,\u2020, Qinyang Tan^2 , Yadhu Kartha^3 , and Lu Lu1,* \u673a\u6784: College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA \u65f6\u95f4: 2022-07-21 \u9884\u5370: arXiv:2207.10289v1 \u9886\u57df: physics.comp-ph \u6807\u7b7e: \u504f\u5fae\u5206\u65b9\u7a0b, \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc, \u6b8b\u5dee\u70b9\u5206\u5e03, \u975e\u81ea\u9002\u5e94\u5747\u5300\u91c7\u6837, \u5e26\u91cd\u91c7\u6837\u7684\u5747\u5300\u91c7\u6837, \u57fa\u4e8e\u6b8b\u5dee\u7684\u81ea\u9002\u5e94\u91c7\u6837 \u5f15\u7528: 47 \u7bc7 Abstract Physics-informed neural networks (PINNs) have shown to be an effective tool for solving both forward and inverse problems of partial differential equations (PDEs). PINNs embed the PDEs into the loss of the neural network using automatic differentiation, and this PDE loss is evaluated at a set of scattered spatio-temporal points (called residual points). The location and distribution of these residual points are highly important to the performance of PINNs. However, in the existing studies on PINNs, only a few simple residual point sampling methods have mainly been used. Here, we present a comprehensive study of two categories of sampling for PINNs: non-adaptive uniform sampling and adaptive nonuniform sampling. We consider six uniform sampling methods, including (1) equispaced uniform grid, (2) uniformly random sampling, (3) Latin hypercube sampling, (4) Halton sequence, (5) Hammersley sequence, and (6) Sobol sequence. We also consider a resampling strategy for uniform sampling. To improve the sampling efficiency and the accuracy of PINNs, we propose two new residual-based adaptive sampling methods: residual-based adaptive distribution (RAD) and residual-based adaptive refinement with distribution (RAR-D), which dynamically improve the distribution of residual points based on the PDE residuals during training. Hence, we have considered a total of 10 different sampling methods, including six non-adaptive uniform sampling, uniform sampling with resampling, two proposed adaptive sampling, and an existing adaptive sampling. We extensively tested the performance of these sampling methods for four forward problems and two inverse problems in many setups. Our numerical results presented in this study are summarized from more than 6000 simulations of PINNs. We show that the proposed adaptive sampling methods of RAD and RAR-D significantly improve the accuracy of PINNs with fewer residual points for both forward and inverse problems. The results obtained in this study can also be used as a practical guideline in choosing sampling methods. 1 Introduction Physics-informed neural networks (PINNs) [1] have emerged in recent years and quickly became a powerful tool for solving both forward and inverse problems of partial differential equations (PDEs) via deep neural networks (DNNs) [2, 3, 4]. PINNs embed the PDEs into the loss of the neural network using automatic differentiation. Compared with traditional numerical PDE solvers, such as the finite difference method (FDM) and the finite element method (FEM), PINNs are mesh free and therefore highly flexible. Moreover, PINNs can easily incorporate both physics-based constraints and data measurements into the loss function. PINNs have been applied to tackle diverse problems in computational science and engineering, such as inverse problems in nanooptics, metamaterials [5], and fluid dynamics [2], parameter estimation in systems biology [6, 7], and problems of inverse design and topology optimization [8]. In addition to standard PDEs, PINNs have also been extended to solve other types of PDEs, including integro-differential equations [3], fractional PDEs [9], and stochastic PDEs [10]. Despite the past success, addressing a wide range of PDE problems with increasing levels of complexity can be theoretically and practically challenging, and thus many aspects of PINNs still require further improvements to achieve more accurate prediction, higher computational efficiency, and training robustness [4]. A series of extensions to the vanilla PINN have been proposed to boost the performance of PINNs from various aspects. For example, better loss functions have been discovered via meta-learning [11], and gradient-enhanced PINNs (gPINNs) have been developed to embed the gradient information of the PDE residual into the loss [12]. In PINNs, the total loss is a weighted summation of multiple loss terms corresponding to the PDE and initial/boundary conditions, and different methods have been developed to automatically tune these weights and balance the losses [13, 14, 15]. Moreover, a different weight for each loss term could be set at every training point [16, 17, 8, 18]. For problems in a large domain, decomposition of the spatiotemporal domain accelerates the training of PINNs and improves their accuracy [19, 20, 21]. For time-dependent problems, it is usually helpful to first train PINNs within a short time domain and then gradually expand the time intervals of training until the entire time domain is covered [22, 23, 24, 25, 26]. In addition to these general methods, other problem-specific techniques have also been developed, e.g., enforcing Dirichlet or periodic boundary conditions exactly by constructing special neural network architectures [27, 28, 8]. PINNs are mainly optimized against the PDE loss, which guarantees that the trained network is consistent with the PDE to be solved. PDE loss is evaluated at a set of scattered residual points. Intuitively, the effect of residual points on PINNs is similar to the effect of mesh points on FEM, and thus the location and distribution of these residual points should be highly important to the performance of PINNs. However, in previous studies on PINNs, two simple residual point sampling methods (i.e., an equispaced uniform grid and uniformly random sampling) have mainly been used, and the importance of residual point sampling has largely been overlooked. 1.1 Related work and our contributions Different residual point sampling methods can be classified into two categories: uniform sampling and nonuniform sampling. Uniform sampling can be obtained in multiple ways. For example, we could use the nodes of an equispaced uniform grid as the residual points or randomly sample the points according to a continuous uniform distribution in the computational domain. Although these two sampling methods are simple and widely used, alternative sampling methods may be applied. The Latin hypercube sampling (LHS) [29, 30] was used in Ref. [1], and the Sobol sequence [31] was first used for PINNs in Ref. [9]. The Sobol sequence is one type of quasi random low-discrepancy sequences among other sequences, such as the Halton sequence [32], and the Hammersley sequence [33]. Low-discrepancy sequences usually perform better than uniformly distributed random numbers in many applications such as numerical integration; hence, a comprehensive comparison of these methods for PINNs is required. However, very few comparisons [34, 35] have been performed. In this study, we extensively compared the performance of different uniform sampling methods, including (1) equispaced uniform grid, (2) uniformly random sampling, (3) LHS, (4) Sobol sequence, (5) Halton sequence, and (6) Hammersley sequence. In supervised learning, the dataset is fixed during training, but in PINNs, we can select residual points at any location. Hence, instead of using the same residual points during training, in each optimization iteration, we could select a new set of residual points, as first emphasized in Ref. [3]. While this strategy has been used in some works, it has not yet been systematically tested. Thus, in this study, we tested the performance of such a resampling strategy and investigated the effect of the number of residual points and the resampling period for the first time. Uniform sampling works well for some simple PDEs, but it may not be efficient for those that are more complicated. To improve the accuracy, we could manually select the residual points in a nonuniform way, as was done in Ref. [36] for high-speed flows, but this approach is highly problem-dependent and usually tedious and time-consuming. In this study, we focus on automatic and adaptive nonuniform sampling. Motivated by the adaptive mesh refinement in FEM, Lu et al. [3] proposed the first adaptive nonuniform sampling for PINNs in 2019, the residual-based adaptive refinement (RAR) method, which adds new residual points in the locations with large PDE residuals. In 2021, another sampling strategy [37] was developed, where all the residual points were resampled according to a probability density function (PDF) proportional to the PDE residual. In this study, motivated by these two ideas, we proposed two new sampling strategies: residual-based adaptive distribution (RAD), where the PDF for sampling is a nonlinear func- tion of the PDE residual; residual-based adaptive refinement with distribution (RAR-D), which is a hybrid method of RAR and RAD, i.e., the new residual points are added according to a PDF. During the preparation of this paper, a few new studies appeared [38, 39, 40, 41, 42, 43, 44] that also proposed modified versions of RAR or PDF-based resampling. Most of these methods are special cases of the proposed RAD and RAR-D, and our methods can achieve better performance. We include a detailed comparison of these strategies in Section 2.4, after introducing several notations and our new proposed methods. In this study, we have considered a total of 10 different sampling methods, including seven non-adaptive sampling methods (six different uniform samplings and one uniform sampling with resampling) and three adaptive sampling approaches (RAR, RAD, and RAR-D). We compared the performance of these sampling methods for four forward problems of PDEs and investigated the effect of the number of residual points. We also compared their performance for two inverse problems that have not yet been consid- ered in the literature. We performed more than 6000 simulations of PINNs to obtain all the results shown in this study. 1.2 Organization This paper is organized as follows. In Section 2, after providing a brief overview of PINNs and different non-adaptive sampling strategies, two new adaptive nonuniform sampling strategies (RAD and RAR-D) are proposed. In Section 3, we compare the performance of 10 different methods for six different PDE problems, including four forward problems and two inverse problems. Section 4 summarizes the findings and concludes the paper. 2 Methods This section briefly reviews physics-informed neural networks (PINNs) in solving forward and inverse partial differential equations (PDEs). Then different types of uniformly sampling are introduced. Next, two nonuniform residual-based adaptive sampling methods are proposed to enhance the accuracy and training efficiency of PINNs. Finally, a comparison of related methods is presented. 2.1 PINNs in solving forward and inverse PDEs We consider the PDE parameterized by\u03bbdefined on a domain \u2126\u2282Rd, f(x;u(x)) =f ( x; \u2202u \u2202x 1 \u2202u \u2202xd \u2202^2 u \u2202x 1 \u2202x 1 \u2202^2 u \u2202x 1 \u2202xd ;...;\u03bb ) = 0, x= (x 1 ,...,xd)\u2208\u2126, with boundary conditions on\u2202\u2126 B(u,x) = 0, andu(x) denotes the solution atx. In PINNs, the initial condition is treated as the Dirichlet boundary condition. A forward problem is aimed to obtain the solutionu across the entire domain, where the model parameters\u03bbare known. In practice, the model parameters\u03bbmight be unknown, but some observations from the solutionuare available, which lead to an inverse problem. An inverse problem is aimed to discover parameters\u03bbthat best describe the observed data from the solution. PINNs are capable of addressing both forward and inverse problems. To solve a forward problem, the solutionuis represented with a neural network \u02c6u(x;\u03b8). The network parameters\u03b8are trained to approximate the solutionu, such that the loss function is minimized [1, 3]: L(\u03b8;T) =wfLf(\u03b8;Tf) +wbLb(\u03b8;Tb), where Lf(\u03b8;Tf) = 1 |Tf| \u2211 x\u2208Tf \u2223\u2223 \u2223\u2223f(x; \u2202u\u02c6 \u2202x 1 \u2202\u02c6u \u2202xd \u2202^2 u\u02c6 \u2202x 1 \u2202x 1 \u2202^2 u\u02c6 \u2202x 1 \u2202xd ;...;\u03bb) \u2223\u2223 \u2223\u2223 2 , (1) Lb(\u03b8;Tb) = 1 |Tb| \u2211 x\u2208Tb |B(\u02c6u,x)|^2 , andwfandwbare the weights. Two sets of points are samples both inside the domain (Tf) and on the boundaries (Tb). Here,TfandTbare referred to as the sets of \u201cresidual points\u201d, andT=Tf\u222aTb. To solve the inverse problem, an additional loss term corresponding to the misfit of the observed data at the locationsTi, defined as Li(\u03b8,\u03bb;Ti) = 1 |Ti| \u2211 x\u2208Ti |u\u02c6(x)\u2212u(x)|^2 , is added to the loss function. The loss function is then defined as L(\u03b8,\u03bb;T) =wfLf(\u03b8,\u03bb;Tf) +wbLb(\u03b8,\u03bb;Tb) +wiLi(\u03b8,\u03bb;Ti), with an additional weightwi. Then the network parameters\u03b8are trained simultaneously with\u03bb. For certain PDE problems, it is possible to enforce boundary conditions directly by constructing a special network architecture [27, 28, 8, 12], which eliminates the loss term of boundary conditions. In this study, the boundary conditions are enforced exactly and automatically. Hence, for a forward problem, the loss function is L(\u03b8,\u03bb;T) =Lf(\u03b8,\u03bb;Tf). For an inverse problem, the loss function is L(\u03b8,\u03bb;T) =wfLf(\u03b8,\u03bb;Tf) +wiLi(\u03b8,\u03bb;Ti), where we choosewf=wi= 1 for the diffusion-reaction equation in Section 3.6, andwf= 1,wi= 1000 for the Korteweg-de Vries equation in Section 3.7. 2.2 Uniformly-distributed non-adaptive sampling The training of PINNs requires a set of residual points (Tf). The sampling strategy ofTf plays a vital role in promoting the accuracy and computational efficiency of PINNs. Here, we discuss several sampling approaches. 2.2.1 Fixed residual points In most studies of PINNs, we specify the residual points at the beginning of training and never change them during the training process. Two simple sampling methods (equispaced uniform grids and uniformly random sampling) have been commonly used. Other sampling methods, such as the Latin hypercube sampling (LHS) [29, 30] and the Sobol sequence [31], have also been used in some studies [1, 9, 34]. The Sobol sequence is one type of quasi-random low-discrepancy sequences. Low-discrepancy sequences are commonly used as a replacement for uniformly distributed random numbers and usually perform better in many applications such as numerical integration. This study also considers other low-discrepancy sequences, including the Halton sequence [32] and the Hammersley sequence [33]. We list the six uniform sampling methods as follows, and the examples of 400 points generated in [0,1]^2 using different methods are shown in Fig. 1. Equispaced uniform grid (Grid): The residual points are chosen as the nodes of an equispaced uniform grid of the computational domain. Uniformly random sampling (Random): The residual points are randomly sampled according to a continuous uniform distribution over the domain. In practice, this is usually done using pseudo-random number generators such as the PCG-64 algorithm [45]. Latin hypercube sampling LHS : The LHS is a stratified Monte Carlo sampling method that generates random samples that occur within intervals on the basis of equal probability and with normal distribution for each range. Quasi-random low-discrepancy sequences: (a)Halton sequence Halton : The Halton samples are generated according to the reversing or flipping the base conversion of numbers using primes. (b)Hammersley sequence Hammersley : The Hammersley sequence is the same as the Halton sequence, except in the first dimension where points are located equidistant from each other. (c) Sobol sequence Sobol : The Sobol sequence is a base-2 digital sequence that fills in a highly uniform manner. Figure 1: Examples of 400 points generated in[0,1]^2 using different uniform sampling methods in Section 2.2.1. 2.2.2 Uniform points with resampling In PINNs, a point at any location can be used to evaluate the PDE loss. Instead of using the fixed residual points during training, we could also select a new set of residual points in every certain optimization iteration [3]. The specific method to sample the points each time can be chosen from those methods discussed in Section 2.2.1. We can even use different sampling methods at different times, so many possible implementations make it impossible to be completely covered in this study. In this study, we only consider Random sampling with resampling (Random-R). The RandomR method is the same as the Random method, except that the residual points are resampled for everyNiteration. Theresampling periodNis also an important hyperparameter for accuracy, as we demonstrate in our empirical experiments in Section 3. 2.3 Nonuniform adaptive sampling Although the uniform sampling strategies were predominantly employed, recent studies on the nonuniform adaptive sampling strategies [3, 37] have demonstrated promising improvement in the distribution of residual points during the training processes and achieved better accuracy. 2.3.1 Residual-based adaptive refinement with greed (RAR-G) The first adaptive sampling method for PINNs is the residual-based adaptive refinement method (RAR) proposed in Ref. [3]. RAR aims to improve the distribution of residual points during the training process by sampling more points in the locations where the PDE residual is large. Specifically, after every certain iteration, RAR adds new points in the locations with large PDE residuals (Algorithm 1). RAR only focuses on the points with large residual, and thus it is a greedy algorithm. To better distinguish from the other sampling methods, the RAR method is referred to as RAR-G in this study. Algorithm 1: RAR-G [3]. 1 Sample the initial residual pointsT using one of the methods in Section 2.2.1; 2 Train the PINN for a certain number of iterations; 3 repeat 4 Sample a set of dense pointsS 0 using one of the methods in Section 2.2.1; 5 Compute the PDE residuals for the points inS 0 ; 6 S \u2190mpoints with the largest residuals inS 0 ; 7 T \u2190T \u222aS; 8 Train the PINN for a certain number of iterations; 9 untilthe total number of iterations or the total number of residual points reaches the limit; 2.3.2 Residual-based adaptive distribution (RAD) RAR-G significantly improves the performance of PINNs when solving certain PDEs of solutions with steep gradients [3, 12]. Nevertheless, RAR-G focuses mainly on the location where the PDE residual is largest and disregards the locations of smaller residuals. Another sampling strategy was developed later in Ref. [37], where all the residual points are resampled according to a probability density function (PDF)p(x) proportional to the PDE residual. Specifically, for any pointx, we first compute the PDE residual\u03b5(x) =|f(x; \u02c6u(x))|, and then compute a probability as p(x)\u221d\u03b5(x), i.e., p(x) = \u03b5(x) A whereA= \u222b \u2126\u03b5(x)dxis a normalizing constant. Then all the residual points are sampled according top(x). This approach works for certain PDEs, but as we show in our numerical examples, it does not work well in some cases. Following this idea, we propose an improved version called the residualbased adaptive distribution (RAD) method (Algorithm 2), where we use a new PDF defined as p(x)\u221d \u03b5k(x) E[\u03b5k(x)] +c, (2) wherek\u22650 andc\u22650 are two hyperparameters. E[\u03b5k(x)] can be approximated by a numerical integration such as Monte Carlo integration. We note that the Random-R method in Section 2.2.2 is a special case of RAD by choosingk= 0 orc\u2192\u221e. Algorithm 2: RAD. 1 Sample the initial residual pointsT using one of the methods in Section 2.2.1; 2 Train the PINN for a certain number of iterations; 3 repeat 4 T \u2190A new set of points randomly sampled according to the PDF of Eq. (2); 5 Train the PINN for a certain number of iterations; 6 untilthe total number of iterations reaches the limit; In RAD (Algorithm 2 line 4), we need to sample a set of points according top(x), which can be done in a few ways. Whenxis low-dimensional, we can sample the points approximately in the following brute-force way: Sample a set of dense pointsS 0 using one of the methods in Section 2.2.1; Computep(x) for the points inS 0 ; Define a probability mass function \u0303p(x) =p(Ax)with the normalizing constantA= \u2211 x\u2208S 0 p(x); Sample a subset of points fromS 0 according to \u0303p(x). This method is simple, easy to implement, and sufficient for many PDE problems. For more complicated cases, we can use other methods such as inverse transform sampling, Markov chain Monte Carlo (MCMC) methods, and generative adversarial networks (GANs) [46]. The two hyperparameterskandcin Eq. (2) control the profile ofp(x) and thus the distribution of sampled points. We illustrate the effect ofkandcusing a simple 2D example, \u03b5(x,y) = 2^4 axa(1\u2212x)aya(1\u2212y)a, (3) witha= 10 in Fig. 2. Whenk= 0, it becomes a uniform distribution. As the value ofkincreases, more residual points will large PDE residuals are sampled. As the value ofcincreases, the residual points exhibit an inclination to be uniformly distributed. Compared with RAR, RAD provides more freedom to balance the points in the locations with large and small residuals by tuningkand c. The optimal values ofkandcare problem-dependent, and based on our numerical results, the combination ofk= 1 andc= 1 is usually a good default choice. 2.3.3 Residual-based adaptive refinement with distribution (RAR-D) We also propose a hybrid method of RAR-G and RAD, namely, residual-based adaptive refinement with distribution (RAR-D) (Algorithm 3). Similar to RAR-G, RAR-D repeatedly adds new points to the training dataset; similar to RAD, the new points are sampled based on the PDF in Eq. (2). We note that whenk\u2192 \u221e, only points with the largest PDE residual are added, which recovers RAR-G. The optimal values ofkandcare problem dependent, and based on our numerical results, the combination ofk= 2 andc= 0 is usually a good default choice. Figure 2:Examples of 1000 residual points sampled by RAD with different values ofk andcfor the PDE residual\u03b5(x,y)in Eq.(3). Algorithm 3: RAR-D. 1 Sample the initial residual pointsT using one of the methods in Section 2.2.1; 2 Train the PINN for a certain number of iterations; 3 repeat 4 S \u2190mpoints randomly sampled according to the PDF of Eq. (2); 5 T \u2190T \u222aS; 6 Train the PINN for a certain number of iterations; 7 untilthe total number of iterations or the total number of residual points reaches the limit; 2.4 Comparison with related work As discussed in Section 2.3, our proposed RAD and RAR-D are improved versions of the methods in Refs. [3, 37]. Here, we summarize the similarities between their methods and ours. Lu et al. [3] (in July 2019) proposed RAR (renamed to RAR-G here), which is a special case of RAR-D by choosing a large value ofk. The method proposed by Nabian et al. [37] (in April 2021) is a special case of RAD by choosingk= 1 andc= 0. During the preparation of this paper, a few new papers appeared [38, 39, 40, 41, 42, 43, 44] that also proposed similar methods. Here, we summarize the similarities and differences between these studies. The method proposed by Gao et al. [40] (in December 2021) is a special case of RAD by choosingc= 0. Tang et al. [41] (in December 2021) proposed two methods. One is a special case of RAD by choosingk= 2 andc= 0, and the other is a special case of RAR-D by choosingk= 2 and c= 0. Zeng et al. [43] (in April 2022) proposed a subdomain version of RAR-G. The entire domain is divided into many subdomains, and then new points are added to the several subdomains with large average PDE residual. Similar to RAR-G, Peng et al. [42] (in May 2022) proposed to add more points with large PDE residual, but they used the node generation technology proposed in Ref. [47]. We note that this method only works for a two-dimensional space. Zapf et al. [38] (in May 2022) proposed a modified version of RAR-G, where some points with small PDE residual are removed while adding points with large PDE residual. They show that compared with RAR, this reduces the computational cost, but the accuracy keeps similar. Hanna et al. [44] (in May 2022) proposed a similar method as RAR-D, but they chosep(x)\u221d max{log(\u03b5(x)/\u03b5 0 ), 0 }, where\u03b5 0 is a small tolerance. Similar to the work of Zapf et al., Daw et al. [39] (in July 2022) also proposed to remove the points with small PDE residual, but instead of adding new points with large PDE residual, they added new uniformly random sampled points. Thus all these methods are special cases of our proposed RAD and RAR-D (or with minor modification). However, in our study, two tunable variableskandcare introduced. As we show in our results, the values ofkandccould be crucial since they significantly influence the residual points distribution. By choosing proper values ofkandc, our methods would outperform the other methods. We also note that the point-wise weighting [16, 17, 8, 18] can be viewed as a special case of adaptive sampling, described as follows. When the residual points are randomly sampled from a uniform distributionU(\u2126), and the number of residual points is large, the PDE loss in Eq. (1) can be approximated byEU[\u03b5^2 (x)]. If we consider a point-wise weighting functionw(x), then the loss becomesEU[w(x)\u03b5^2 (x)], while for RAD the loss isEp[\u03b5^2 (x)]. If we choosew(x) (divided by a normalizing constant) as the PDFp(x), then the two losses are equal. 3 Results We apply PINNs with all the ten sampling methods in Section 2 to solve six forward and inverse PDE problems. In all examples, the hyperbolic tangent (tanh) is selected as the activation function. Table 1 summarizes the network width, depth, and optimizers used for each example. More details of the hyperparameters and training procedure can be found in each section of the specific problem. Table 1:The hyperparameters used for each numerical experiment.The learning rate of Adam optimizer is chosen as 0.001. Problems Depth Width Optimizer Section 3.2 Diffusion equation 4 32 Adam Section 3.3 Burgers\u2019 equation 4 64 Adam + L-BFGS Section 3.4 Allen-Cahn equation 4 64 Adam + L-BFGS Section 3.5 Wave equation 6 100 Adam + L-BFGS Section 3.6 Diffusion-reaction equation (inverse) 4 20 Adam Section 3.7 Korteweg-de Vries equation (inverse) 4 100 Adam For both forward and inverse problems, to evaluate the accuracy of the solution \u02c6u, theL^2 relative error is used: \u2016u\u02c6\u2212u\u2016 2 \u2016u\u2016 2 For inverse problems, to evaluate the accuracy of the predicted coefficients\u03bb\u02c6, the relative error is also computed: |\u03bb\u02c6\u2212\u03bb| |\u03bb| As the result of PINN has randomness due to the random sampling, network initialization, and optimization, thus, for each case, we run the same experiment at least 10 times and then compute the geometric mean and standard deviation of the errors. The code in this study is implemented by using the library DeepXDE [3] and is publicly available from the GitHub repositoryhttps: //github.com/lu-group/pinn-sampling. 3.1 Summary Here, we first present a summary of the accuracy of all the methods for the forward and inverse problems listed in Tables 2 and Table 3, respectively. A relatively small number of residual points is chosen to show the difference among different methods. In the specific section of each problem (Sections 3.2\u20133.7), we discuss all the detailed analyses, including the convergence of error during the training process, the convergence of error with respect to the number of residual points, and the effects of different hyperparameters (e.g., the period of resampling in Random-R, the values of kandcin RAD and RAR-D, and the number of new points added each time in RAR-D). We note that Random-R is a special case of RAD by choosingk= 0 orc\u2192 \u221e, and RAR-G is a special case of RAR-D by choosingk\u2192\u221e. Our main findings from the results are as follows. The proposed RAD method has always performed the best among the 10 sampling methods when solving all forward and inverse problems. For PDEs with complicated solutions, such as the Burgers\u2019 and multi-scale wave equation, the proposed RAD and RAR-D methods are predominately effective and yield errors magnitudes lower. For PDEs with smooth solutions, such as the diffusion equation and diffusion-reaction equa- tion, some uniform sampling methods, such as the Hammersley and Random-R, also produce sufficiently low errors. Compared with other uniform sampling methods, Random-R usually demonstrates better performance. Among the six uniform sampling methods with fixed residual points, the low-discrepancy sequences (Halton, Hammersley, and Sobol) generally perform better than Random and LHS, and both are better than Grid. Table 2: L^2 relative error of the PINN solution for the forward problems. Bold font indicates the smallest three errors for each problem. Underlined text indicates the smallest error for each problem. Diffusion Burgers\u2019 Allen-Cahn Wave No. of residual points 30 2000 1000 2000 Grid 0.66\u00b10.06% 13.7\u00b12.37% 93.4\u00b16.98% 81.3\u00b113.7% Random 0.74\u00b10.17% 13.3\u00b18.35% 22.2\u00b116.9% 68.4\u00b120.1% LHS 0.48\u00b10.24% 13.5\u00b19.05% 26.6\u00b115.8% 75.9\u00b133.1% Halton 0.24\u00b10.17% 4.51\u00b13.93% 0.29\u00b10.14% 60.2\u00b110.0% Hammersley 0.17\u00b10.07% 3.02\u00b12.98% 0.14\u00b10.14% 58.9\u00b18.52% Sobol 0.19\u00b10.07% 3.38\u00b13.21% 0.35\u00b10.24% 57.5\u00b114.7% Random-R 0.12\u00b10.06% 1.69\u00b11.67% 0.55\u00b10.34% 0.72\u00b10.90% RAR-G [3] 0.20\u00b10.07% 0.12\u00b10.04% 0.53\u00b10.19% 0.81\u00b10.11% RAD 0.11\u00b10.07% 0.02\u00b10.00% 0.08\u00b10.06% 0.09\u00b10.04% RAR-D 0.14\u00b10.11% 0.03\u00b10.01% 0.09\u00b10.03% 0.29\u00b10.04% 3.2 Diffusion equation We first consider the following one-dimensional diffusion equation: \u2202u \u2202t = \u2202^2 u \u2202x^2 +e\u2212t ( \u2212sin(\u03c0x) +\u03c0^2 sin(\u03c0x) ) , x\u2208[\u2212 1 ,1],t\u2208[0,1], u(x,0) = sin(\u03c0x), u(\u2212 1 ,t) =u(1,t) = 0, whereuis the concentration of the diffusing material. The exact solution isu(x,t) = sin(\u03c0x)e\u2212t. We first compare the performance of the six uniform sampling methods with fixed residual points (Fig. 3A). The number of residual points is ranged from 10 to 80 with an increment of 10 points each time. For each number of residual points, the maximum iteration is set to be 15 000 with Adam as the optimizer. When the number of points is large (e.g., more than 70), all these methods Figure 3:L^2 relative errors of different sampling methods for the diffusion equation in Section 3.2.(A) Six uniform sampling with fixed residual points. (B) Random-R with different periods of resampling when using 30 residual points. (CandD) The training trajectory of RAD with different values ofkandcwhen using 30 residual points. (C)k= 1. (D)c= 1. (EandF) RAR-D with different values ofkandc. Each time one new point is added. (E)k= 2. (F)c= The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. Table 3:L^2 relative error of the PINN solution and relative error of the inferred parameters for the inverse problems.Bold font indicates the smallest three errors for each problem. Underlined text indicates the smallest error for each problem. Diffusion-reaction Korteweg-de Vries u(x) k(x) u(x,t) \u03bb 1 \u03bb 2 No. of residual points 15 600 Grid 0.36\u00b10.12% 8.58\u00b12.14% 24.4\u00b111.1% 53.7\u00b130.7% 42.0\u00b122.3% Random 0.35\u00b10.17% 5.77\u00b12.05% 8.86\u00b12.80% 16.4\u00b17.33% 16.8\u00b17.40% LHS 0.36\u00b10.14% 7.00\u00b12.62% 10.9\u00b12.60% 22.0\u00b16.68% 22.6\u00b16.36% Halton 0.23\u00b10.08% 6.16\u00b11.08% 8.76\u00b13.33% 16.7\u00b16.16% 17.2\u00b16.20% Hammersley 0.28\u00b10.08% 6.37\u00b10.91% 4.49\u00b13.56% 5.24\u00b17.08% 5.71\u00b17.32% Sobol 0.21\u00b10.06% 3.09\u00b10.75% 8.59\u00b13.67% 15.8\u00b16.15% 15.6\u00b15.79% Random-R 0.19\u00b10.09% 3.43\u00b11.80% 0.97\u00b10.15% 0.41\u00b10.30% 1.14\u00b10.31% RAR-G [3] 1.12\u00b10.11% 15.9\u00b11.53% 8.83\u00b11.98% 15.4\u00b19.29% 14.5\u00b19.25% RAD 0.17\u00b10.09% 2.76\u00b11.32% 0.77\u00b10.11% 0.31\u00b10.19% 0.86\u00b10.25% RAR-D 0.76\u00b10.24% 10.3\u00b13.28% 2.36\u00b10.98% 3.49\u00b12.21% 3.18\u00b12.02% have similar performance. However, when the number of residual points is small such as 50, the Hammersley and Sobol sequences perform better than others, and the equispaced uniform grid and random sampling have the largest errors (about one order of magnitude larger than Hammersley and Sobol). We then test the Random-R method using 30 residual points (Fig. 3B). The accuracy of Random-R has a strong dependence on the period of resampling, and the optimal period of resampling in this problem is around 200. Compared with Random without resampling, the Random-R method always leads to lowerL^2 relative errors regardless of the period of resampling. The error can be lower by one order of magnitude by choosing a proper resampling period. Among all the non-adaptive methods, Random-R performs the best. Next, we test the performance of the nonuniform adaptive sampling methods. In Algorithms 2 and 3, the neural network is first trained using 10 000 steps of Adam. In the RAD method, we use 30 residual points and resample every 1000 iterations. The errors of RAD with different values of kandcare shown in Figs. 3C and D. We note that Random-R is a special case of RAD with either c\u2192 \u221eork= 0. Here, RAD with large values ofcor small values ofkleads to better accuracy, i.e., the points are almost uniformly distributed. For the RAR-D method (Figs. 3E and F), one residual point is added after every 1000 iterations starting from 10 points. When usingk= 2 and c= 0 (the two red lines in Figs. 3E red F), RAR-D performs the best. When using 30 residual points, the errors of all the methods are listed in Table 2. In this diffusion equation, all the methods achieve a good accuracy (<1%). Compared with Random-R (0.12%), RAD and RAR-D (0.11%) are not significantly better. The reason could be that the solution of this diffusion equation is very smooth, so uniformly distributed points are good enough. In our following examples, we show that RAD and RAR-D work significantly better and achieve an error of orders of magnitude smaller than the non-adaptive methods. 3.3 Burgers\u2019 equation The Burgers\u2019 equation is considered defined as: \u2202u \u2202t +u \u2202u \u2202x =\u03bd \u2202^2 u \u2202x^2 , x\u2208[\u2212 1 ,1],t\u2208[0,1], u(x,0) =\u2212sin(\u03c0x), u(\u2212 1 ,t) =u(1,t) = 0, whereuis the flow velocity and\u03bd is the viscosity of the fluid. In this study,\u03bdis set at 0. 01 /\u03c0. Different from the diffusion equation with a smooth solution, the solution of the Burgers\u2019 equation has a sharp front whenx= 0 andtis close to 1. We first test the uniform sampling methods by using the number of residual points ranging from 1,000 to 10,000 (Fig. 4A). The maximum iteration is 15,000 steps with Adam as optimizer followed by 15,000 steps of L-BFGS. Fig. 4A shows that the Hammersley method converges the fastest and reaches the lowestL^2 relative error among all the uniform sampling methods, while the Halton and Sobol sequences also perform adequately. Fig. 4B shows theL^2 relative error as a function of the period of resampling using the RandomR method with 2,000 residual points. Similar to the diffusion equation, the Random-R method always outperforms the Random method. However, the performance of Random-R is not sensitive to the period of resampling if the period is smaller than 100. Choosing a period of resampling too large can negatively affect its performance. When applying the nonuniform adaptive methods, the neural network is first trained using 15,000 steps of Adam and then 1,000 steps of L-BFGS. In the RAD method, we use 2000 residual points, which are resampled every 2,000 iterations (1,000 iterations using Adam followed by 1,000 iterations using L-BFGS). As indicated by Fig. 4C, the RAD method possesses significantly greater advantages over the Random-R method (a special case of RAD by choosingk= 0 orc\u2192 \u221e), whoseL^2 relative errors barely decrease during the training processes. This fact reflects that both extreme cases show worse performance. In contrast, fork= 1 andc= 1 (the red lines in Figs. 4C and D), theL^2 relative error declines rapidly and quickly reaches\u223c 2 \u00d7 10 \u2212^4. The RAD method is also effective when choosing a set ofkandcin a moderate range. For the RAR-D method, 1,000 residual points are selected in the pre-trained process, and 10 residual points are added every 2,000 iterations (1,000 iterations using Adam and 1,000 iterations using L-BFGS as optimizer) until the total number of residual points reaches 2,000. Shown by Figs. 4E and F, the optimal values forkandcare found to be 2 and 0, respectively. Since the solution of Burgers\u2019 equation has a very steep region, when using 2000 residual points, both RAD and RAR-D have competitive advantages over the uniform sampling methods in terms of accuracy and efficiency. For the following three forward PDE problems (Allen-Cahn equation in Section 3.4, wave equation in Section 3.5, and diffusion-reaction equation in Section 3.6), unless otherwise stated, the maximum iterations, the use of optimizer, and the training processes remain the same as the Burgers\u2019 equation. Table 2 summarizes theL^2 relative error for all methods when we fix the number of residual points at 2000. All uniform sampling methods fail to capture the solution well. TheL^2 relative errors given by the Halton, Hammersley, and Sobol methods (\u223c4%) are around one-fourth of that given by the Grid, Random, and LHS methods (>13%). Even though the Random-R performs the best among all uniform methods (1.69\u00b11.67%), the proposed RAD and RAR-D methods can achieve anL^2 relative error two orders of magnitude lower than that (0.02%). Figure 4:L^2 relative errors of different sampling methods for the Burgers\u2019 equation in Section 3.3.(A) Six uniform sampling with fixed residual points. (B) Random-R with different periods of resampling when using 2000 residual points. (CandD) The training trajectory of RAD with different values ofkandcwhen using 2000 residual points. (C)k= 1. (D)c= 1. (EandF) RAR-D with different values ofkandc. Each time 10 new points are added. (E)k= 2. (F)c= The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. 3.4 Allen-Cahn equation Next, we consider the Allen-Cahn equation in the following form: \u2202u \u2202t =D \u2202^2 u \u2202x^2 5(u\u2212u^3 ), x\u2208[\u2212 1 ,1],t\u2208[0,1], u(x,0) =x^2 cos(\u03c0x), u(\u2212 1 ,t) =u(1,t) =\u2212 1 , where the diffusion coefficientD= 0.001. Fig. 5 outlines theL^2 relative errors of different sampling methods for the Allen-Cahn equation. Similar patterns are found for the nonadaptive uniform sampling as in the previous examples. The Hammersley method has the best accuracy (Fig. 5A). As the number of residual points becomes significantly large, the difference between these uniform sampling methods becomes negligible. Except for the equispaced uniform grid method, other uniform sampling methods converge toL^2 relative errors of 10\u2212^3 , about the same magnitude as the number of residual points reaching 10^4. Fig. 5B shows that when using 1000 residual points for Random-R, lowerL^2 relative errors can be obtained if we select a period of resampling less than 500. We next test the performance of RAD for different values ofkandcwhen using a different number of residual points. In Figs. 5C and D, we resampled 500 residual points every 2000 iteration, while in Figs. 5E and F, we used 1000 residual points instead. For both cases, the combination of k= 1 andc= 1 (the red lines in Figs. 5C\u2013F) gives good accuracy. When fewer residual points (e.g., 500) are used, the RAD methods boost the performance of PINNs. Similarly, we also test RAR-D in Figs. 5G\u2013J. In Figs. 5G and H, we pre-train the neural network with 500 residual points and add 10 residual points after every 2000 iterations until the total number of residual points reaches 1000. In Figs. 5I and J, we pre-train the neural network using 1000 residual points and heading to 2000 residual points in the same fashion. We recognize that 2 and 0 are the bestkandcvalues for the RAR-D method for both scenarios, which outperform the RAR-G method. As proven in this example, when applying the RAD and the RAR-D methods, the optimal values ofkandcremain stable even though we choose a different number of residual points. In addition, we find that the optimalkandcfor the Burgers\u2019 and Allen Cahn equations are the same for both the RAD and the RAR-D methods. Thus, we could choose (k= 1,c= 1) for the RAD methods and (k= 2,c= 0) for the RAR-D methods by default when first applied these methods to a new PDE problem. To make a comparison across all sampling methods, Table 2 shows theL^2 relative error for the Allen-Cahn equation when we fix the number of residual points at 1000. The Grid, Random, and LHS methods are prone to substantial errors, which are all larger than 20%. Nevertheless, the other four uniform methods (Halton, Hammersley, Sobol, and Random-R) have greater performance and can achieveL^2 relative errors of less than 1%. Remarkably, the RAD and RAR-D methods we proposed can further bring down theL^2 relative error below 0.1%. Figure 5:L^2 relative errors of different sampling methods for the Allen-Cahn equation in Section 3.4.(A) Six uniform sampling with fixed residual points. (B) Random-R with different periods of resampling when using 1000 residual points. (C\u2013F) The training trajectory of RAD with different values ofkandc. (C and D) 500 residual points are used. (C)k= 1. (D)c= 1. (E and F) 1000 residual points are used. (E)k= 1. (F)c= 1. (G\u2013J) RAR-D with different values ofk andc. (G and H) The number of residual points is increased from 500 to 1000. Each time 10 new points are added. (G)k= 2. (H)c= 0. (I and J) The number of residual points is increased from 1000 to 2000. Each time 10 new points are added. (I)k= 2. (J)c= 0. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. 18 3.5 Wave equation In this example, the following one-dimensional wave equation is considered: \u2202^2 u \u2202t^2 \u2212 4 \u2202^2 u \u2202x^2 = 0, x\u2208[0,1],t\u2208[0,1], u(0,t) =u(1,t) = 0, t\u2208[0,1], u(x,0) = sin(\u03c0x) + 1 2 sin(4\u03c0x), x\u2208[0,1], \u2202u \u2202t (x,0) = 0, x\u2208[0,1], where the exact solution is given as: u(x,t) = sin(\u03c0x) cos(2\u03c0t) + 1 2 sin(4\u03c0x) cos(8\u03c0t). The solution has a multi-scale behavior in both spatial and temporal directions. When we test the six uniform sampling methods, the number of residual points are ranged from 1000 to 6000, with an increment of 1000 each time. The Hammersley method achieves the lowest L^2 relative error with the fastest rate (Fig. 6A). When the number of residual points approaches 6000, the Random, Halton, and Hammersley methods can all obtain anL^2 relative error\u223c 10 \u2212^3. To determine the effectiveness of Random-R when using different numbers of residual points, we test the following three scenarios: small (1000 points), medium (4000 points), and large (10.000) sets of residual points (Figs. 6B, C, and D). In the medium case (Fig. 6C), the Random-R attainsL^2 relative errors magnitudes lower than the Random method. However, in the small and large cases (Figs. 6B and D), the Random-R methods show no advantage over the Random method regardless of the period of resampling. This is because when the number of residual points is small, both the Random and Random-R methods fail to provide accurate predictions. On the other hand, if the number of residual points is large, the predictions by the Random method are already highly accurate, so the Random-R is unable to further improve the accuracy. Since the optimal sets ofkandcfor both RAD and RAR-D methods are found to be the same for the Burgers\u2019 and the Allen Cahn equations, in this numerical experiment, we only apply the default settings (i.e., RAD:k= 1 andc= 1; RAR-D:k= 2 andc= 0) to investigate the effect of other factors, including the number of residual points for the RAD method and the number of points added to the RAR-D method. In Fig. 6E, we compare the performance of three nonuniform adaptive sampling methods under the same number of residual points from 1000 to 10 000. We first train the network using 15 000 iterations of Adam and 1000 iterations of L-BFGS, and then after each resampling in RAD or adding new points in RAR-D/RAR-G, we train the network with 1000 iterations of L-BFGS. For the RAR-G and the RAR-D methods, we first train the network with 50% of the final number of the residual points and add 10 residual points each time until reaching the total number of residual points. As we can see from Fig. 6E, the RAD achieves much better results when the number of residual points is small. As the number of residual points increases, the RAR-D method acts more effectively and eventually reaches comparable accuracy to the RAD method. Since the RAD method is more computationally costly than the RAR-D methods with the same number of residual points, we suggest applying the RAD method when the number of residual points is small and the RAR-D method when the number of residual points is large. We next investigate the RAD method with a different number of residual points (i.e., 1000, 2000, 5000, and 10 000). Fig. 6F illustrates that if we increase the number of residual points, lower Figure 6: L^2 relative errors of different sampling methods for the wave equation in Section 3.5.(A) Six uniform sampling with fixed residual points. (B,C, andD) Random-R with different periods of resampling when using (B) 1000 residual points, (C) 4000 residual points, and (D) 10000 residual points. (E) Comparison among RAD (k= 1 andc= 1), RAR-D (k= 2 and c= 0), and RAR-G for different numbers of residual points. (F) The training trajectory of RAD (k= 1 andc= 1) uses different numbers of residual points. (GandH) Convergence of RAR-D (k= 2 andc= 0) when adding a different number of new points each time. (G) New points are added starting from 1000 residual points. (H) New points are added starting from 2500 residual points. (I) Convergence of RAR-G when adding a different number of new points each time. New points are added starting from 2500 residual points. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. L^2 relative error can be achieved but with diminishing marginal effect. We train the network for more than 500 000 iterations to see if theL^2 relative error can further decrease. However, theL^2 relative errors converge and remain relatively stable after 100 000 iterations. One important factor to consider in the RAR-D and the RAR-G methods is how new points are added. We can either add a small number of residual points each time and prolong the training process or add a large number of residual points each time and shorten the process. In Fig. 6G, we first train the network with 1000 residual points and then add new residual points at different rates until the total number of residual points reaches 2000. After adding new residual points each time, we train the network using 1000 steps of L-BFGS. Likewise, in Fig. 6H, we first train the network with 2500 residual points and add new points at different rates until the total number of residual points reaches 5000. In both cases (Figs. 6G and H) that use the RAR-D methods, we find that the best strategy is to add 10 points each time. However, shown by two red-shaded regions in Figs. 6G and H, the results are more stable when we use a larger number of residual points. Fig. 6I is set up the same way as Fig. 6H but tests the RAR-G method. The best strategy for the RAR-G is identical to that of the RAR-D. Table 2 outlines theL^2 relative error for the wave equation using all methods when the number of residual points equals 2000. All uniform methods with fixed residual points perform poorly (error >50%) and fail to approximate the truth values. Random-R, as a special case of the proposed RAD, givesL^2 relative errors of around 1%. The RAR-D method significantly enhances the prediction accuracy resulting inL^2 relative errors under 0.3%. In addition, the RAD with the default setting ofkandcconverges toL^2 relative errors under 0.1%. 3.6 Diffusion-reaction equation The first inverse problem we consider is the diffusion-reaction system as follows: \u03bb d^2 u dx^2 \u2212k(x)u=f, x\u2208[0,1], wheref = sin(2\u03c0x) is the source term. \u03bb= 0.01 is the diffusion coefficient, anduis the solute concentration. In this problem, we aim to infer the space-dependent reaction ratek(x) with given measurements on the solutionu. The exact unknown reaction rate is k(x) = 0.1 +e\u2212^0.^5 (x\u2212 0 .5)^2 (^152). We aim to learn the unknown functionk(x) and solve foru(x) by using eight observations ofu, which are uniformly distributed on the domainx\u2208[0,1], including two points on both sides of the boundaries. TheL^2 relative errors for both the solutionu(Figs. 7A, C, and E) and the unknown functionk(Figs. 7B, D, and F) are computed. The maximum number of iterations is 50 000 steps of Adam. Figs. 7A and B summarize the performance of all uniform sampling methods. We note that in 1D, the Hammersley and Halton sequences are identical and outperform other uniform methods. We fix the residual points at 15 and compare the Random method with the Random-R method. TheL^2 relative errors (Figs. 7C and D) given by the Random-R remain steady, disregarding the changes in the period of resampling, and are approximately the same as that produced by the Random method. This is because the reaction-diffusion system is fairly simple and can be easily handled by uniform sampling methods without resampling. Next, we compare the Random, RAD, RAR-G, and RAR-D methods with default settings (i.e., RAD:k= 1 andc= 1; RAR-D:k= 2 andc= 0) using a different number of residual points. For the random and RAD methods, the maximum number of iterations is 50 000 steps of Adam. For Figure 7:L^2 relative errors of different sampling methods foruandkin the diffusionreaction equation in Section 3.6.(AandB) Six uniform sampling with fixed residual points. (CandD) Random-R with different periods of resampling when using 15 residual points. (Eand F) Comparison among Random, RAD (k= 1 andc= 1), RAR-G, and RAR-D (k= 2 andc= 0) for different numbers of residual points. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. the RAR-G/RAR-D, we first train the neural network with 50% of the total number of residual points for 10 000 steps of Adam; then we add one point each time and train for 1000 steps of Adam until we meet the total number of residual points. As shown by Figs. 7E and F, the RAD method surpasses other methods and is able to produce lowL^2 relative error even when the number of residual points is very small. However, RAR-G and RAR-D are even worse than the Random sampling. To sum up, we fix the number of residual points at 15 and present theL^2 relative error for both the solution and unknown function in Table 3. The RAD yields the minimumL^2 relative error (0.17% foru(x); 2.76% fork(x)). However, due to the simplicity of this PDE problem, some uniform sampling methods, especially the Sobol and Random-R, have comparable performance to the RAD. Generally speaking, we recognize that the uniform sampling methods are adequate when solving this inverse PDE with smooth solutions. Still, the RAD method can further enhance the performance of PINNs, especially when the number of residual points is small. 3.7 Korteweg-de Vries equation The second inverse problem we solve is the Korteweg-de Vries (KdV) equation: \u2202u \u2202t +\u03bb 1 u \u2202u \u2202x +\u03bb 2 \u2202^3 u \u2202x^3 = 0, x\u2208[\u2212 1 ,1], t\u2208[0,1], where\u03bb 1 and\u03bb 2 are two unknown parameters. The exact values for\u03bb 1 and\u03bb 2 are 1 and 0.0025, respectively. The initial condition isu(x,t= 0) = cos(\u03c0x), and periodic boundary conditions are used. To infer\u03bb 1 and\u03bb 2 , we assume that we have the observations of two solution snapshots u(x,t= 0.2) andu(x,t= 0.8) at 64 uniformly distributed points at each time. In Fig. 8, the first column (Figs. 8A, D, and G) shows theL^2 relative error of the solutionu, while the second column (Figs. 8B, E, and H) and the third column (Figs. 8C, F, and I) illustrate the relative errors for\u03bb 1 and\u03bb 2 , respectively. The maximum iteration is 100 000 steps of Adam. Hammersley achieves better accuracy than the other uniform sampling methods. The Sobol and Halton methods behave comparably as these two curves (the yellow and green curves in Figs. 8A, B, and C) are almost overlapping. Shown in Figs. 8D, E and F, the Random-R method yields higher accuracy than the Random method by about one order of magnitude in all cases when using 1000 residual points. A smaller period of resampling leads to smaller errors. Figs. 8G, H, and I compare the Random-R, Random, RAD, RAR-G, and RAR-D methods using the same number of residual points and the total number of iterations. For the Random and the Random-R methods, we train the network for 100 000 steps of Adams. For the RAD methods, we first train the network using 50 000 steps of Adams; then, we resample the residual points and train for 1000 steps of Adams 50 times. In order to fix the total number of iterations for the RARG/RAR-D methods to 100 000, we accordingly adjust the number of new residual points added each time. For example, if the final number of residual points is 500, we first train the network using 250 residual points (i.e., 50% of the total number of residual points) with 50 000 steps of Adams; and we consequently add 5 points and train for 1000 steps of Adams each time. If the final number of residual points is 1000, we first train the network using 500 residual points with 50 000 steps of Adams; and then we add 10 points and train for 1000 steps of Adams each time. As demonstrated by Figs. 8G, H, and I, the RAD method is the best, while the Random-R method is also reasonably accurate. We show one example of the training process (Figs. 8J, K, and L) when the number of residual points is 600 to illustrate the convergence of the solution,\u03bb 1 , and\u03bb 2 during training. The resampling strategies, especially the RAD method, achieve the greatest success among all sampling methods. Figure 8:L^2 relative errors ofuand relative errors of\u03bb 1 and\u03bb 2 using different sampling methods for the Korteweg-de Vries equation in Section 3.7. (A,B, andC) Six uniform sampling with fixed residual points. (D,E, andF) Random-R with different periods of resampling when using 1000 residual points. (G,H, andI) Comparison among Random, Random-R, RAD (k= 1 andc= 1), RAR-G, and RAR-D (k= 2 andc= 0) for different number of residual points. (J,K, andL) Examples of the training trajectories using Random, Random-R, RAD (k= 1 and c= 1), RAR-G, and RAR-D (k= 2 andc= 0) with 600 residual points. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. Table 3 demonstrates theL^2 relative errors for the solutionu(x,t) and the relative error of two unknown parameters\u03bb 1 and\u03bb 2 , for all methods when the number of residual points is set at The lowestL^2 relative errors for uniform sampling with fixed points are given by Hammersley (\u223c 5%). The Random-R is the second-best method and providesL^2 relative errors of around 1%. With the smallest errors (<1%) and standard deviations, the RAD method has compelling advantages over all other methods in terms of accuracy and robustness. It is noteworthy that the RAR-D method provides adequate accuracy (\u223c3%) and is less expensive than the Random-R and RAD methods when the number of residual points is the same. Therefore, the RAR-D is also a valuable approach to consider. 4 Conclusions In this paper, we present a comprehensive study of two categories of sampling for physics-informed neural networks (PINNs), including non-adaptive uniform sampling and adaptive nonuniform sampling. For the non-adaptive uniform sampling, we have considered six methods: (1) equispaced uniform grid (Grid), (2) uniformly random sampling (Random), (3) Latin hypercube sampling (LHS), (4) Halton sequence (Halton), (5) Hammersley sequence (Hammersley), and (6) Sobol sequence (Sobol). We have also considered a resampling strategy for uniform sampling (Random-R). For the adaptive nonuniform sampling, motivated by the residual-based adaptive refinement with greed (RAR-G) [3], we proposed two new residual-based adaptive sampling methods: residual-based adaptive distribution (RAD) and residual-based adaptive refinement with distribution (RAR-D). We extensively investigated the performance of these ten sampling methods in solving four forward and two inverse problems of partial differential equations (PDEs) with many setups, such as a different number of residual points. Our results show that the proposed RAD and RAR-D significantly improve the accuracy of PINNs by orders of magnitude, especially when the number of residual points is small. RAD and RAR-D also have great advantages for the PDEs with complicated solutions, e.g., the solution of the Burgers\u2019 equation with steep gradients and the solution of the wave equation with a multi-scale behavior. A summary of the comparison of these methods can be found in Section 3.1. Based on our empirical results, we summarize the following suggestions as a practical guideline in choosing sampling methods for PINNs. RAD withk= 1 andc= 1 can be chosen as the default sampling method when solving a new PDE. The hyperparameterskandccan be tuned to balance the points in the locations with large and small PDE residuals. RAR-D can achieve comparable accuracy to RAD, but RAR-D is more computationally efficient as it gradually increases the number of residual points. Hence, RAR-D (k= 2 and c= 0 by default) is preferable for the case with limited computational resources. Random-R can be used in the situation where adaptive sampling is not allowed, e.g., it is difficult to sample residual points according to a probability density function. The period of resampling should not be chosen as too small or too large. A low-discrepancy sequence (e.g., Hammersley) should be considered rather than Grid, Ran- dom, or LHS, when we have to use a fixed set of residual points, such as in PINNs with the augmented Lagrangian method (hPINNs) [8]. In this study, we sample residual points in RAD and RAR-D by using a brute-force approach, which is simple, easy to implement, and sufficient for many PDEs. However, for high-dimensional problems, we need to use other methods, such as generative adversarial networks (GANs) [46], as was done in Ref. [41]. Moreover, the probability of sampling a pointxis only considered as p(x)\u221d \u03b5 k(x) E[\u03b5k(x)]+c. While this probability works very well in this study, it is possible that there exists another better choice. We can learn a new probability density function by meta-learning, as was done for loss functions of PINNs in Ref. [11]. References [1] M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.Journal of Computational Physics, 378:686\u2013707, 2019. [2] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations.Science, 367(6481):1026\u20131030, 2020. [3] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. DeepXDE: A deep learning library for solving differential equations.SIAM Review, 63(1):208\u2013228, 2021. [4] George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning.Nature Reviews Physics, 3(6):422\u2013440, 2021. [5] Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse problems in nano-optics and metamaterials.Optics Express, 28(8):11618, 2020. [6] Alireza Yazdani, Lu Lu, Maziar Raissi, and George Em Karniadakis. Systems biology informed deep learning for inferring parameters and hidden dynamics. PLOS Computational Biology, 16(11), 2020. [7] Mitchell Daneker, Zhen Zhang, George Em Kevrekidis, and Lu Lu. Systems biology: Identifiability analysis and parameter identification via systems-biology informed neural networks. arXiv preprint arXiv:2202.01723, 2022. [8] Lu Lu, Rapha \u0308el Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G. Johnson. Physics-informed neural networks with hard constraints for inverse design. SIAM Journal on Scientific Computing, 43(6), 2021. [9] Guofei Pang, Lu Lu, and George Em Karniadakis. fPINNs: Fractional physics-informed neural networks. SIAM Journal on Scientific Computing, 41(4), 2019. [10] Dongkun Zhang, Lu Lu, Ling Guo, and George Em Karniadakis. Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems.Journal of Computational Physics, 397:108850, 2019. [11] Apostolos F Psaros, Kenji Kawaguchi, and George Em Karniadakis. Meta-learning PINN loss functions. Journal of Computational Physics, 458:111121, 2022. [12] Jeremy Yu, Lu Lu, Xuhui Meng, and George Em Karniadakis. Gradient-enhanced physicsinformed neural networks for forward and inverse PDE problems.Computer Methods in Applied Mechanics and Engineering, 393:114823, 2022. [13] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow pathologies in physics-informed neural networks. SIAM Journal on Scientific Computing, 43(5):A3055\u2013A3081, 2021. [14] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why PINNs fail to train: A neural tangent kernel perspective. Journal of Computational Physics, 449:110768, 2022. [15] Zixue Xiang, Wei Peng, Xu Liu, and Wen Yao. Self-adaptive loss balanced physics-informed neural networks.Neurocomputing, 2022. [16] Levi McClenny and Ulisses Braga-Neto. Self-adaptive physics-informed neural networks using a soft attention mechanism.arXiv preprint arXiv:2009.04544, 2020. [17] Yiqi Gu, Haizhao Yang, and Chao Zhou. SelectNet: Self-paced learning for high-dimensional partial differential equations.Journal of Computational Physics, 441:110444, 2021. [18] Wensheng Li, Chao Zhang, Chuncheng Wang, Hanting Guan, and Dacheng Tao. Revisiting PINNs: Generative adversarial physics-informed neural networks and point-weighting method. arXiv preprint arXiv:2205.08754, 2022. [19] Xuhui Meng, Zhen Li, Dongkun Zhang, and George Em Karniadakis. PPINN: Parareal physicsinformed neural network for time-dependent pdes. Computer Methods in Applied Mechanics and Engineering, 370:113250, 2020. [20] Khemraj Shukla, Ameya D. Jagtap, and George Em Karniadakis. Parallel physics-informed neural networks via domain decomposition. Journal of Computational Physics, 447:110683, 2021. [21] Ameya D. Jagtap and George Em Karniadakis. Extended physics-informed neural networks (XPINNs): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations. Communications in Computational Physics, 28(5):2002\u20132041, 2020. [22] Colby L Wight and Jia Zhao. Solving Allen-Cahn and Cahn-Hilliard equations using the adaptive physics informed neural networks. arXiv preprint arXiv:2007.04542, 2020. [23] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks.Advances in Neural Information Processing Systems, 34:26548\u201326560, 2021. [24] Revanth Mattey and Susanta Ghosh. A novel sequential method to train physics informed neural networks for allen cahn and cahn hilliard equations. Computer Methods in Applied Mechanics and Engineering, 390:114474, 2022. [25] Katsiaryna Haitsiukevich and Alexander Ilin. Improved training of physics-informed neural networks with model ensembles. arXiv preprint arXiv:2204.05108, 2022. [26] Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality is all you need for training physics-informed neural networks. arXiv preprint arXiv:2203.07404, 2022. [27] Pola Lydia Lagari, Lefteri H Tsoukalas, Salar Safarkhani, and Isaac E Lagaris. Systematic construction of neural forms for solving partial differential equations inside rectangular domains, subject to initial, boundary and interface conditions. International Journal on Artificial Intelligence Tools, 29(05):2050009, 2020. [28] Suchuan Dong and Naxian Ni. A method for representing periodic functions and enforcing exactly periodic boundary conditions with deep neural networks. Journal of Computational Physics, 435:110242, 2021. [29] Michael D McKay, Richard J Beckman, and William J Conover. A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. Technometrics, 42(1):55\u201361, 2000. [30] Michael Stein. Large sample properties of simulations using Latin hypercube sampling.Technometrics, 29(2):143\u2013151, 1987. [31] Il\u2019ya Meerovich Sobol\u2019. On the distribution of points in a cube and the approximate evaluation of integrals.Zhurnal Vychislitel\u2019noi Matematiki i Matematicheskoi Fiziki, 7(4):784\u2013802, 1967. [32] John H Halton. On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals. Numerische Mathematik, 2(1):84\u201390, 1960. [33] JM Hammersley and DC Handscomb. Monte-Carlo methods, mathuen, 1964. [34] Hongwei Guo, Xiaoying Zhuang, Xiaoyu Meng, and Timon Rabczuk. Analysis of three dimensional potential problems in non-homogeneous media with deep learning based collocation method.arXiv preprint arXiv:2010.12060, 2020. [35] Sourav Das and Solomon Tesfamariam. State-of-the-art review of design of experiments for physics-informed deep learning.arXiv preprint arXiv:2202.06416, 2022. [36] Zhiping Mao, Ameya D Jagtap, and George Em Karniadakis. Physics-informed neural networks for high-speed flows. Computer Methods in Applied Mechanics and Engineering, 360:112789, 2020. [37] Mohammad Amin Nabian, Rini Jasmine Gladstone, and Hadi Meidani. Efficient training of physics-informed neural networks via importance sampling.Computer-Aided Civil and Infrastructure Engineering, 2021. [38] Bastian Zapf, Johannes Haubner, Miroslav Kuchta, Geir Ringstad, Per Kristian Eide, and Kent-Andre Mardal. Investigating molecular transport in the human brain from MRI with physics-informed neural networks.arXiv preprint arXiv:2205.02592, 2022. [39] Arka Daw, Jie Bu, Sifan Wang, Paris Perdikaris, and Anuj Karpatne. Rethinking the importance of sampling in physics-informed neural networks. arXiv preprint arXiv:2207.02338, 2022. [40] Wenhan Gao and Chunmei Wang. Active learning based sampling for high-dimensional nonlinear partial differential equations. arXiv preprint arXiv:2112.13988, 2021. [41] Kejun Tang, Xiaoliang Wan, and Chao Yang. DAS: A deep adaptive sampling method for solving partial differential equations.arXiv preprint arXiv:2112.14038, 2021. [42] Wei Peng, Weien Zhou, Xiaoya Zhang, Wen Yao, and Zheliang Liu. RANG: a residualbased adaptive node generation method for physics-informed neural networks. arXiv preprint arXiv:2205.01051, 2022. [43] Shaojie Zeng, Zong Zhang, and Qingsong Zou. Adaptive deep neural networks methods for high-dimensional partial differential equations.Journal of Computational Physics, 463:111232, 2022. [44] John M Hanna, Jose V Aguado, Sebastien Comas-Cardona, Ramzi Askri, and Domenico Borzacchiello. Residual-based adaptivity for two-phase flow simulation in porous media using physics-informed neural networks.Computer Methods in Applied Mechanics and Engineering, 396:115100, 2022. [45] Melissa E O\u2019Neill. Pcg: A family of simple fast space-efficient statistically good algorithms for random number generation.ACM Transactions on Mathematical Software, 2014. [46] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.Advances in neural information processing systems, 27, 2014. [47] Bengt Fornberg and Natasha Flyer. Fast generation of 2-D node distributions for mesh-free pde discretizations.Computers & Mathematics with Applications, 69(7):531\u2013544, 2015.","title":"A Comprehensive Study of Non-Adaptive and Residual-Based Adaptive Sampling for Physics-Informed Neural Networks <br> \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u81ea\u9002\u5e94\u91c7\u6837\u548c\u57fa\u4e8e\u6b8b\u5dee\u7684\u81ea\u9002\u5e94\u91c7\u6837\u7684\u7efc\u5408\u7814\u7a76"},{"location":"Models/PINNs/PN-2207.10289/#a-comprehensive-study-of-non-adaptive-and-residual-based-adaptive-sampling-for-physics-informed-neural-networks","text":"\u4f5c\u8005: Chenxi Wu1,\u2020, Min Zhu1,\u2020, Qinyang Tan^2 , Yadhu Kartha^3 , and Lu Lu1,* \u673a\u6784: College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA \u65f6\u95f4: 2022-07-21 \u9884\u5370: arXiv:2207.10289v1 \u9886\u57df: physics.comp-ph \u6807\u7b7e: \u504f\u5fae\u5206\u65b9\u7a0b, \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc, \u6b8b\u5dee\u70b9\u5206\u5e03, \u975e\u81ea\u9002\u5e94\u5747\u5300\u91c7\u6837, \u5e26\u91cd\u91c7\u6837\u7684\u5747\u5300\u91c7\u6837, \u57fa\u4e8e\u6b8b\u5dee\u7684\u81ea\u9002\u5e94\u91c7\u6837 \u5f15\u7528: 47 \u7bc7","title":"A Comprehensive Study of Non-Adaptive and Residual-Based Adaptive Sampling for Physics-Informed Neural Networks  \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u81ea\u9002\u5e94\u91c7\u6837\u548c\u57fa\u4e8e\u6b8b\u5dee\u7684\u81ea\u9002\u5e94\u91c7\u6837\u7684\u7efc\u5408\u7814\u7a76"},{"location":"Models/PINNs/PN-2207.10289/#abstract","text":"Physics-informed neural networks (PINNs) have shown to be an effective tool for solving both forward and inverse problems of partial differential equations (PDEs). PINNs embed the PDEs into the loss of the neural network using automatic differentiation, and this PDE loss is evaluated at a set of scattered spatio-temporal points (called residual points). The location and distribution of these residual points are highly important to the performance of PINNs. However, in the existing studies on PINNs, only a few simple residual point sampling methods have mainly been used. Here, we present a comprehensive study of two categories of sampling for PINNs: non-adaptive uniform sampling and adaptive nonuniform sampling. We consider six uniform sampling methods, including (1) equispaced uniform grid, (2) uniformly random sampling, (3) Latin hypercube sampling, (4) Halton sequence, (5) Hammersley sequence, and (6) Sobol sequence. We also consider a resampling strategy for uniform sampling. To improve the sampling efficiency and the accuracy of PINNs, we propose two new residual-based adaptive sampling methods: residual-based adaptive distribution (RAD) and residual-based adaptive refinement with distribution (RAR-D), which dynamically improve the distribution of residual points based on the PDE residuals during training. Hence, we have considered a total of 10 different sampling methods, including six non-adaptive uniform sampling, uniform sampling with resampling, two proposed adaptive sampling, and an existing adaptive sampling. We extensively tested the performance of these sampling methods for four forward problems and two inverse problems in many setups. Our numerical results presented in this study are summarized from more than 6000 simulations of PINNs. We show that the proposed adaptive sampling methods of RAD and RAR-D significantly improve the accuracy of PINNs with fewer residual points for both forward and inverse problems. The results obtained in this study can also be used as a practical guideline in choosing sampling methods.","title":"Abstract"},{"location":"Models/PINNs/PN-2207.10289/#1-introduction","text":"Physics-informed neural networks (PINNs) [1] have emerged in recent years and quickly became a powerful tool for solving both forward and inverse problems of partial differential equations (PDEs) via deep neural networks (DNNs) [2, 3, 4]. PINNs embed the PDEs into the loss of the neural network using automatic differentiation. Compared with traditional numerical PDE solvers, such as the finite difference method (FDM) and the finite element method (FEM), PINNs are mesh free and therefore highly flexible. Moreover, PINNs can easily incorporate both physics-based constraints and data measurements into the loss function. PINNs have been applied to tackle diverse problems in computational science and engineering, such as inverse problems in nanooptics, metamaterials [5], and fluid dynamics [2], parameter estimation in systems biology [6, 7], and problems of inverse design and topology optimization [8]. In addition to standard PDEs, PINNs have also been extended to solve other types of PDEs, including integro-differential equations [3], fractional PDEs [9], and stochastic PDEs [10]. Despite the past success, addressing a wide range of PDE problems with increasing levels of complexity can be theoretically and practically challenging, and thus many aspects of PINNs still require further improvements to achieve more accurate prediction, higher computational efficiency, and training robustness [4]. A series of extensions to the vanilla PINN have been proposed to boost the performance of PINNs from various aspects. For example, better loss functions have been discovered via meta-learning [11], and gradient-enhanced PINNs (gPINNs) have been developed to embed the gradient information of the PDE residual into the loss [12]. In PINNs, the total loss is a weighted summation of multiple loss terms corresponding to the PDE and initial/boundary conditions, and different methods have been developed to automatically tune these weights and balance the losses [13, 14, 15]. Moreover, a different weight for each loss term could be set at every training point [16, 17, 8, 18]. For problems in a large domain, decomposition of the spatiotemporal domain accelerates the training of PINNs and improves their accuracy [19, 20, 21]. For time-dependent problems, it is usually helpful to first train PINNs within a short time domain and then gradually expand the time intervals of training until the entire time domain is covered [22, 23, 24, 25, 26]. In addition to these general methods, other problem-specific techniques have also been developed, e.g., enforcing Dirichlet or periodic boundary conditions exactly by constructing special neural network architectures [27, 28, 8]. PINNs are mainly optimized against the PDE loss, which guarantees that the trained network is consistent with the PDE to be solved. PDE loss is evaluated at a set of scattered residual points. Intuitively, the effect of residual points on PINNs is similar to the effect of mesh points on FEM, and thus the location and distribution of these residual points should be highly important to the performance of PINNs. However, in previous studies on PINNs, two simple residual point sampling methods (i.e., an equispaced uniform grid and uniformly random sampling) have mainly been used, and the importance of residual point sampling has largely been overlooked.","title":"1 Introduction"},{"location":"Models/PINNs/PN-2207.10289/#11-related-work-and-our-contributions","text":"Different residual point sampling methods can be classified into two categories: uniform sampling and nonuniform sampling. Uniform sampling can be obtained in multiple ways. For example, we could use the nodes of an equispaced uniform grid as the residual points or randomly sample the points according to a continuous uniform distribution in the computational domain. Although these two sampling methods are simple and widely used, alternative sampling methods may be applied. The Latin hypercube sampling (LHS) [29, 30] was used in Ref. [1], and the Sobol sequence [31] was first used for PINNs in Ref. [9]. The Sobol sequence is one type of quasi random low-discrepancy sequences among other sequences, such as the Halton sequence [32], and the Hammersley sequence [33]. Low-discrepancy sequences usually perform better than uniformly distributed random numbers in many applications such as numerical integration; hence, a comprehensive comparison of these methods for PINNs is required. However, very few comparisons [34, 35] have been performed. In this study, we extensively compared the performance of different uniform sampling methods, including (1) equispaced uniform grid, (2) uniformly random sampling, (3) LHS, (4) Sobol sequence, (5) Halton sequence, and (6) Hammersley sequence. In supervised learning, the dataset is fixed during training, but in PINNs, we can select residual points at any location. Hence, instead of using the same residual points during training, in each optimization iteration, we could select a new set of residual points, as first emphasized in Ref. [3]. While this strategy has been used in some works, it has not yet been systematically tested. Thus, in this study, we tested the performance of such a resampling strategy and investigated the effect of the number of residual points and the resampling period for the first time. Uniform sampling works well for some simple PDEs, but it may not be efficient for those that are more complicated. To improve the accuracy, we could manually select the residual points in a nonuniform way, as was done in Ref. [36] for high-speed flows, but this approach is highly problem-dependent and usually tedious and time-consuming. In this study, we focus on automatic and adaptive nonuniform sampling. Motivated by the adaptive mesh refinement in FEM, Lu et al. [3] proposed the first adaptive nonuniform sampling for PINNs in 2019, the residual-based adaptive refinement (RAR) method, which adds new residual points in the locations with large PDE residuals. In 2021, another sampling strategy [37] was developed, where all the residual points were resampled according to a probability density function (PDF) proportional to the PDE residual. In this study, motivated by these two ideas, we proposed two new sampling strategies: residual-based adaptive distribution (RAD), where the PDF for sampling is a nonlinear func- tion of the PDE residual; residual-based adaptive refinement with distribution (RAR-D), which is a hybrid method of RAR and RAD, i.e., the new residual points are added according to a PDF. During the preparation of this paper, a few new studies appeared [38, 39, 40, 41, 42, 43, 44] that also proposed modified versions of RAR or PDF-based resampling. Most of these methods are special cases of the proposed RAD and RAR-D, and our methods can achieve better performance. We include a detailed comparison of these strategies in Section 2.4, after introducing several notations and our new proposed methods. In this study, we have considered a total of 10 different sampling methods, including seven non-adaptive sampling methods (six different uniform samplings and one uniform sampling with resampling) and three adaptive sampling approaches (RAR, RAD, and RAR-D). We compared the performance of these sampling methods for four forward problems of PDEs and investigated the effect of the number of residual points. We also compared their performance for two inverse problems that have not yet been consid- ered in the literature. We performed more than 6000 simulations of PINNs to obtain all the results shown in this study.","title":"1.1 Related work and our contributions"},{"location":"Models/PINNs/PN-2207.10289/#12-organization","text":"This paper is organized as follows. In Section 2, after providing a brief overview of PINNs and different non-adaptive sampling strategies, two new adaptive nonuniform sampling strategies (RAD and RAR-D) are proposed. In Section 3, we compare the performance of 10 different methods for six different PDE problems, including four forward problems and two inverse problems. Section 4 summarizes the findings and concludes the paper.","title":"1.2 Organization"},{"location":"Models/PINNs/PN-2207.10289/#2-methods","text":"This section briefly reviews physics-informed neural networks (PINNs) in solving forward and inverse partial differential equations (PDEs). Then different types of uniformly sampling are introduced. Next, two nonuniform residual-based adaptive sampling methods are proposed to enhance the accuracy and training efficiency of PINNs. Finally, a comparison of related methods is presented.","title":"2 Methods"},{"location":"Models/PINNs/PN-2207.10289/#21-pinns-in-solving-forward-and-inverse-pdes","text":"We consider the PDE parameterized by\u03bbdefined on a domain \u2126\u2282Rd, f(x;u(x)) =f","title":"2.1 PINNs in solving forward and inverse PDEs"},{"location":"Models/PINNs/PN-2207.10289/#_1","text":"x; \u2202u \u2202x 1","title":"("},{"location":"Models/PINNs/PN-2207.10289/#_2","text":"\u2202u \u2202xd","title":""},{"location":"Models/PINNs/PN-2207.10289/#_3","text":"\u2202^2 u \u2202x 1 \u2202x 1","title":""},{"location":"Models/PINNs/PN-2207.10289/#_4","text":"\u2202^2 u \u2202x 1 \u2202xd ;...;\u03bb","title":""},{"location":"Models/PINNs/PN-2207.10289/#_5","text":"= 0, x= (x 1 ,...,xd)\u2208\u2126, with boundary conditions on\u2202\u2126 B(u,x) = 0, andu(x) denotes the solution atx. In PINNs, the initial condition is treated as the Dirichlet boundary condition. A forward problem is aimed to obtain the solutionu across the entire domain, where the model parameters\u03bbare known. In practice, the model parameters\u03bbmight be unknown, but some observations from the solutionuare available, which lead to an inverse problem. An inverse problem is aimed to discover parameters\u03bbthat best describe the observed data from the solution. PINNs are capable of addressing both forward and inverse problems. To solve a forward problem, the solutionuis represented with a neural network \u02c6u(x;\u03b8). The network parameters\u03b8are trained to approximate the solutionu, such that the loss function is minimized [1, 3]: L(\u03b8;T) =wfLf(\u03b8;Tf) +wbLb(\u03b8;Tb), where Lf(\u03b8;Tf) =","title":")"},{"location":"Models/PINNs/PN-2207.10289/#1","text":"|Tf|","title":"1"},{"location":"Models/PINNs/PN-2207.10289/#_6","text":"x\u2208Tf","title":"\u2211"},{"location":"Models/PINNs/PN-2207.10289/#_7","text":"\u2223\u2223f(x; \u2202u\u02c6 \u2202x 1","title":"\u2223\u2223"},{"location":"Models/PINNs/PN-2207.10289/#_8","text":"\u2202\u02c6u \u2202xd","title":""},{"location":"Models/PINNs/PN-2207.10289/#_9","text":"\u2202^2 u\u02c6 \u2202x 1 \u2202x 1","title":""},{"location":"Models/PINNs/PN-2207.10289/#_10","text":"\u2202^2 u\u02c6 \u2202x 1 \u2202xd ;...;\u03bb)","title":""},{"location":"Models/PINNs/PN-2207.10289/#_11","text":"","title":"\u2223\u2223"},{"location":"Models/PINNs/PN-2207.10289/#_12","text":"2 , (1) Lb(\u03b8;Tb) =","title":"\u2223\u2223"},{"location":"Models/PINNs/PN-2207.10289/#1_1","text":"|Tb|","title":"1"},{"location":"Models/PINNs/PN-2207.10289/#_13","text":"x\u2208Tb |B(\u02c6u,x)|^2 , andwfandwbare the weights. Two sets of points are samples both inside the domain (Tf) and on the boundaries (Tb). Here,TfandTbare referred to as the sets of \u201cresidual points\u201d, andT=Tf\u222aTb. To solve the inverse problem, an additional loss term corresponding to the misfit of the observed data at the locationsTi, defined as Li(\u03b8,\u03bb;Ti) =","title":"\u2211"},{"location":"Models/PINNs/PN-2207.10289/#1_2","text":"|Ti|","title":"1"},{"location":"Models/PINNs/PN-2207.10289/#_14","text":"x\u2208Ti |u\u02c6(x)\u2212u(x)|^2 , is added to the loss function. The loss function is then defined as L(\u03b8,\u03bb;T) =wfLf(\u03b8,\u03bb;Tf) +wbLb(\u03b8,\u03bb;Tb) +wiLi(\u03b8,\u03bb;Ti), with an additional weightwi. Then the network parameters\u03b8are trained simultaneously with\u03bb. For certain PDE problems, it is possible to enforce boundary conditions directly by constructing a special network architecture [27, 28, 8, 12], which eliminates the loss term of boundary conditions. In this study, the boundary conditions are enforced exactly and automatically. Hence, for a forward problem, the loss function is L(\u03b8,\u03bb;T) =Lf(\u03b8,\u03bb;Tf). For an inverse problem, the loss function is L(\u03b8,\u03bb;T) =wfLf(\u03b8,\u03bb;Tf) +wiLi(\u03b8,\u03bb;Ti), where we choosewf=wi= 1 for the diffusion-reaction equation in Section 3.6, andwf= 1,wi= 1000 for the Korteweg-de Vries equation in Section 3.7.","title":"\u2211"},{"location":"Models/PINNs/PN-2207.10289/#22-uniformly-distributed-non-adaptive-sampling","text":"The training of PINNs requires a set of residual points (Tf). The sampling strategy ofTf plays a vital role in promoting the accuracy and computational efficiency of PINNs. Here, we discuss several sampling approaches. 2.2.1 Fixed residual points In most studies of PINNs, we specify the residual points at the beginning of training and never change them during the training process. Two simple sampling methods (equispaced uniform grids and uniformly random sampling) have been commonly used. Other sampling methods, such as the Latin hypercube sampling (LHS) [29, 30] and the Sobol sequence [31], have also been used in some studies [1, 9, 34]. The Sobol sequence is one type of quasi-random low-discrepancy sequences. Low-discrepancy sequences are commonly used as a replacement for uniformly distributed random numbers and usually perform better in many applications such as numerical integration. This study also considers other low-discrepancy sequences, including the Halton sequence [32] and the Hammersley sequence [33]. We list the six uniform sampling methods as follows, and the examples of 400 points generated in [0,1]^2 using different methods are shown in Fig. 1. Equispaced uniform grid (Grid): The residual points are chosen as the nodes of an equispaced uniform grid of the computational domain. Uniformly random sampling (Random): The residual points are randomly sampled according to a continuous uniform distribution over the domain. In practice, this is usually done using pseudo-random number generators such as the PCG-64 algorithm [45]. Latin hypercube sampling LHS : The LHS is a stratified Monte Carlo sampling method that generates random samples that occur within intervals on the basis of equal probability and with normal distribution for each range. Quasi-random low-discrepancy sequences: (a)Halton sequence Halton : The Halton samples are generated according to the reversing or flipping the base conversion of numbers using primes. (b)Hammersley sequence Hammersley : The Hammersley sequence is the same as the Halton sequence, except in the first dimension where points are located equidistant from each other. (c) Sobol sequence Sobol : The Sobol sequence is a base-2 digital sequence that fills in a highly uniform manner. Figure 1: Examples of 400 points generated in[0,1]^2 using different uniform sampling methods in Section 2.2.1. 2.2.2 Uniform points with resampling In PINNs, a point at any location can be used to evaluate the PDE loss. Instead of using the fixed residual points during training, we could also select a new set of residual points in every certain optimization iteration [3]. The specific method to sample the points each time can be chosen from those methods discussed in Section 2.2.1. We can even use different sampling methods at different times, so many possible implementations make it impossible to be completely covered in this study. In this study, we only consider Random sampling with resampling (Random-R). The RandomR method is the same as the Random method, except that the residual points are resampled for everyNiteration. Theresampling periodNis also an important hyperparameter for accuracy, as we demonstrate in our empirical experiments in Section 3.","title":"2.2 Uniformly-distributed non-adaptive sampling"},{"location":"Models/PINNs/PN-2207.10289/#23-nonuniform-adaptive-sampling","text":"Although the uniform sampling strategies were predominantly employed, recent studies on the nonuniform adaptive sampling strategies [3, 37] have demonstrated promising improvement in the distribution of residual points during the training processes and achieved better accuracy. 2.3.1 Residual-based adaptive refinement with greed (RAR-G) The first adaptive sampling method for PINNs is the residual-based adaptive refinement method (RAR) proposed in Ref. [3]. RAR aims to improve the distribution of residual points during the training process by sampling more points in the locations where the PDE residual is large. Specifically, after every certain iteration, RAR adds new points in the locations with large PDE residuals (Algorithm 1). RAR only focuses on the points with large residual, and thus it is a greedy algorithm. To better distinguish from the other sampling methods, the RAR method is referred to as RAR-G in this study. Algorithm 1: RAR-G [3]. 1 Sample the initial residual pointsT using one of the methods in Section 2.2.1; 2 Train the PINN for a certain number of iterations; 3 repeat 4 Sample a set of dense pointsS 0 using one of the methods in Section 2.2.1; 5 Compute the PDE residuals for the points inS 0 ; 6 S \u2190mpoints with the largest residuals inS 0 ; 7 T \u2190T \u222aS; 8 Train the PINN for a certain number of iterations; 9 untilthe total number of iterations or the total number of residual points reaches the limit; 2.3.2 Residual-based adaptive distribution (RAD) RAR-G significantly improves the performance of PINNs when solving certain PDEs of solutions with steep gradients [3, 12]. Nevertheless, RAR-G focuses mainly on the location where the PDE residual is largest and disregards the locations of smaller residuals. Another sampling strategy was developed later in Ref. [37], where all the residual points are resampled according to a probability density function (PDF)p(x) proportional to the PDE residual. Specifically, for any pointx, we first compute the PDE residual\u03b5(x) =|f(x; \u02c6u(x))|, and then compute a probability as p(x)\u221d\u03b5(x), i.e., p(x) = \u03b5(x) A","title":"2.3 Nonuniform adaptive sampling"},{"location":"Models/PINNs/PN-2207.10289/#_15","text":"whereA=","title":""},{"location":"Models/PINNs/PN-2207.10289/#_16","text":"\u2126\u03b5(x)dxis a normalizing constant. Then all the residual points are sampled according top(x). This approach works for certain PDEs, but as we show in our numerical examples, it does not work well in some cases. Following this idea, we propose an improved version called the residualbased adaptive distribution (RAD) method (Algorithm 2), where we use a new PDF defined as p(x)\u221d \u03b5k(x) E[\u03b5k(x)] +c, (2) wherek\u22650 andc\u22650 are two hyperparameters. E[\u03b5k(x)] can be approximated by a numerical integration such as Monte Carlo integration. We note that the Random-R method in Section 2.2.2 is a special case of RAD by choosingk= 0 orc\u2192\u221e. Algorithm 2: RAD. 1 Sample the initial residual pointsT using one of the methods in Section 2.2.1; 2 Train the PINN for a certain number of iterations; 3 repeat 4 T \u2190A new set of points randomly sampled according to the PDF of Eq. (2); 5 Train the PINN for a certain number of iterations; 6 untilthe total number of iterations reaches the limit; In RAD (Algorithm 2 line 4), we need to sample a set of points according top(x), which can be done in a few ways. Whenxis low-dimensional, we can sample the points approximately in the following brute-force way: Sample a set of dense pointsS 0 using one of the methods in Section 2.2.1; Computep(x) for the points inS 0 ; Define a probability mass function \u0303p(x) =p(Ax)with the normalizing constantA=","title":"\u222b"},{"location":"Models/PINNs/PN-2207.10289/#_17","text":"x\u2208S 0 p(x); Sample a subset of points fromS 0 according to \u0303p(x). This method is simple, easy to implement, and sufficient for many PDE problems. For more complicated cases, we can use other methods such as inverse transform sampling, Markov chain Monte Carlo (MCMC) methods, and generative adversarial networks (GANs) [46]. The two hyperparameterskandcin Eq. (2) control the profile ofp(x) and thus the distribution of sampled points. We illustrate the effect ofkandcusing a simple 2D example, \u03b5(x,y) = 2^4 axa(1\u2212x)aya(1\u2212y)a, (3) witha= 10 in Fig. 2. Whenk= 0, it becomes a uniform distribution. As the value ofkincreases, more residual points will large PDE residuals are sampled. As the value ofcincreases, the residual points exhibit an inclination to be uniformly distributed. Compared with RAR, RAD provides more freedom to balance the points in the locations with large and small residuals by tuningkand c. The optimal values ofkandcare problem-dependent, and based on our numerical results, the combination ofk= 1 andc= 1 is usually a good default choice. 2.3.3 Residual-based adaptive refinement with distribution (RAR-D) We also propose a hybrid method of RAR-G and RAD, namely, residual-based adaptive refinement with distribution (RAR-D) (Algorithm 3). Similar to RAR-G, RAR-D repeatedly adds new points to the training dataset; similar to RAD, the new points are sampled based on the PDF in Eq. (2). We note that whenk\u2192 \u221e, only points with the largest PDE residual are added, which recovers RAR-G. The optimal values ofkandcare problem dependent, and based on our numerical results, the combination ofk= 2 andc= 0 is usually a good default choice. Figure 2:Examples of 1000 residual points sampled by RAD with different values ofk andcfor the PDE residual\u03b5(x,y)in Eq.(3). Algorithm 3: RAR-D. 1 Sample the initial residual pointsT using one of the methods in Section 2.2.1; 2 Train the PINN for a certain number of iterations; 3 repeat 4 S \u2190mpoints randomly sampled according to the PDF of Eq. (2); 5 T \u2190T \u222aS; 6 Train the PINN for a certain number of iterations; 7 untilthe total number of iterations or the total number of residual points reaches the limit;","title":"\u2211"},{"location":"Models/PINNs/PN-2207.10289/#24-comparison-with-related-work","text":"As discussed in Section 2.3, our proposed RAD and RAR-D are improved versions of the methods in Refs. [3, 37]. Here, we summarize the similarities between their methods and ours. Lu et al. [3] (in July 2019) proposed RAR (renamed to RAR-G here), which is a special case of RAR-D by choosing a large value ofk. The method proposed by Nabian et al. [37] (in April 2021) is a special case of RAD by choosingk= 1 andc= 0. During the preparation of this paper, a few new papers appeared [38, 39, 40, 41, 42, 43, 44] that also proposed similar methods. Here, we summarize the similarities and differences between these studies. The method proposed by Gao et al. [40] (in December 2021) is a special case of RAD by choosingc= 0. Tang et al. [41] (in December 2021) proposed two methods. One is a special case of RAD by choosingk= 2 andc= 0, and the other is a special case of RAR-D by choosingk= 2 and c= 0. Zeng et al. [43] (in April 2022) proposed a subdomain version of RAR-G. The entire domain is divided into many subdomains, and then new points are added to the several subdomains with large average PDE residual. Similar to RAR-G, Peng et al. [42] (in May 2022) proposed to add more points with large PDE residual, but they used the node generation technology proposed in Ref. [47]. We note that this method only works for a two-dimensional space. Zapf et al. [38] (in May 2022) proposed a modified version of RAR-G, where some points with small PDE residual are removed while adding points with large PDE residual. They show that compared with RAR, this reduces the computational cost, but the accuracy keeps similar. Hanna et al. [44] (in May 2022) proposed a similar method as RAR-D, but they chosep(x)\u221d max{log(\u03b5(x)/\u03b5 0 ), 0 }, where\u03b5 0 is a small tolerance. Similar to the work of Zapf et al., Daw et al. [39] (in July 2022) also proposed to remove the points with small PDE residual, but instead of adding new points with large PDE residual, they added new uniformly random sampled points. Thus all these methods are special cases of our proposed RAD and RAR-D (or with minor modification). However, in our study, two tunable variableskandcare introduced. As we show in our results, the values ofkandccould be crucial since they significantly influence the residual points distribution. By choosing proper values ofkandc, our methods would outperform the other methods. We also note that the point-wise weighting [16, 17, 8, 18] can be viewed as a special case of adaptive sampling, described as follows. When the residual points are randomly sampled from a uniform distributionU(\u2126), and the number of residual points is large, the PDE loss in Eq. (1) can be approximated byEU[\u03b5^2 (x)]. If we consider a point-wise weighting functionw(x), then the loss becomesEU[w(x)\u03b5^2 (x)], while for RAD the loss isEp[\u03b5^2 (x)]. If we choosew(x) (divided by a normalizing constant) as the PDFp(x), then the two losses are equal.","title":"2.4 Comparison with related work"},{"location":"Models/PINNs/PN-2207.10289/#3-results","text":"We apply PINNs with all the ten sampling methods in Section 2 to solve six forward and inverse PDE problems. In all examples, the hyperbolic tangent (tanh) is selected as the activation function. Table 1 summarizes the network width, depth, and optimizers used for each example. More details of the hyperparameters and training procedure can be found in each section of the specific problem. Table 1:The hyperparameters used for each numerical experiment.The learning rate of Adam optimizer is chosen as 0.001. Problems Depth Width Optimizer Section 3.2 Diffusion equation 4 32 Adam Section 3.3 Burgers\u2019 equation 4 64 Adam + L-BFGS Section 3.4 Allen-Cahn equation 4 64 Adam + L-BFGS Section 3.5 Wave equation 6 100 Adam + L-BFGS Section 3.6 Diffusion-reaction equation (inverse) 4 20 Adam Section 3.7 Korteweg-de Vries equation (inverse) 4 100 Adam For both forward and inverse problems, to evaluate the accuracy of the solution \u02c6u, theL^2 relative error is used: \u2016u\u02c6\u2212u\u2016 2 \u2016u\u2016 2","title":"3 Results"},{"location":"Models/PINNs/PN-2207.10289/#_18","text":"For inverse problems, to evaluate the accuracy of the predicted coefficients\u03bb\u02c6, the relative error is also computed: |\u03bb\u02c6\u2212\u03bb| |\u03bb|","title":""},{"location":"Models/PINNs/PN-2207.10289/#_19","text":"As the result of PINN has randomness due to the random sampling, network initialization, and optimization, thus, for each case, we run the same experiment at least 10 times and then compute the geometric mean and standard deviation of the errors. The code in this study is implemented by using the library DeepXDE [3] and is publicly available from the GitHub repositoryhttps: //github.com/lu-group/pinn-sampling.","title":""},{"location":"Models/PINNs/PN-2207.10289/#31-summary","text":"Here, we first present a summary of the accuracy of all the methods for the forward and inverse problems listed in Tables 2 and Table 3, respectively. A relatively small number of residual points is chosen to show the difference among different methods. In the specific section of each problem (Sections 3.2\u20133.7), we discuss all the detailed analyses, including the convergence of error during the training process, the convergence of error with respect to the number of residual points, and the effects of different hyperparameters (e.g., the period of resampling in Random-R, the values of kandcin RAD and RAR-D, and the number of new points added each time in RAR-D). We note that Random-R is a special case of RAD by choosingk= 0 orc\u2192 \u221e, and RAR-G is a special case of RAR-D by choosingk\u2192\u221e. Our main findings from the results are as follows. The proposed RAD method has always performed the best among the 10 sampling methods when solving all forward and inverse problems. For PDEs with complicated solutions, such as the Burgers\u2019 and multi-scale wave equation, the proposed RAD and RAR-D methods are predominately effective and yield errors magnitudes lower. For PDEs with smooth solutions, such as the diffusion equation and diffusion-reaction equa- tion, some uniform sampling methods, such as the Hammersley and Random-R, also produce sufficiently low errors. Compared with other uniform sampling methods, Random-R usually demonstrates better performance. Among the six uniform sampling methods with fixed residual points, the low-discrepancy sequences (Halton, Hammersley, and Sobol) generally perform better than Random and LHS, and both are better than Grid. Table 2: L^2 relative error of the PINN solution for the forward problems. Bold font indicates the smallest three errors for each problem. Underlined text indicates the smallest error for each problem. Diffusion Burgers\u2019 Allen-Cahn Wave No. of residual points 30 2000 1000 2000 Grid 0.66\u00b10.06% 13.7\u00b12.37% 93.4\u00b16.98% 81.3\u00b113.7% Random 0.74\u00b10.17% 13.3\u00b18.35% 22.2\u00b116.9% 68.4\u00b120.1% LHS 0.48\u00b10.24% 13.5\u00b19.05% 26.6\u00b115.8% 75.9\u00b133.1% Halton 0.24\u00b10.17% 4.51\u00b13.93% 0.29\u00b10.14% 60.2\u00b110.0% Hammersley 0.17\u00b10.07% 3.02\u00b12.98% 0.14\u00b10.14% 58.9\u00b18.52% Sobol 0.19\u00b10.07% 3.38\u00b13.21% 0.35\u00b10.24% 57.5\u00b114.7% Random-R 0.12\u00b10.06% 1.69\u00b11.67% 0.55\u00b10.34% 0.72\u00b10.90% RAR-G [3] 0.20\u00b10.07% 0.12\u00b10.04% 0.53\u00b10.19% 0.81\u00b10.11% RAD 0.11\u00b10.07% 0.02\u00b10.00% 0.08\u00b10.06% 0.09\u00b10.04% RAR-D 0.14\u00b10.11% 0.03\u00b10.01% 0.09\u00b10.03% 0.29\u00b10.04%","title":"3.1 Summary"},{"location":"Models/PINNs/PN-2207.10289/#32-diffusion-equation","text":"We first consider the following one-dimensional diffusion equation: \u2202u \u2202t","title":"3.2 Diffusion equation"},{"location":"Models/PINNs/PN-2207.10289/#_20","text":"\u2202^2 u \u2202x^2 +e\u2212t","title":"="},{"location":"Models/PINNs/PN-2207.10289/#_21","text":"\u2212sin(\u03c0x) +\u03c0^2 sin(\u03c0x)","title":"("},{"location":"Models/PINNs/PN-2207.10289/#_22","text":", x\u2208[\u2212 1 ,1],t\u2208[0,1], u(x,0) = sin(\u03c0x), u(\u2212 1 ,t) =u(1,t) = 0, whereuis the concentration of the diffusing material. The exact solution isu(x,t) = sin(\u03c0x)e\u2212t. We first compare the performance of the six uniform sampling methods with fixed residual points (Fig. 3A). The number of residual points is ranged from 10 to 80 with an increment of 10 points each time. For each number of residual points, the maximum iteration is set to be 15 000 with Adam as the optimizer. When the number of points is large (e.g., more than 70), all these methods Figure 3:L^2 relative errors of different sampling methods for the diffusion equation in Section 3.2.(A) Six uniform sampling with fixed residual points. (B) Random-R with different periods of resampling when using 30 residual points. (CandD) The training trajectory of RAD with different values ofkandcwhen using 30 residual points. (C)k= 1. (D)c= 1. (EandF) RAR-D with different values ofkandc. Each time one new point is added. (E)k= 2. (F)c= The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. Table 3:L^2 relative error of the PINN solution and relative error of the inferred parameters for the inverse problems.Bold font indicates the smallest three errors for each problem. Underlined text indicates the smallest error for each problem. Diffusion-reaction Korteweg-de Vries u(x) k(x) u(x,t) \u03bb 1 \u03bb 2 No. of residual points 15 600 Grid 0.36\u00b10.12% 8.58\u00b12.14% 24.4\u00b111.1% 53.7\u00b130.7% 42.0\u00b122.3% Random 0.35\u00b10.17% 5.77\u00b12.05% 8.86\u00b12.80% 16.4\u00b17.33% 16.8\u00b17.40% LHS 0.36\u00b10.14% 7.00\u00b12.62% 10.9\u00b12.60% 22.0\u00b16.68% 22.6\u00b16.36% Halton 0.23\u00b10.08% 6.16\u00b11.08% 8.76\u00b13.33% 16.7\u00b16.16% 17.2\u00b16.20% Hammersley 0.28\u00b10.08% 6.37\u00b10.91% 4.49\u00b13.56% 5.24\u00b17.08% 5.71\u00b17.32% Sobol 0.21\u00b10.06% 3.09\u00b10.75% 8.59\u00b13.67% 15.8\u00b16.15% 15.6\u00b15.79% Random-R 0.19\u00b10.09% 3.43\u00b11.80% 0.97\u00b10.15% 0.41\u00b10.30% 1.14\u00b10.31% RAR-G [3] 1.12\u00b10.11% 15.9\u00b11.53% 8.83\u00b11.98% 15.4\u00b19.29% 14.5\u00b19.25% RAD 0.17\u00b10.09% 2.76\u00b11.32% 0.77\u00b10.11% 0.31\u00b10.19% 0.86\u00b10.25% RAR-D 0.76\u00b10.24% 10.3\u00b13.28% 2.36\u00b10.98% 3.49\u00b12.21% 3.18\u00b12.02% have similar performance. However, when the number of residual points is small such as 50, the Hammersley and Sobol sequences perform better than others, and the equispaced uniform grid and random sampling have the largest errors (about one order of magnitude larger than Hammersley and Sobol). We then test the Random-R method using 30 residual points (Fig. 3B). The accuracy of Random-R has a strong dependence on the period of resampling, and the optimal period of resampling in this problem is around 200. Compared with Random without resampling, the Random-R method always leads to lowerL^2 relative errors regardless of the period of resampling. The error can be lower by one order of magnitude by choosing a proper resampling period. Among all the non-adaptive methods, Random-R performs the best. Next, we test the performance of the nonuniform adaptive sampling methods. In Algorithms 2 and 3, the neural network is first trained using 10 000 steps of Adam. In the RAD method, we use 30 residual points and resample every 1000 iterations. The errors of RAD with different values of kandcare shown in Figs. 3C and D. We note that Random-R is a special case of RAD with either c\u2192 \u221eork= 0. Here, RAD with large values ofcor small values ofkleads to better accuracy, i.e., the points are almost uniformly distributed. For the RAR-D method (Figs. 3E and F), one residual point is added after every 1000 iterations starting from 10 points. When usingk= 2 and c= 0 (the two red lines in Figs. 3E red F), RAR-D performs the best. When using 30 residual points, the errors of all the methods are listed in Table 2. In this diffusion equation, all the methods achieve a good accuracy (<1%). Compared with Random-R (0.12%), RAD and RAR-D (0.11%) are not significantly better. The reason could be that the solution of this diffusion equation is very smooth, so uniformly distributed points are good enough. In our following examples, we show that RAD and RAR-D work significantly better and achieve an error of orders of magnitude smaller than the non-adaptive methods.","title":")"},{"location":"Models/PINNs/PN-2207.10289/#33-burgers-equation","text":"The Burgers\u2019 equation is considered defined as: \u2202u \u2202t +u \u2202u \u2202x =\u03bd \u2202^2 u \u2202x^2 , x\u2208[\u2212 1 ,1],t\u2208[0,1], u(x,0) =\u2212sin(\u03c0x), u(\u2212 1 ,t) =u(1,t) = 0, whereuis the flow velocity and\u03bd is the viscosity of the fluid. In this study,\u03bdis set at 0. 01 /\u03c0. Different from the diffusion equation with a smooth solution, the solution of the Burgers\u2019 equation has a sharp front whenx= 0 andtis close to 1. We first test the uniform sampling methods by using the number of residual points ranging from 1,000 to 10,000 (Fig. 4A). The maximum iteration is 15,000 steps with Adam as optimizer followed by 15,000 steps of L-BFGS. Fig. 4A shows that the Hammersley method converges the fastest and reaches the lowestL^2 relative error among all the uniform sampling methods, while the Halton and Sobol sequences also perform adequately. Fig. 4B shows theL^2 relative error as a function of the period of resampling using the RandomR method with 2,000 residual points. Similar to the diffusion equation, the Random-R method always outperforms the Random method. However, the performance of Random-R is not sensitive to the period of resampling if the period is smaller than 100. Choosing a period of resampling too large can negatively affect its performance. When applying the nonuniform adaptive methods, the neural network is first trained using 15,000 steps of Adam and then 1,000 steps of L-BFGS. In the RAD method, we use 2000 residual points, which are resampled every 2,000 iterations (1,000 iterations using Adam followed by 1,000 iterations using L-BFGS). As indicated by Fig. 4C, the RAD method possesses significantly greater advantages over the Random-R method (a special case of RAD by choosingk= 0 orc\u2192 \u221e), whoseL^2 relative errors barely decrease during the training processes. This fact reflects that both extreme cases show worse performance. In contrast, fork= 1 andc= 1 (the red lines in Figs. 4C and D), theL^2 relative error declines rapidly and quickly reaches\u223c 2 \u00d7 10 \u2212^4. The RAD method is also effective when choosing a set ofkandcin a moderate range. For the RAR-D method, 1,000 residual points are selected in the pre-trained process, and 10 residual points are added every 2,000 iterations (1,000 iterations using Adam and 1,000 iterations using L-BFGS as optimizer) until the total number of residual points reaches 2,000. Shown by Figs. 4E and F, the optimal values forkandcare found to be 2 and 0, respectively. Since the solution of Burgers\u2019 equation has a very steep region, when using 2000 residual points, both RAD and RAR-D have competitive advantages over the uniform sampling methods in terms of accuracy and efficiency. For the following three forward PDE problems (Allen-Cahn equation in Section 3.4, wave equation in Section 3.5, and diffusion-reaction equation in Section 3.6), unless otherwise stated, the maximum iterations, the use of optimizer, and the training processes remain the same as the Burgers\u2019 equation. Table 2 summarizes theL^2 relative error for all methods when we fix the number of residual points at 2000. All uniform sampling methods fail to capture the solution well. TheL^2 relative errors given by the Halton, Hammersley, and Sobol methods (\u223c4%) are around one-fourth of that given by the Grid, Random, and LHS methods (>13%). Even though the Random-R performs the best among all uniform methods (1.69\u00b11.67%), the proposed RAD and RAR-D methods can achieve anL^2 relative error two orders of magnitude lower than that (0.02%). Figure 4:L^2 relative errors of different sampling methods for the Burgers\u2019 equation in Section 3.3.(A) Six uniform sampling with fixed residual points. (B) Random-R with different periods of resampling when using 2000 residual points. (CandD) The training trajectory of RAD with different values ofkandcwhen using 2000 residual points. (C)k= 1. (D)c= 1. (EandF) RAR-D with different values ofkandc. Each time 10 new points are added. (E)k= 2. (F)c= The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted.","title":"3.3 Burgers\u2019 equation"},{"location":"Models/PINNs/PN-2207.10289/#34-allen-cahn-equation","text":"Next, we consider the Allen-Cahn equation in the following form: \u2202u \u2202t","title":"3.4 Allen-Cahn equation"},{"location":"Models/PINNs/PN-2207.10289/#d","text":"\u2202^2 u \u2202x^2 5(u\u2212u^3 ), x\u2208[\u2212 1 ,1],t\u2208[0,1], u(x,0) =x^2 cos(\u03c0x), u(\u2212 1 ,t) =u(1,t) =\u2212 1 , where the diffusion coefficientD= 0.001. Fig. 5 outlines theL^2 relative errors of different sampling methods for the Allen-Cahn equation. Similar patterns are found for the nonadaptive uniform sampling as in the previous examples. The Hammersley method has the best accuracy (Fig. 5A). As the number of residual points becomes significantly large, the difference between these uniform sampling methods becomes negligible. Except for the equispaced uniform grid method, other uniform sampling methods converge toL^2 relative errors of 10\u2212^3 , about the same magnitude as the number of residual points reaching 10^4. Fig. 5B shows that when using 1000 residual points for Random-R, lowerL^2 relative errors can be obtained if we select a period of resampling less than 500. We next test the performance of RAD for different values ofkandcwhen using a different number of residual points. In Figs. 5C and D, we resampled 500 residual points every 2000 iteration, while in Figs. 5E and F, we used 1000 residual points instead. For both cases, the combination of k= 1 andc= 1 (the red lines in Figs. 5C\u2013F) gives good accuracy. When fewer residual points (e.g., 500) are used, the RAD methods boost the performance of PINNs. Similarly, we also test RAR-D in Figs. 5G\u2013J. In Figs. 5G and H, we pre-train the neural network with 500 residual points and add 10 residual points after every 2000 iterations until the total number of residual points reaches 1000. In Figs. 5I and J, we pre-train the neural network using 1000 residual points and heading to 2000 residual points in the same fashion. We recognize that 2 and 0 are the bestkandcvalues for the RAR-D method for both scenarios, which outperform the RAR-G method. As proven in this example, when applying the RAD and the RAR-D methods, the optimal values ofkandcremain stable even though we choose a different number of residual points. In addition, we find that the optimalkandcfor the Burgers\u2019 and Allen Cahn equations are the same for both the RAD and the RAR-D methods. Thus, we could choose (k= 1,c= 1) for the RAD methods and (k= 2,c= 0) for the RAR-D methods by default when first applied these methods to a new PDE problem. To make a comparison across all sampling methods, Table 2 shows theL^2 relative error for the Allen-Cahn equation when we fix the number of residual points at 1000. The Grid, Random, and LHS methods are prone to substantial errors, which are all larger than 20%. Nevertheless, the other four uniform methods (Halton, Hammersley, Sobol, and Random-R) have greater performance and can achieveL^2 relative errors of less than 1%. Remarkably, the RAD and RAR-D methods we proposed can further bring down theL^2 relative error below 0.1%. Figure 5:L^2 relative errors of different sampling methods for the Allen-Cahn equation in Section 3.4.(A) Six uniform sampling with fixed residual points. (B) Random-R with different periods of resampling when using 1000 residual points. (C\u2013F) The training trajectory of RAD with different values ofkandc. (C and D) 500 residual points are used. (C)k= 1. (D)c= 1. (E and F) 1000 residual points are used. (E)k= 1. (F)c= 1. (G\u2013J) RAR-D with different values ofk andc. (G and H) The number of residual points is increased from 500 to 1000. Each time 10 new points are added. (G)k= 2. (H)c= 0. (I and J) The number of residual points is increased from 1000 to 2000. Each time 10 new points are added. (I)k= 2. (J)c= 0. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. 18","title":"=D"},{"location":"Models/PINNs/PN-2207.10289/#35-wave-equation","text":"In this example, the following one-dimensional wave equation is considered: \u2202^2 u \u2202t^2","title":"3.5 Wave equation"},{"location":"Models/PINNs/PN-2207.10289/#4","text":"\u2202^2 u \u2202x^2 = 0, x\u2208[0,1],t\u2208[0,1], u(0,t) =u(1,t) = 0, t\u2208[0,1], u(x,0) = sin(\u03c0x) +","title":"\u2212 4"},{"location":"Models/PINNs/PN-2207.10289/#1_3","text":"","title":"1"},{"location":"Models/PINNs/PN-2207.10289/#2","text":"sin(4\u03c0x), x\u2208[0,1], \u2202u \u2202t (x,0) = 0, x\u2208[0,1], where the exact solution is given as: u(x,t) = sin(\u03c0x) cos(2\u03c0t) +","title":"2"},{"location":"Models/PINNs/PN-2207.10289/#1_4","text":"","title":"1"},{"location":"Models/PINNs/PN-2207.10289/#2_1","text":"sin(4\u03c0x) cos(8\u03c0t). The solution has a multi-scale behavior in both spatial and temporal directions. When we test the six uniform sampling methods, the number of residual points are ranged from 1000 to 6000, with an increment of 1000 each time. The Hammersley method achieves the lowest L^2 relative error with the fastest rate (Fig. 6A). When the number of residual points approaches 6000, the Random, Halton, and Hammersley methods can all obtain anL^2 relative error\u223c 10 \u2212^3. To determine the effectiveness of Random-R when using different numbers of residual points, we test the following three scenarios: small (1000 points), medium (4000 points), and large (10.000) sets of residual points (Figs. 6B, C, and D). In the medium case (Fig. 6C), the Random-R attainsL^2 relative errors magnitudes lower than the Random method. However, in the small and large cases (Figs. 6B and D), the Random-R methods show no advantage over the Random method regardless of the period of resampling. This is because when the number of residual points is small, both the Random and Random-R methods fail to provide accurate predictions. On the other hand, if the number of residual points is large, the predictions by the Random method are already highly accurate, so the Random-R is unable to further improve the accuracy. Since the optimal sets ofkandcfor both RAD and RAR-D methods are found to be the same for the Burgers\u2019 and the Allen Cahn equations, in this numerical experiment, we only apply the default settings (i.e., RAD:k= 1 andc= 1; RAR-D:k= 2 andc= 0) to investigate the effect of other factors, including the number of residual points for the RAD method and the number of points added to the RAR-D method. In Fig. 6E, we compare the performance of three nonuniform adaptive sampling methods under the same number of residual points from 1000 to 10 000. We first train the network using 15 000 iterations of Adam and 1000 iterations of L-BFGS, and then after each resampling in RAD or adding new points in RAR-D/RAR-G, we train the network with 1000 iterations of L-BFGS. For the RAR-G and the RAR-D methods, we first train the network with 50% of the final number of the residual points and add 10 residual points each time until reaching the total number of residual points. As we can see from Fig. 6E, the RAD achieves much better results when the number of residual points is small. As the number of residual points increases, the RAR-D method acts more effectively and eventually reaches comparable accuracy to the RAD method. Since the RAD method is more computationally costly than the RAR-D methods with the same number of residual points, we suggest applying the RAD method when the number of residual points is small and the RAR-D method when the number of residual points is large. We next investigate the RAD method with a different number of residual points (i.e., 1000, 2000, 5000, and 10 000). Fig. 6F illustrates that if we increase the number of residual points, lower Figure 6: L^2 relative errors of different sampling methods for the wave equation in Section 3.5.(A) Six uniform sampling with fixed residual points. (B,C, andD) Random-R with different periods of resampling when using (B) 1000 residual points, (C) 4000 residual points, and (D) 10000 residual points. (E) Comparison among RAD (k= 1 andc= 1), RAR-D (k= 2 and c= 0), and RAR-G for different numbers of residual points. (F) The training trajectory of RAD (k= 1 andc= 1) uses different numbers of residual points. (GandH) Convergence of RAR-D (k= 2 andc= 0) when adding a different number of new points each time. (G) New points are added starting from 1000 residual points. (H) New points are added starting from 2500 residual points. (I) Convergence of RAR-G when adding a different number of new points each time. New points are added starting from 2500 residual points. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. L^2 relative error can be achieved but with diminishing marginal effect. We train the network for more than 500 000 iterations to see if theL^2 relative error can further decrease. However, theL^2 relative errors converge and remain relatively stable after 100 000 iterations. One important factor to consider in the RAR-D and the RAR-G methods is how new points are added. We can either add a small number of residual points each time and prolong the training process or add a large number of residual points each time and shorten the process. In Fig. 6G, we first train the network with 1000 residual points and then add new residual points at different rates until the total number of residual points reaches 2000. After adding new residual points each time, we train the network using 1000 steps of L-BFGS. Likewise, in Fig. 6H, we first train the network with 2500 residual points and add new points at different rates until the total number of residual points reaches 5000. In both cases (Figs. 6G and H) that use the RAR-D methods, we find that the best strategy is to add 10 points each time. However, shown by two red-shaded regions in Figs. 6G and H, the results are more stable when we use a larger number of residual points. Fig. 6I is set up the same way as Fig. 6H but tests the RAR-G method. The best strategy for the RAR-G is identical to that of the RAR-D. Table 2 outlines theL^2 relative error for the wave equation using all methods when the number of residual points equals 2000. All uniform methods with fixed residual points perform poorly (error >50%) and fail to approximate the truth values. Random-R, as a special case of the proposed RAD, givesL^2 relative errors of around 1%. The RAR-D method significantly enhances the prediction accuracy resulting inL^2 relative errors under 0.3%. In addition, the RAD with the default setting ofkandcconverges toL^2 relative errors under 0.1%.","title":"2"},{"location":"Models/PINNs/PN-2207.10289/#36-diffusion-reaction-equation","text":"The first inverse problem we consider is the diffusion-reaction system as follows: \u03bb d^2 u dx^2 \u2212k(x)u=f, x\u2208[0,1], wheref = sin(2\u03c0x) is the source term. \u03bb= 0.01 is the diffusion coefficient, anduis the solute concentration. In this problem, we aim to infer the space-dependent reaction ratek(x) with given measurements on the solutionu. The exact unknown reaction rate is k(x) = 0.1 +e\u2212^0.^5 (x\u2212 0 .5)^2 (^152). We aim to learn the unknown functionk(x) and solve foru(x) by using eight observations ofu, which are uniformly distributed on the domainx\u2208[0,1], including two points on both sides of the boundaries. TheL^2 relative errors for both the solutionu(Figs. 7A, C, and E) and the unknown functionk(Figs. 7B, D, and F) are computed. The maximum number of iterations is 50 000 steps of Adam. Figs. 7A and B summarize the performance of all uniform sampling methods. We note that in 1D, the Hammersley and Halton sequences are identical and outperform other uniform methods. We fix the residual points at 15 and compare the Random method with the Random-R method. TheL^2 relative errors (Figs. 7C and D) given by the Random-R remain steady, disregarding the changes in the period of resampling, and are approximately the same as that produced by the Random method. This is because the reaction-diffusion system is fairly simple and can be easily handled by uniform sampling methods without resampling. Next, we compare the Random, RAD, RAR-G, and RAR-D methods with default settings (i.e., RAD:k= 1 andc= 1; RAR-D:k= 2 andc= 0) using a different number of residual points. For the random and RAD methods, the maximum number of iterations is 50 000 steps of Adam. For Figure 7:L^2 relative errors of different sampling methods foruandkin the diffusionreaction equation in Section 3.6.(AandB) Six uniform sampling with fixed residual points. (CandD) Random-R with different periods of resampling when using 15 residual points. (Eand F) Comparison among Random, RAD (k= 1 andc= 1), RAR-G, and RAR-D (k= 2 andc= 0) for different numbers of residual points. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. the RAR-G/RAR-D, we first train the neural network with 50% of the total number of residual points for 10 000 steps of Adam; then we add one point each time and train for 1000 steps of Adam until we meet the total number of residual points. As shown by Figs. 7E and F, the RAD method surpasses other methods and is able to produce lowL^2 relative error even when the number of residual points is very small. However, RAR-G and RAR-D are even worse than the Random sampling. To sum up, we fix the number of residual points at 15 and present theL^2 relative error for both the solution and unknown function in Table 3. The RAD yields the minimumL^2 relative error (0.17% foru(x); 2.76% fork(x)). However, due to the simplicity of this PDE problem, some uniform sampling methods, especially the Sobol and Random-R, have comparable performance to the RAD. Generally speaking, we recognize that the uniform sampling methods are adequate when solving this inverse PDE with smooth solutions. Still, the RAD method can further enhance the performance of PINNs, especially when the number of residual points is small.","title":"3.6 Diffusion-reaction equation"},{"location":"Models/PINNs/PN-2207.10289/#37-korteweg-de-vries-equation","text":"The second inverse problem we solve is the Korteweg-de Vries (KdV) equation: \u2202u \u2202t +\u03bb 1 u \u2202u \u2202x +\u03bb 2 \u2202^3 u \u2202x^3 = 0, x\u2208[\u2212 1 ,1], t\u2208[0,1], where\u03bb 1 and\u03bb 2 are two unknown parameters. The exact values for\u03bb 1 and\u03bb 2 are 1 and 0.0025, respectively. The initial condition isu(x,t= 0) = cos(\u03c0x), and periodic boundary conditions are used. To infer\u03bb 1 and\u03bb 2 , we assume that we have the observations of two solution snapshots u(x,t= 0.2) andu(x,t= 0.8) at 64 uniformly distributed points at each time. In Fig. 8, the first column (Figs. 8A, D, and G) shows theL^2 relative error of the solutionu, while the second column (Figs. 8B, E, and H) and the third column (Figs. 8C, F, and I) illustrate the relative errors for\u03bb 1 and\u03bb 2 , respectively. The maximum iteration is 100 000 steps of Adam. Hammersley achieves better accuracy than the other uniform sampling methods. The Sobol and Halton methods behave comparably as these two curves (the yellow and green curves in Figs. 8A, B, and C) are almost overlapping. Shown in Figs. 8D, E and F, the Random-R method yields higher accuracy than the Random method by about one order of magnitude in all cases when using 1000 residual points. A smaller period of resampling leads to smaller errors. Figs. 8G, H, and I compare the Random-R, Random, RAD, RAR-G, and RAR-D methods using the same number of residual points and the total number of iterations. For the Random and the Random-R methods, we train the network for 100 000 steps of Adams. For the RAD methods, we first train the network using 50 000 steps of Adams; then, we resample the residual points and train for 1000 steps of Adams 50 times. In order to fix the total number of iterations for the RARG/RAR-D methods to 100 000, we accordingly adjust the number of new residual points added each time. For example, if the final number of residual points is 500, we first train the network using 250 residual points (i.e., 50% of the total number of residual points) with 50 000 steps of Adams; and we consequently add 5 points and train for 1000 steps of Adams each time. If the final number of residual points is 1000, we first train the network using 500 residual points with 50 000 steps of Adams; and then we add 10 points and train for 1000 steps of Adams each time. As demonstrated by Figs. 8G, H, and I, the RAD method is the best, while the Random-R method is also reasonably accurate. We show one example of the training process (Figs. 8J, K, and L) when the number of residual points is 600 to illustrate the convergence of the solution,\u03bb 1 , and\u03bb 2 during training. The resampling strategies, especially the RAD method, achieve the greatest success among all sampling methods. Figure 8:L^2 relative errors ofuand relative errors of\u03bb 1 and\u03bb 2 using different sampling methods for the Korteweg-de Vries equation in Section 3.7. (A,B, andC) Six uniform sampling with fixed residual points. (D,E, andF) Random-R with different periods of resampling when using 1000 residual points. (G,H, andI) Comparison among Random, Random-R, RAD (k= 1 andc= 1), RAR-G, and RAR-D (k= 2 andc= 0) for different number of residual points. (J,K, andL) Examples of the training trajectories using Random, Random-R, RAD (k= 1 and c= 1), RAR-G, and RAR-D (k= 2 andc= 0) with 600 residual points. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. Table 3 demonstrates theL^2 relative errors for the solutionu(x,t) and the relative error of two unknown parameters\u03bb 1 and\u03bb 2 , for all methods when the number of residual points is set at The lowestL^2 relative errors for uniform sampling with fixed points are given by Hammersley (\u223c 5%). The Random-R is the second-best method and providesL^2 relative errors of around 1%. With the smallest errors (<1%) and standard deviations, the RAD method has compelling advantages over all other methods in terms of accuracy and robustness. It is noteworthy that the RAR-D method provides adequate accuracy (\u223c3%) and is less expensive than the Random-R and RAD methods when the number of residual points is the same. Therefore, the RAR-D is also a valuable approach to consider.","title":"3.7 Korteweg-de Vries equation"},{"location":"Models/PINNs/PN-2207.10289/#4-conclusions","text":"In this paper, we present a comprehensive study of two categories of sampling for physics-informed neural networks (PINNs), including non-adaptive uniform sampling and adaptive nonuniform sampling. For the non-adaptive uniform sampling, we have considered six methods: (1) equispaced uniform grid (Grid), (2) uniformly random sampling (Random), (3) Latin hypercube sampling (LHS), (4) Halton sequence (Halton), (5) Hammersley sequence (Hammersley), and (6) Sobol sequence (Sobol). We have also considered a resampling strategy for uniform sampling (Random-R). For the adaptive nonuniform sampling, motivated by the residual-based adaptive refinement with greed (RAR-G) [3], we proposed two new residual-based adaptive sampling methods: residual-based adaptive distribution (RAD) and residual-based adaptive refinement with distribution (RAR-D). We extensively investigated the performance of these ten sampling methods in solving four forward and two inverse problems of partial differential equations (PDEs) with many setups, such as a different number of residual points. Our results show that the proposed RAD and RAR-D significantly improve the accuracy of PINNs by orders of magnitude, especially when the number of residual points is small. RAD and RAR-D also have great advantages for the PDEs with complicated solutions, e.g., the solution of the Burgers\u2019 equation with steep gradients and the solution of the wave equation with a multi-scale behavior. A summary of the comparison of these methods can be found in Section 3.1. Based on our empirical results, we summarize the following suggestions as a practical guideline in choosing sampling methods for PINNs. RAD withk= 1 andc= 1 can be chosen as the default sampling method when solving a new PDE. The hyperparameterskandccan be tuned to balance the points in the locations with large and small PDE residuals. RAR-D can achieve comparable accuracy to RAD, but RAR-D is more computationally efficient as it gradually increases the number of residual points. Hence, RAR-D (k= 2 and c= 0 by default) is preferable for the case with limited computational resources. Random-R can be used in the situation where adaptive sampling is not allowed, e.g., it is difficult to sample residual points according to a probability density function. The period of resampling should not be chosen as too small or too large. A low-discrepancy sequence (e.g., Hammersley) should be considered rather than Grid, Ran- dom, or LHS, when we have to use a fixed set of residual points, such as in PINNs with the augmented Lagrangian method (hPINNs) [8]. In this study, we sample residual points in RAD and RAR-D by using a brute-force approach, which is simple, easy to implement, and sufficient for many PDEs. However, for high-dimensional problems, we need to use other methods, such as generative adversarial networks (GANs) [46], as was done in Ref. [41]. Moreover, the probability of sampling a pointxis only considered as p(x)\u221d \u03b5 k(x) E[\u03b5k(x)]+c. While this probability works very well in this study, it is possible that there exists another better choice. We can learn a new probability density function by meta-learning, as was done for loss functions of PINNs in Ref. [11].","title":"4 Conclusions"},{"location":"Models/PINNs/PN-2207.10289/#references","text":"[1] M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.Journal of Computational Physics, 378:686\u2013707, 2019. [2] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations.Science, 367(6481):1026\u20131030, 2020. [3] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. DeepXDE: A deep learning library for solving differential equations.SIAM Review, 63(1):208\u2013228, 2021. [4] George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning.Nature Reviews Physics, 3(6):422\u2013440, 2021. [5] Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse problems in nano-optics and metamaterials.Optics Express, 28(8):11618, 2020. [6] Alireza Yazdani, Lu Lu, Maziar Raissi, and George Em Karniadakis. Systems biology informed deep learning for inferring parameters and hidden dynamics. PLOS Computational Biology, 16(11), 2020. [7] Mitchell Daneker, Zhen Zhang, George Em Kevrekidis, and Lu Lu. Systems biology: Identifiability analysis and parameter identification via systems-biology informed neural networks. arXiv preprint arXiv:2202.01723, 2022. [8] Lu Lu, Rapha \u0308el Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G. Johnson. Physics-informed neural networks with hard constraints for inverse design. SIAM Journal on Scientific Computing, 43(6), 2021. [9] Guofei Pang, Lu Lu, and George Em Karniadakis. fPINNs: Fractional physics-informed neural networks. SIAM Journal on Scientific Computing, 41(4), 2019. [10] Dongkun Zhang, Lu Lu, Ling Guo, and George Em Karniadakis. Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems.Journal of Computational Physics, 397:108850, 2019. [11] Apostolos F Psaros, Kenji Kawaguchi, and George Em Karniadakis. Meta-learning PINN loss functions. Journal of Computational Physics, 458:111121, 2022. [12] Jeremy Yu, Lu Lu, Xuhui Meng, and George Em Karniadakis. Gradient-enhanced physicsinformed neural networks for forward and inverse PDE problems.Computer Methods in Applied Mechanics and Engineering, 393:114823, 2022. [13] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow pathologies in physics-informed neural networks. SIAM Journal on Scientific Computing, 43(5):A3055\u2013A3081, 2021. [14] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why PINNs fail to train: A neural tangent kernel perspective. Journal of Computational Physics, 449:110768, 2022. [15] Zixue Xiang, Wei Peng, Xu Liu, and Wen Yao. Self-adaptive loss balanced physics-informed neural networks.Neurocomputing, 2022. [16] Levi McClenny and Ulisses Braga-Neto. Self-adaptive physics-informed neural networks using a soft attention mechanism.arXiv preprint arXiv:2009.04544, 2020. [17] Yiqi Gu, Haizhao Yang, and Chao Zhou. SelectNet: Self-paced learning for high-dimensional partial differential equations.Journal of Computational Physics, 441:110444, 2021. [18] Wensheng Li, Chao Zhang, Chuncheng Wang, Hanting Guan, and Dacheng Tao. Revisiting PINNs: Generative adversarial physics-informed neural networks and point-weighting method. arXiv preprint arXiv:2205.08754, 2022. [19] Xuhui Meng, Zhen Li, Dongkun Zhang, and George Em Karniadakis. PPINN: Parareal physicsinformed neural network for time-dependent pdes. Computer Methods in Applied Mechanics and Engineering, 370:113250, 2020. [20] Khemraj Shukla, Ameya D. Jagtap, and George Em Karniadakis. Parallel physics-informed neural networks via domain decomposition. Journal of Computational Physics, 447:110683, 2021. [21] Ameya D. Jagtap and George Em Karniadakis. Extended physics-informed neural networks (XPINNs): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations. Communications in Computational Physics, 28(5):2002\u20132041, 2020. [22] Colby L Wight and Jia Zhao. Solving Allen-Cahn and Cahn-Hilliard equations using the adaptive physics informed neural networks. arXiv preprint arXiv:2007.04542, 2020. [23] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks.Advances in Neural Information Processing Systems, 34:26548\u201326560, 2021. [24] Revanth Mattey and Susanta Ghosh. A novel sequential method to train physics informed neural networks for allen cahn and cahn hilliard equations. Computer Methods in Applied Mechanics and Engineering, 390:114474, 2022. [25] Katsiaryna Haitsiukevich and Alexander Ilin. Improved training of physics-informed neural networks with model ensembles. arXiv preprint arXiv:2204.05108, 2022. [26] Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality is all you need for training physics-informed neural networks. arXiv preprint arXiv:2203.07404, 2022. [27] Pola Lydia Lagari, Lefteri H Tsoukalas, Salar Safarkhani, and Isaac E Lagaris. Systematic construction of neural forms for solving partial differential equations inside rectangular domains, subject to initial, boundary and interface conditions. International Journal on Artificial Intelligence Tools, 29(05):2050009, 2020. [28] Suchuan Dong and Naxian Ni. A method for representing periodic functions and enforcing exactly periodic boundary conditions with deep neural networks. Journal of Computational Physics, 435:110242, 2021. [29] Michael D McKay, Richard J Beckman, and William J Conover. A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. Technometrics, 42(1):55\u201361, 2000. [30] Michael Stein. Large sample properties of simulations using Latin hypercube sampling.Technometrics, 29(2):143\u2013151, 1987. [31] Il\u2019ya Meerovich Sobol\u2019. On the distribution of points in a cube and the approximate evaluation of integrals.Zhurnal Vychislitel\u2019noi Matematiki i Matematicheskoi Fiziki, 7(4):784\u2013802, 1967. [32] John H Halton. On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals. Numerische Mathematik, 2(1):84\u201390, 1960. [33] JM Hammersley and DC Handscomb. Monte-Carlo methods, mathuen, 1964. [34] Hongwei Guo, Xiaoying Zhuang, Xiaoyu Meng, and Timon Rabczuk. Analysis of three dimensional potential problems in non-homogeneous media with deep learning based collocation method.arXiv preprint arXiv:2010.12060, 2020. [35] Sourav Das and Solomon Tesfamariam. State-of-the-art review of design of experiments for physics-informed deep learning.arXiv preprint arXiv:2202.06416, 2022. [36] Zhiping Mao, Ameya D Jagtap, and George Em Karniadakis. Physics-informed neural networks for high-speed flows. Computer Methods in Applied Mechanics and Engineering, 360:112789, 2020. [37] Mohammad Amin Nabian, Rini Jasmine Gladstone, and Hadi Meidani. Efficient training of physics-informed neural networks via importance sampling.Computer-Aided Civil and Infrastructure Engineering, 2021. [38] Bastian Zapf, Johannes Haubner, Miroslav Kuchta, Geir Ringstad, Per Kristian Eide, and Kent-Andre Mardal. Investigating molecular transport in the human brain from MRI with physics-informed neural networks.arXiv preprint arXiv:2205.02592, 2022. [39] Arka Daw, Jie Bu, Sifan Wang, Paris Perdikaris, and Anuj Karpatne. Rethinking the importance of sampling in physics-informed neural networks. arXiv preprint arXiv:2207.02338, 2022. [40] Wenhan Gao and Chunmei Wang. Active learning based sampling for high-dimensional nonlinear partial differential equations. arXiv preprint arXiv:2112.13988, 2021. [41] Kejun Tang, Xiaoliang Wan, and Chao Yang. DAS: A deep adaptive sampling method for solving partial differential equations.arXiv preprint arXiv:2112.14038, 2021. [42] Wei Peng, Weien Zhou, Xiaoya Zhang, Wen Yao, and Zheliang Liu. RANG: a residualbased adaptive node generation method for physics-informed neural networks. arXiv preprint arXiv:2205.01051, 2022. [43] Shaojie Zeng, Zong Zhang, and Qingsong Zou. Adaptive deep neural networks methods for high-dimensional partial differential equations.Journal of Computational Physics, 463:111232, 2022. [44] John M Hanna, Jose V Aguado, Sebastien Comas-Cardona, Ramzi Askri, and Domenico Borzacchiello. Residual-based adaptivity for two-phase flow simulation in porous media using physics-informed neural networks.Computer Methods in Applied Mechanics and Engineering, 396:115100, 2022. [45] Melissa E O\u2019Neill. Pcg: A family of simple fast space-efficient statistically good algorithms for random number generation.ACM Transactions on Mathematical Software, 2014. [46] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.Advances in neural information processing systems, 27, 2014. [47] Bengt Fornberg and Natasha Flyer. Fast generation of 2-D node distributions for mesh-free pde discretizations.Computers & Mathematics with Applications, 69(7):531\u2013544, 2015.","title":"References"},{"location":"Models/PINNs/PN-2210.00279/","text":"Failure-Informed Adaptive Sampling for PINNs \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u5931\u8d25\u4fe1\u606f\u81ea\u9002\u5e94\u91c7\u6837","title":"Failure-Informed Adaptive Sampling for PINNs <br> \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u5931\u8d25\u4fe1\u606f\u81ea\u9002\u5e94\u91c7\u6837"},{"location":"Models/PINNs/PN-2210.00279/#failure-informed-adaptive-sampling-for-pinns","text":"","title":"Failure-Informed Adaptive Sampling for PINNs  \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u5931\u8d25\u4fe1\u606f\u81ea\u9002\u5e94\u91c7\u6837"},{"location":"Models/PINNs/PN-2210.12914/","text":"A Novel Adaptive Causal Sampling Method for PINNs \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u4e00\u79cd\u65b0\u81ea\u9002\u5e94\u56e0\u679c\u91c7\u6837\u65b9\u6cd5","title":"A Novel Adaptive Causal Sampling Method for PINNs <br> \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u4e00\u79cd\u65b0\u81ea\u9002\u5e94\u56e0\u679c\u91c7\u6837\u65b9\u6cd5"},{"location":"Models/PINNs/PN-2210.12914/#a-novel-adaptive-causal-sampling-method-for-pinns","text":"","title":"A Novel Adaptive Causal Sampling Method for PINNs  \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u4e00\u79cd\u65b0\u81ea\u9002\u5e94\u56e0\u679c\u91c7\u6837\u65b9\u6cd5"},{"location":"Songs/%E6%B5%AA%E6%BC%AB%E4%B9%8B%E7%BA%A6/","text":"\u30ed\u30de\u30f3\u30b9\u306e\u7d04\u675f/\u6d6a\u6f2b\u4e4b\u7ea6 \u4f5c\u8bcd\uff1a\u5e7e\u7530\u308a\u3089 \u4f5c\u66f2\uff1a\u5e7e\u7530\u308a\u3089 \u6f14\u5531\uff1a\u5e7e\u7530\u308a\u3089 \u3053\u308c\u304b\u3089\u4e8c\u4eba(\u3075\u305f\u308a)\u904e(\u3059)\u3054\u3057\u3066\u3044\u304f\u305f\u3081\u306b \u4e3a\u4e86\u4eca\u540e\u4e24\u4e2a\u4eba\u7684\u751f\u6d3b \u7d04\u675f(\u3084\u304f\u305d\u304f)\u3057\u3066\u307b\u3057\u3044\u3053\u3068\u304c\u3042\u308b\u306e \u60f3\u8981\u548c\u4f60\u6709\u4e2a\u7ea6\u5b9a \u58f0(\u3053\u3048)\u304c\u67af(\u304b)\u308c\u3066\u540d\u524d(\u306a\u307e\u3048)\u304c\u547c(\u3088)\u3079\u306a\u304f\u306a\u308b \u76f4\u5230\u58f0\u97f3\u6c99\u54d1\u5730\u65e0\u6cd5\u547c\u5524\u5f7c\u6b64\u59d3\u540d \u305d\u306e\u65e5(\u3072)\u307e\u3067\u5fd8(\u308f\u3059)\u308c\u306a\u3044\u3067 \u90a3\u5929\u4e4b\u524d\u90fd\u4e0d\u8981\u5fd8\u8bb0 \u5149(\u3072\u304b\u308a)\u3092\u63a2(\u3055\u304c)\u3059\u3088\u3046\u306a\u7720(\u306d\u3080)\u308c\u306a\u3044\u591c(\u3088\u308b)\u306f \u4eff\u4f5b\u5728\u5bfb\u627e\u5149\u660e\u822c\u65e0\u6cd5\u5165\u7720\u7684\u591c\u665a \u671d(\u3042\u3055)\u307e\u3067\u624b(\u3066)\u3092\u63e1(\u306b\u304e)\u3063\u3066\u3044\u3066\u307b\u3057\u3044 \u5e0c\u671b\u4f60\u80fd\u4e00\u76f4\u63e1\u7740\u6211\u7684\u624b\u76f4\u5230\u6e05\u6668 \u6ca2\u5c71(\u305f\u304f\u3055\u3093)\u306e\u611b(\u3042\u3044)\u3067\u6ea2(\u3042\u3075)\u308c\u305f\u306a\u3089 \u5982\u679c\u7231\u591a\u5230\u6ee1\u6ea2\u800c\u51fa \u660e(\u3042)\u3051\u306a\u3044\u591c(\u3088\u308b)\u306e\u5922(\u3086\u3081)\u3092\u898b(\u307f)\u305b\u3066\u307b\u3057\u3044 \u5e0c\u671b\u4f60\u80fd\u8ba9\u6211\u505a\u4e00\u4e2a\u6c38\u4e0d\u7834\u6653\u7684\u7f8e\u68a6 \u5929\u79e4(\u3066\u3093\u3073\u3093)\u306f\u3044\u3064\u3082\u50be(\u304b\u305f\u3080)\u304f\u3051\u3069 \u5929\u5e73\u5c3d\u7ba1\u603b\u662f\u503e\u5411\u4e00\u7aef \u4eca\u591c(\u3053\u3093\u3084)\u3060\u3051\u306f\u540c(\u304a\u306a)\u3058\u3067\u3044\u305f\u3044 \u54ea\u6015\u53ea\u5728\u4eca\u665a\u4e5f\u60f3\u8ba9\u5b83\u8b8a\u5f97\u4e00\u6837 \u4e8c\u4eba(\u3075\u305f\u308a)\u3067\u9032(\u3059\u3059)\u307f\u59cb(\u306f\u3058)\u3081\u305f\u3053\u306e\u5217\u8eca(\u308c\u3063\u3057\u3083)\u306e \u4e24\u4e2a\u4eba\u4e00\u8d77\u5f00\u59cb\u4e58\u5750\u7684\u8fd9\u8d9f\u5217\u8f66 \u5207\u7b26(\u304d\u3063\u3077)\u306f\u6700\u5f8c(\u3055\u3044\u3054)\u307e\u3067\u5931(\u306a)\u304f\u3055\u306a\u3044\u3067\u306d \u76f4\u5230\u6700\u540e\u90fd\u8bf7\u4e0d\u8981\u5c06\u8f66\u7968\u5f04\u4e22\u54e6 \u3082\u3057\u3082\u884c(\u3086)\u304d\u5148(\u3055\u304d)\u3092\u898b(\u307f)\u5931(\u3046\u3057\u306a)\u3063\u305f\u306a\u3089 \u5982\u679c\u8ff7\u5931\u4e86\u76ee\u7684\u5730\u7684\u8bdd \u305d\u306e\u5834\u6240(\u3070\u3057\u3087)\u3067\u307e\u305f\u59cb(\u306f\u3058)\u3081\u3088\u3046 \u5c31\u5728\u90a3\u4e2a\u5730\u65b9\u91cd\u65b0\u5f00\u59cb\u5427 \u982c(\u307b\u307b)\u3092\u6fe1(\u306c)\u3089\u3059\u3088\u3046\u306a\u7720(\u306d\u3080)\u308c\u306a\u3044\u591c(\u3088\u308b)\u306f \u5728\u6cea\u6e7f\u8138\u988a\u7684\u4e0d\u7720\u4e4b\u591c \u5fc3\u5730(\u3053\u3053\u3061)\u3044\u3044\u5de6\u80a9(\u3072\u3060\u308a\u304b\u305f)\u3092\u8cb8(\u304b)\u3057\u3066\u307b\u3057\u3044 \u8bf7\u501f\u7ed9\u6211\u4f60\u90a3\u8212\u9002\u7684\u5de6\u80a9 \u6ca2\u5c71(\u305f\u304f\u3055\u3093)\u306e\u611b(\u3042\u3044)\u3092\u77e5(\u3057)\u308c\u305f\u306e\u306a\u3089 \u5982\u679c\u77e5\u9053\u4e86\u6211\u6eff\u5fc3\u7684\u7231\u610f \u53e3\u7d05(\u304f\u3061\u3079\u306b)\u3092\u6eb6(\u3068)\u304b\u3059\u3088\u3046\u306a\u30ad\u30b9(\u304d\u3059)\u3092\u3057\u3066 \u5c31\u7ed9\u6211\u6765\u4e2a\u80fd\u5c06\u53e3\u7ea2\u90fd\u878d\u5316\u7684\u543b\u5427 \u305d\u306e\u3042\u3068\u306f\u9f3b\u5148(\u306f\u306a\u3055\u304d)\u3067\u304f\u3059\u3063\u3068\u7b11(\u308f\u3089)\u3063\u3066 \u7136\u540e\u8138\u4e0a\u5e26\u7740\u7b11\u610f \u7d42(\u304a)\u308f\u308a\u306f\u306a\u3044\u3068\u8a00(\u3044)\u3063\u3066\u62b1(\u3060)\u304d\u3057\u3081\u3066 \u8bf4\u8fd8\u6ca1\u7ed3\u675f\u518d\u7d27\u62b1\u4f4f\u6211 \u541b(\u304d\u307f)\u306e\u77ed\u6240(\u305f\u3093\u3057\u3087)\u3084\u79c1(\u308f\u305f\u3057)\u306e\u9577\u6240(\u3061\u3087\u3046\u3057\u3087)\u304c\u5909(\u304b)\u308f\u3063\u3066\u3057\u307e\u3063\u3066\u3082 \u5373\u4f7f\u4f60\u7684\u7f3a\u70b9\u548c\u6211\u7684\u4f18\u70b9\u90fd\u6539\u53d8\u4e86 \u4ee3(\u304b)\u308f\u308a\u306f\u5c45(\u3044)\u306a\u3044\u3088 \u304d\u3063\u3068 \u4e5f\u6ca1\u6709\u80fd\u591f\u4ee3\u66ff\u7684\u5b58\u5728 \u601d(\u304a\u3082)\u3044\u51fa(\u3067)\u304c\u793a(\u3057\u3081)\u3059\u3088 \u307e\u305f\u624b(\u3066)\u3092\u53d6(\u3068)\u308d\u3046 \u50cf\u8bb0\u5fc6\u4e2d\u4e00\u6837\u518d\u6b21\u7275\u7740\u624b\u5427 \u661f\u5c51(\u307b\u3057\u304f\u305a)\u306e\u3088\u3046\u306a\u3053\u306e\u4e16\u754c(\u305b\u304b\u3044)\u3067 \u5728\u8fd9\u5982\u661f\u5c18\u822c\u7684\u4e16\u754c\u91cc \u7167(\u3066)\u3089\u3055\u308c\u305f\u5149(\u3072\u304b\u308a)\u306e\u5148(\u3055\u304d)\u306b\u3044\u305f\u3093\u3060 \u5728\u88ab\u7167\u8000\u7684\u5149\u8292\u4e4b\u524d \u541b(\u304d\u307f)\u306e\u307e\u307e\u305d\u306e\u307e\u307e\u304c\u7f8e(\u3046\u3064\u304f)\u3057\u3044\u304b\u3089 \u4f60\u662f\u5982\u6b64\u7684\u7f8e\u4e3d \u305d\u308c\u3067\u3044\u3044 \u305d\u308c\u3060\u3051\u3067\u3044\u3044 \u8fd9\u6837\u5c31\u597d~\u8fd9\u6837\u5c31\u8db3\u5920\u4e86 \u6ca2\u5c71(\u305f\u304f\u3055\u3093)\u306e\u611b(\u3042\u3044)\u3067\u6ea2(\u3042\u3075)\u308c\u305f\u306a\u3089 \u5982\u679c\u7231\u591a\u5230\u6ee1\u6ea2\u800c\u51fa \u660e(\u3042)\u3051\u306a\u3044\u591c(\u3088\u308b)\u306e\u5922(\u3086\u3081)\u3092\u898b(\u307f)\u305b\u3066\u307b\u3057\u3044 \u5e0c\u671b\u4f60\u80fd\u8ba9\u6211\u505a\u4e00\u4e2a\u6c38\u4e0d\u7834\u6653\u7684\u7f8e\u68a6 \u5929\u79e4(\u3066\u3093\u3073\u3093)\u306f\u304d\u3063\u3068\u307e\u305f\u50be(\u304b\u305f\u3080)\u304f\u3051\u3069 \u5c3d\u7ba1\u5929\u5e73\u8fd8\u4f1a\u518d\u5ea6\u503e\u659c \u305a\u3063\u3068\u305a\u3063\u3068\u541b(\u304d\u307f)\u3068\u4e00\u7dd2(\u3044\u3063\u3057\u3087)\u306b\u3044\u305f\u3044~ \u3044\u305f\u3044~ \u8fd8\u662f\u60f3\u8981\u6c38\u8fdc~\u6c38\u8fdc\u548c\u4f60\u5728\u4e00\u8d77","title":"\u30ed\u30de\u30f3\u30b9\u306e\u7d04\u675f/\u6d6a\u6f2b\u4e4b\u7ea6"},{"location":"Songs/%E6%B5%AA%E6%BC%AB%E4%B9%8B%E7%BA%A6/#_1","text":"\u4f5c\u8bcd\uff1a\u5e7e\u7530\u308a\u3089 \u4f5c\u66f2\uff1a\u5e7e\u7530\u308a\u3089 \u6f14\u5531\uff1a\u5e7e\u7530\u308a\u3089 \u3053\u308c\u304b\u3089\u4e8c\u4eba(\u3075\u305f\u308a)\u904e(\u3059)\u3054\u3057\u3066\u3044\u304f\u305f\u3081\u306b \u4e3a\u4e86\u4eca\u540e\u4e24\u4e2a\u4eba\u7684\u751f\u6d3b \u7d04\u675f(\u3084\u304f\u305d\u304f)\u3057\u3066\u307b\u3057\u3044\u3053\u3068\u304c\u3042\u308b\u306e \u60f3\u8981\u548c\u4f60\u6709\u4e2a\u7ea6\u5b9a \u58f0(\u3053\u3048)\u304c\u67af(\u304b)\u308c\u3066\u540d\u524d(\u306a\u307e\u3048)\u304c\u547c(\u3088)\u3079\u306a\u304f\u306a\u308b \u76f4\u5230\u58f0\u97f3\u6c99\u54d1\u5730\u65e0\u6cd5\u547c\u5524\u5f7c\u6b64\u59d3\u540d \u305d\u306e\u65e5(\u3072)\u307e\u3067\u5fd8(\u308f\u3059)\u308c\u306a\u3044\u3067 \u90a3\u5929\u4e4b\u524d\u90fd\u4e0d\u8981\u5fd8\u8bb0 \u5149(\u3072\u304b\u308a)\u3092\u63a2(\u3055\u304c)\u3059\u3088\u3046\u306a\u7720(\u306d\u3080)\u308c\u306a\u3044\u591c(\u3088\u308b)\u306f \u4eff\u4f5b\u5728\u5bfb\u627e\u5149\u660e\u822c\u65e0\u6cd5\u5165\u7720\u7684\u591c\u665a \u671d(\u3042\u3055)\u307e\u3067\u624b(\u3066)\u3092\u63e1(\u306b\u304e)\u3063\u3066\u3044\u3066\u307b\u3057\u3044 \u5e0c\u671b\u4f60\u80fd\u4e00\u76f4\u63e1\u7740\u6211\u7684\u624b\u76f4\u5230\u6e05\u6668 \u6ca2\u5c71(\u305f\u304f\u3055\u3093)\u306e\u611b(\u3042\u3044)\u3067\u6ea2(\u3042\u3075)\u308c\u305f\u306a\u3089 \u5982\u679c\u7231\u591a\u5230\u6ee1\u6ea2\u800c\u51fa \u660e(\u3042)\u3051\u306a\u3044\u591c(\u3088\u308b)\u306e\u5922(\u3086\u3081)\u3092\u898b(\u307f)\u305b\u3066\u307b\u3057\u3044 \u5e0c\u671b\u4f60\u80fd\u8ba9\u6211\u505a\u4e00\u4e2a\u6c38\u4e0d\u7834\u6653\u7684\u7f8e\u68a6 \u5929\u79e4(\u3066\u3093\u3073\u3093)\u306f\u3044\u3064\u3082\u50be(\u304b\u305f\u3080)\u304f\u3051\u3069 \u5929\u5e73\u5c3d\u7ba1\u603b\u662f\u503e\u5411\u4e00\u7aef \u4eca\u591c(\u3053\u3093\u3084)\u3060\u3051\u306f\u540c(\u304a\u306a)\u3058\u3067\u3044\u305f\u3044 \u54ea\u6015\u53ea\u5728\u4eca\u665a\u4e5f\u60f3\u8ba9\u5b83\u8b8a\u5f97\u4e00\u6837 \u4e8c\u4eba(\u3075\u305f\u308a)\u3067\u9032(\u3059\u3059)\u307f\u59cb(\u306f\u3058)\u3081\u305f\u3053\u306e\u5217\u8eca(\u308c\u3063\u3057\u3083)\u306e \u4e24\u4e2a\u4eba\u4e00\u8d77\u5f00\u59cb\u4e58\u5750\u7684\u8fd9\u8d9f\u5217\u8f66 \u5207\u7b26(\u304d\u3063\u3077)\u306f\u6700\u5f8c(\u3055\u3044\u3054)\u307e\u3067\u5931(\u306a)\u304f\u3055\u306a\u3044\u3067\u306d \u76f4\u5230\u6700\u540e\u90fd\u8bf7\u4e0d\u8981\u5c06\u8f66\u7968\u5f04\u4e22\u54e6 \u3082\u3057\u3082\u884c(\u3086)\u304d\u5148(\u3055\u304d)\u3092\u898b(\u307f)\u5931(\u3046\u3057\u306a)\u3063\u305f\u306a\u3089 \u5982\u679c\u8ff7\u5931\u4e86\u76ee\u7684\u5730\u7684\u8bdd \u305d\u306e\u5834\u6240(\u3070\u3057\u3087)\u3067\u307e\u305f\u59cb(\u306f\u3058)\u3081\u3088\u3046 \u5c31\u5728\u90a3\u4e2a\u5730\u65b9\u91cd\u65b0\u5f00\u59cb\u5427 \u982c(\u307b\u307b)\u3092\u6fe1(\u306c)\u3089\u3059\u3088\u3046\u306a\u7720(\u306d\u3080)\u308c\u306a\u3044\u591c(\u3088\u308b)\u306f \u5728\u6cea\u6e7f\u8138\u988a\u7684\u4e0d\u7720\u4e4b\u591c \u5fc3\u5730(\u3053\u3053\u3061)\u3044\u3044\u5de6\u80a9(\u3072\u3060\u308a\u304b\u305f)\u3092\u8cb8(\u304b)\u3057\u3066\u307b\u3057\u3044 \u8bf7\u501f\u7ed9\u6211\u4f60\u90a3\u8212\u9002\u7684\u5de6\u80a9 \u6ca2\u5c71(\u305f\u304f\u3055\u3093)\u306e\u611b(\u3042\u3044)\u3092\u77e5(\u3057)\u308c\u305f\u306e\u306a\u3089 \u5982\u679c\u77e5\u9053\u4e86\u6211\u6eff\u5fc3\u7684\u7231\u610f \u53e3\u7d05(\u304f\u3061\u3079\u306b)\u3092\u6eb6(\u3068)\u304b\u3059\u3088\u3046\u306a\u30ad\u30b9(\u304d\u3059)\u3092\u3057\u3066 \u5c31\u7ed9\u6211\u6765\u4e2a\u80fd\u5c06\u53e3\u7ea2\u90fd\u878d\u5316\u7684\u543b\u5427 \u305d\u306e\u3042\u3068\u306f\u9f3b\u5148(\u306f\u306a\u3055\u304d)\u3067\u304f\u3059\u3063\u3068\u7b11(\u308f\u3089)\u3063\u3066 \u7136\u540e\u8138\u4e0a\u5e26\u7740\u7b11\u610f \u7d42(\u304a)\u308f\u308a\u306f\u306a\u3044\u3068\u8a00(\u3044)\u3063\u3066\u62b1(\u3060)\u304d\u3057\u3081\u3066 \u8bf4\u8fd8\u6ca1\u7ed3\u675f\u518d\u7d27\u62b1\u4f4f\u6211 \u541b(\u304d\u307f)\u306e\u77ed\u6240(\u305f\u3093\u3057\u3087)\u3084\u79c1(\u308f\u305f\u3057)\u306e\u9577\u6240(\u3061\u3087\u3046\u3057\u3087)\u304c\u5909(\u304b)\u308f\u3063\u3066\u3057\u307e\u3063\u3066\u3082 \u5373\u4f7f\u4f60\u7684\u7f3a\u70b9\u548c\u6211\u7684\u4f18\u70b9\u90fd\u6539\u53d8\u4e86 \u4ee3(\u304b)\u308f\u308a\u306f\u5c45(\u3044)\u306a\u3044\u3088 \u304d\u3063\u3068 \u4e5f\u6ca1\u6709\u80fd\u591f\u4ee3\u66ff\u7684\u5b58\u5728 \u601d(\u304a\u3082)\u3044\u51fa(\u3067)\u304c\u793a(\u3057\u3081)\u3059\u3088 \u307e\u305f\u624b(\u3066)\u3092\u53d6(\u3068)\u308d\u3046 \u50cf\u8bb0\u5fc6\u4e2d\u4e00\u6837\u518d\u6b21\u7275\u7740\u624b\u5427 \u661f\u5c51(\u307b\u3057\u304f\u305a)\u306e\u3088\u3046\u306a\u3053\u306e\u4e16\u754c(\u305b\u304b\u3044)\u3067 \u5728\u8fd9\u5982\u661f\u5c18\u822c\u7684\u4e16\u754c\u91cc \u7167(\u3066)\u3089\u3055\u308c\u305f\u5149(\u3072\u304b\u308a)\u306e\u5148(\u3055\u304d)\u306b\u3044\u305f\u3093\u3060 \u5728\u88ab\u7167\u8000\u7684\u5149\u8292\u4e4b\u524d \u541b(\u304d\u307f)\u306e\u307e\u307e\u305d\u306e\u307e\u307e\u304c\u7f8e(\u3046\u3064\u304f)\u3057\u3044\u304b\u3089 \u4f60\u662f\u5982\u6b64\u7684\u7f8e\u4e3d \u305d\u308c\u3067\u3044\u3044 \u305d\u308c\u3060\u3051\u3067\u3044\u3044 \u8fd9\u6837\u5c31\u597d~\u8fd9\u6837\u5c31\u8db3\u5920\u4e86 \u6ca2\u5c71(\u305f\u304f\u3055\u3093)\u306e\u611b(\u3042\u3044)\u3067\u6ea2(\u3042\u3075)\u308c\u305f\u306a\u3089 \u5982\u679c\u7231\u591a\u5230\u6ee1\u6ea2\u800c\u51fa \u660e(\u3042)\u3051\u306a\u3044\u591c(\u3088\u308b)\u306e\u5922(\u3086\u3081)\u3092\u898b(\u307f)\u305b\u3066\u307b\u3057\u3044 \u5e0c\u671b\u4f60\u80fd\u8ba9\u6211\u505a\u4e00\u4e2a\u6c38\u4e0d\u7834\u6653\u7684\u7f8e\u68a6 \u5929\u79e4(\u3066\u3093\u3073\u3093)\u306f\u304d\u3063\u3068\u307e\u305f\u50be(\u304b\u305f\u3080)\u304f\u3051\u3069 \u5c3d\u7ba1\u5929\u5e73\u8fd8\u4f1a\u518d\u5ea6\u503e\u659c \u305a\u3063\u3068\u305a\u3063\u3068\u541b(\u304d\u307f)\u3068\u4e00\u7dd2(\u3044\u3063\u3057\u3087)\u306b\u3044\u305f\u3044~ \u3044\u305f\u3044~ \u8fd8\u662f\u60f3\u8981\u6c38\u8fdc~\u6c38\u8fdc\u548c\u4f60\u5728\u4e00\u8d77","title":"\u30ed\u30de\u30f3\u30b9\u306e\u7d04\u675f/\u6d6a\u6f2b\u4e4b\u7ea6"}]}