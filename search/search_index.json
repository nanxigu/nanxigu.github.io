{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gu Nanxi","title":"\u9996\u9875"},{"location":"#gu-nanxi","text":"","title":"Gu Nanxi"},{"location":"Models/","text":"\u6a21\u578b PINN \u7cfb\u5217","title":"\u6a21\u578b"},{"location":"Models/#_1","text":"PINN \u7cfb\u5217","title":"\u6a21\u578b"},{"location":"Scholars/","text":"\u5b66\u8005\u8bba\u6587\u5408\u96c6 \u9102\u7ef4\u5357 \u8bb8\u8dc3\u751f 2022.07 Sparse DNN for Nonlinear PDEs","title":"\u5b66\u8005"},{"location":"Scholars/#_1","text":"","title":"\u5b66\u8005\u8bba\u6587\u5408\u96c6"},{"location":"Scholars/#_2","text":"","title":"\u9102\u7ef4\u5357"},{"location":"Scholars/#_3","text":"2022.07 Sparse DNN for Nonlinear PDEs","title":"\u8bb8\u8dc3\u751f"},{"location":"Songs/","text":"\u6b4c\u8bcd\u8bb0\u5f55 \u5e7e\u7530\u308a\u3089 - \u30ed\u30de\u30f3\u30b9\u306e\u7d04\u675f [ChiliChill - \u534a\u9192]","title":"\u6b4c\u8bcd\u8bb0\u5f55"},{"location":"Songs/#_1","text":"\u5e7e\u7530\u308a\u3089 - \u30ed\u30de\u30f3\u30b9\u306e\u7d04\u675f [ChiliChill - \u534a\u9192]","title":"\u6b4c\u8bcd\u8bb0\u5f55"},{"location":"Concepts/JL%20Lemma/","text":"Johnson-Lindenstrauss Lemma","title":"Johnson-Lindenstrauss Lemma"},{"location":"Concepts/JL%20Lemma/#johnson-lindenstrauss-lemma","text":"","title":"Johnson-Lindenstrauss Lemma"},{"location":"Concepts/Space/","text":"\u7a7a\u95f4 \u5411\u91cf\u7a7a\u95f4 \u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4","title":"\u7a7a\u95f4"},{"location":"Concepts/Space/#_1","text":"","title":"\u7a7a\u95f4"},{"location":"Concepts/Space/#_2","text":"","title":"\u5411\u91cf\u7a7a\u95f4"},{"location":"Concepts/Space/#_3","text":"","title":"\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4"},{"location":"Models/PINNs/PINNs/","text":"PINNs \u7cfb\u5217 2018.06 PINNs: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear PDEs ( JCP ) 2022.07 Adaptive Self-Supervision Algorithms for PINNs A Comprehensive Study of Non-Adaptive and Residual-Based Adaptive Sampling for PINNs 2022.10 Failure-Informed Adaptive Sampling for PINNs A Novel Adaptive Causal Sampling Method for PINNs","title":"PINNs \u7cfb\u5217"},{"location":"Models/PINNs/PINNs/#pinns","text":"","title":"PINNs \u7cfb\u5217"},{"location":"Models/PINNs/PINNs/#201806","text":"PINNs: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear PDEs ( JCP )","title":"2018.06"},{"location":"Models/PINNs/PINNs/#202207","text":"Adaptive Self-Supervision Algorithms for PINNs A Comprehensive Study of Non-Adaptive and Residual-Based Adaptive Sampling for PINNs","title":"2022.07"},{"location":"Models/PINNs/PINNs/#202210","text":"Failure-Informed Adaptive Sampling for PINNs A Novel Adaptive Causal Sampling Method for PINNs","title":"2022.10"},{"location":"Models/PINNs/PN-2207.04084/","text":"Adaptive Self-Supervision Algorithms for Physics-Informed Neural Networks \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u9002\u5e94\u81ea\u76d1\u7763\u7b97\u6cd5 \u4f5c\u8005: Shashank Subramanian | Robert M. Kirby | Michael W. Mahoney | Amir Gholami \u673a\u6784: \u65f6\u95f4: 2022-07-08 \u9884\u5370: arXiv:2207.04084v1 \u9886\u57df: \u6807\u7b7e: #PINN #\u5f00\u6e90 \u5f15\u7528: 29 \u7bc7 (\u81ea\u5f15 1 \u7bc7) \u4ee3\u7801: Github cur{ color:red; } model{ text-decoration: underline; text-decoration-color: orange; } term{ text-decoration: underline; text-decoration-color: purple; } Abstract \u6458\u8981 Physics-informed neural networks (PINNs) incorporate physical knowledge from the problem domain as a soft constraint on the loss function, but recent work has shown that this can lead to optimization difficulties. Here, we study the impact of the location of the collocation points on the trainability of these models. We find that the vanilla PINN performance can be significantly boosted by adapting the location of the collocation points as training proceeds. Specifically, we propose a novel adaptive collocation scheme which progressively allocates more collocation points (without increasing their number) to areas where the model is making higher errors (based on the gradient of the loss function in the domain). This, coupled with a judicious restarting of the training during any optimization stalls (by simply resampling the collocation points in order to adjust the loss landscape) leads to better estimates for the prediction error. We present results for several problems, including a 2D Poisson and diffusion-advection system with different forcing functions. We find that training vanilla PINNs for these problems can result in up to 70% prediction error in the solution, especially in the regime of low collocation points. In contrast, our adaptive schemes can achieve up to an order of magnitude smaller error, with similar computational complexity as the baseline. Furthermore, we find that the adaptive methods consistently perform on-par or slightly better than vanilla PINN method, even for large collocation point regimes. The code for all the experiments has been open sourced and available at Github. 1. Introduction \u4ecb\u7ecd A key aspect that distinguishes scientific ML (SciML) [4,10,14,16,23,28] from other ML tasks is that scientists typically know a great deal about the underlying physical processes that generate their data. For example, while the Ordinary Differential Equations (ODEs) or Partial Differential Equations (PDEs) used to simulate the physical phenomena may not capture every detail of a physical system,they often provide a reasonably good approximation. In some cases, we know that physical systems have to obey conservation laws (mass, energy, momentum, etc.). In other cases, we can learn these constraints, either exactly or approximately, from the data. In either case, the main challenge in SciML lies in combining such scientific prior domain-driven knowledge with large-scale data-driven methods from ML in a principled manner. One popular method to incorporate scientific prior knowledge is to incorporate them as soft-constraints throughout training, as proposed with Physics Informed Neural Networks (PINNs) [10,12,17]. These models use penalty method techniques from optimization [3] and formulate the solution of the PDE as an unconstrained optimization problem that minimizes a self-supervision loss function that incorporates the domain physics (PDEs) as a penalty (regularization) term. Formulating the problem as a soft-constraint makes it very easy to use existing auto-differentiation frameworks for SciML tasks. However, training PINNs this way can be very difficult, and it is often difficult to solve the optimization problem [6,11,25]. This could be partly because the self-supervision term typically contains complex terms such as (higher-order) derivatives of spatial functions and other nonlinearities that cause the loss term to become ill-conditioned [11]. This is very different than unit\u2018pball or other such convex functions, more commonly used as regularization terms in ML. Several solutions such as loss scaling [25], curriculum or sequence-to-sequence learning [11], tuning of loss function weights [13], and novel network architectures [19] have been proposed to address this problem. One overlooked, but very important, parameter of the training process is the way that the self-supervision is performed in PINNs, and in particular which data points in the domain are used for enforcing the physical constraints (commonly referred to as collocation points in numerical analysis). In the original work of [17],the collocation points are randomly sampled in the beginning of the training and kept constant throughout the learning process. However, we find that this is sub-optimal, and instead we propose adaptive collocation schemes. We show that these schemes can result in an order of magnitude better performance, with similar computational overhead. Background. We focus on scientific systems that have a PDE constraint of the following form:(1.1)F(u(x)) = 0,x \u2208 \u2126 \u2282 Rd,whereFis a differential operator representing the PDE,u(x) is the state variable (i.e., physical quantity of interest), \u2126 is the physical domain, andxrepresents spatial domain (2D in all of our results). To ensure existence and uniqueness of an analytical solution, there are additional constraints specified on the boundary,d\u2126, as well (such as periodic or Dirichlet boundary conditions). One possible approach to learn a representation for the solution is to incorporate the PDE as a hard constraint, and formulate a loss function that measures the prediction error on the boundary (where data points are available):(1.2)min\u03b8L(u)s.t.F(u) = 0,whereL(u) is typically a data mismatch term (this includes initial/boundary conditions but can also include observational data points), and whereFis a constraint on the residual of the PDE system (i.e.,F(u) is the residual) under consideration. Since constrained optimization is typically more difficult than unconstrained optimization [3], this constraint is typically relaxed and added as a penalty term to the loss function. This yields the following unconstrained optimization problem, namely the PINNs (soft constrained) optimization problem:(1.3)min\u03b8L(u) + \u03bbFLF. In this problem formulation, the regularization parameter,\u03bbF, controls the weight given to the PDE constraints, as compared to the data misfit term;\u03b8denotes the parameters of the model that predictsu(x),which is often taken to be a neural network; and the PDE loss functional term,LF, can be considered as a self-supervised loss, as all the information comes from the PDE system we are interested in simulating,instead of using direct observations. Typically a Euclidean loss function is used to measure the residual of the PDE for this loss function,kF(u)k22, where the\u20182norm is computed at discrete points (collocation points) that are randomly sampled from \u2126. This loss term is often the source of the training difficulty with PINNs [6, 11], which is the focus of our paper. Main contributions. Unlike other work in the literature, which has focused on changing the training method or the neural network (NN) architecture, here we focus on the self-supervision component of PINNs,and specifically on the selection of the collocation points. In particular, we make the following contributions:\u2022We study the role of the collocation points for two PDE systems: steady state diffusion (Poisson);and diffusion-advection. We find that keeping the collocation points constant throughout training is a sub-optimal strategy and is an important source of the training difficulty with PINNs. This is particularly true for cases where the PDE problem exhibits local behaviour (e.g., in presence of sharp, or very localized, features).\u2022We propose an alternative strategy of resampling the collocation points when training stalls. Although this strategy is simple, it can lead to significantly better reconstruction (see Tab. 1 and Fig. 2). Im-portantly, this approach does not increase the computational complexity, and it is easy to implement.\u2022We propose to improve the basic resampling scheme with a gradient-based adaptive scheme. This adaptive scheme is designed to help to relocate the collocation points to areas with higher loss gradient,without increasing the total number of points (see Algorithm 1 for the algorithm). In particular,we progressively relocate the points to areas of high gradient as training proceeds. This is done through a cosine-annealing that gradually changes the sampling of collocation points from uniform to adaptive through training. We find that this scheme consistently achieves better performance than the basic resampling method, and it can lead to more than 10x improvements in the prediction error(see Tab. 1, Fig. 2).\u2022We extensively test our adaptive schemes for the two PDE systems of Poisson and diffusion-advection while varying the number of collocation points for problems with both smooth or sharp features. While the resampling and adaptive schemes perform similarly in the large collocation point regime,the adaptive approach shows significant improvement when the number of collocation points is small and the forcing function is sharp (see Tab. 2). 2. Related Work \u76f8\u5173\u5de5\u4f5c There has been a large body of work studying PINNs [5,7,9,18,20,21,30] and the challenges associated with their training [6,11,24,25,26,27]. The work of [25] notes these challenges and proposes a loss scaling method to resolve the training difficulty. Similar to this approach, some works have treated the problem as a multi-objective optimization and tune the weights of the different loss terms[2,29]. A more formal approach was suggested in [13] where the weights are learned by solving a minimax optimization problem that ascends in the loss weight space and descends in the model parameter space. This approach was extended in [15] to shift the focus of the weights from the loss terms to the training data points instead, and the minimax forces the optimization to pay attention to specific regions of the domain. However, minimax optimization problems are known to be hard to optimize and introduce additional complexity and computational expense. Furthermore, it has been shown that using curriculum or sequence-to-sequence learning can ameliorate the training difficulty with PINNs [11]. More recently, the work of [24] shows that incorporating causality in time can help training for time-dependent PDEs. There is also recent work that studies the role of the collocation points. For instance, [22] refines the collocation point set without learnable weights. They propose an auxiliary NN that acts as a generative model to sample new collocation points that mimic the PDE residual. However, the auxiliary network also has to be trained in tandem with the PINN. The work of [14] proposes an adaptive collocation scheme where the points are densely sampled uniformly and trained for some number of iterations. Then the set is extended by adding points in increasing rank order of PDE residuals to refine in certain locations (of sharp fronts, for example) and the model is retrained. However, this method can increase the computational overhead, as the number of collocation points is progressively increased. Furthermore, in 1 the authors show that the latter approach can lead to excessive clustering of points throughout training. To address this, instead they propose to add points based on an underlying density function defined by the PDE residual. Both these schemes keep the original collocation set (the low residual points) and increase the training dataset sizes as the optimization proceeds. Unlike the work of [8,14], we focus on using gradient of the loss function, instead of the nominal loss value, as the proxy to guide the adaptive resampling of the collocation points. We show that this approach leads to better localization of the collocation points, especially for problems with sharp features. Furthermore, we incorporate a novel cosine-annealing scheme, which progressively incorporates adaptive sampling as training proceeds. Importantly, we also keep the number of collocation points the same. Not only does this not increase the computational overhead, but this is also easier to implement as well\uff0e 3. Methods \u65b9\u6cd5 In PINNs , we use a feedforward NN, denoted \\(NN(\\pmb{x};\\theta)\\) , that is parameterized by weights and biases, \\(\\theta\\) , takes as input values for coordinate points, \\(\\pmb{x}\\) , and outputs the solution value \\(u(\\pmb{x})\\in\\mathbb{R}\\) at these points. As described in Section 1 , the model parameters \\(\\theta\\) are optimized through the loss function: \u5728 PINNs \u4e2d, \u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u6709\u6743\u91cd\u548c\u504f\u5dee \\(\\theta\\) \u53c2\u6570\u5316\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc \\(NN(\\pmb{x};\\theta)\\) , \u8f93\u5165\u503c\u4e3a\u914d\u7f6e\u70b9 \\(\\pmb{x}\\) \u5e76\u8f93\u51fa\u89e3\u5728\u8fd9\u4e9b\u70b9\u4e0a\u7684\u503c \\(u(\\pmb{x})\\in\\mathbb{R}\\) . \u6b63\u5982\u7b2c\u4e00\u8282\u6240\u63cf\u8ff0\u7684, \u6a21\u578b\u53c2\u6570 \\(\\theta\\) \u901a\u8fc7\u4ee5\u4e0b\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4f18\u5316: \\[ \\min_{\\theta}\\mathcal{L}_{\\mathcal{B}} + \\lambda_{\\mathcal{F}}\\mathcal{L}_{\\mathcal{F}}. \\tag{3.1} \\] We focus on boundary-value (steady state) problems and define the two loss terms as: \u6211\u4eec\u4e3b\u8981\u5173\u6ce8\u8fb9\u503c (\u7a33\u6001) \u95ee\u9898\u5e76\u5b9a\u4e49\u76f8\u5e94\u7684\u4e24\u4e2a\u635f\u5931\u9879\u5982\u4e0b: \\[ \\mathcal{L}_{\\mathcal{B}} =\\frac{1}{n_b} \\sum_{i=1}^{n_b}\\|u(\\pmb{x}_b^i)-\\hat{u}(\\pmb{x}_b^i)\\|_2^2, \\tag{3.2a} \\] \\[ \\mathcal{L}_{\\mathcal{F}} =\\frac{1}{n_c} \\sum_{i=1}^{n_c}\\|\\mathcal{F}(u(\\pmb{x}_c^i))\\|_2^2, \\tag{3.2b} \\] where \\(u\\) is the model predicted solution, \\(\\hat{u}\\) is the true solution or data, \\(\\pmb{x}_b^i\\) are points on the boundary, and \\(\\pmb{x}_c^i\\) are collocation points uniformly sampled from the domain \\(\\Omega\\) . Here, \\(n_b\\) and \\(n_c\\) are the number of boundary and collocation points, respectively; and the boundary loss term \\(\\mathcal{L}_{\\mathcal{B}}\\) implements a Dirichlet boundary condition, where we assume that the solution values are known on the boundary \\(d\\Omega\\) . \u5176\u4e2d \\(u\\) \u662f\u6a21\u578b\u9884\u6d4b\u89e3, \\(\\hat{u}\\) \u662f\u771f\u89e3\u6216\u6570\u636e, \\(\\pmb{x}_b^i\\) \u662f\u8fb9\u754c\u4e0a\u7684\u70b9, \\(\\pmb{x}_c^i\\) \u662f\u4ece\u5b9a\u4e49\u57df \\(\\Omega\\) \u4e2d\u5747\u5300\u91c7\u6837\u7684\u914d\u7f6e\u70b9.\u5f0f\u5b50\u4e2d\u7684 \\(n_b\\) \u548c \\(n_c\\) \u5206\u522b\u8868\u793a\u8fb9\u754c\u70b9\u548c\u914d\u7f6e\u70b9\u7684\u6570\u91cf, \u5e76\u4e14\u8fb9\u754c\u635f\u5931\u9879 \\(\\mathcal{L}_{\\mathcal{B}}\\) \u4f7f\u7528\u4e86 Dirichlet \u8fb9\u754c\u6761\u4ef6, \u5373\u6211\u4eec\u5047\u8bbe\u89e3\u5728\u8fb9\u754c \\(d\\Omega\\) \u4e0a\u7684\u503c\u5df2\u77e5. In PINNs 2 , the collocation points used in Eq.3.2b are randomly sampled with a uniform probability over the entire space \\(\\Omega\\) in the beginning of training and then kept constant afterwards (we refer to this approach as Baseline). While a uniformly distributed collocation point set may be sufficient for simple PDEs with smooth features, we find them to be sub-optimal when the problem exhibits sharp/local features, or even fail to train. To address this, we propose the following schemes. \u5728 PINNs \u4e2d, \u4e0a\u8ff0\u516c\u5f0f\u4f7f\u7528\u7684\u914d\u7f6e\u70b9\u662f\u5728\u8bad\u7ec3\u5f00\u59cb\u662f\u5728\u6574\u4e2a\u7a7a\u95f4 \\(\\Omega\\) \u4e0a\u6839\u636e\u5747\u5300\u5206\u5e03\u8fdb\u884c\u968f\u673a\u91c7\u6837\u7684, \u7136\u540e\u5728\u4e4b\u540e\u4fdd\u6301\u4e0d\u53d8 (\u6211\u4eec\u5c06\u8fd9\u4e00\u65b9\u6cd5\u4f5c\u4e3a\u57fa\u7ebf). \u867d\u7136\u5bf9\u4e8e\u6709\u5149\u6ed1\u6027\u8d28\u7684\u7b80\u5355\u504f\u5fae\u5206\u65b9\u7a0b\u6765\u8bf4, \u4e00\u4e2a\u5747\u5300\u5206\u5e03\u7684\u914d\u7f6e\u70b9\u96c6\u53ef\u80fd\u5df2\u7ecf\u8db3\u591f\u4e86, \u6211\u4eec\u53d1\u73b0\u5f53\u95ee\u9898\u51fa\u73b0 sharp/\u5c40\u90e8\u6027\u65f6, \u53ef\u80fd\u4f1a\u51fa\u73b0\u6b21\u4f18\u89e3\u751a\u81f3\u8bad\u7ec3\u5931\u8d25. \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898, \u6211\u4eec\u63d0\u51fa\u5982\u4e0b\u65b9\u6848. Resampling Collocation Points \u91cd\u91c7\u6837\u914d\u7f6e\u70b9 Current PINNs are typically optimized with LBFGS . In our experiments, we found that in the baseline approach LBFGS often fails to find a descent direction and training stalls, even after hyperparamter tuning. This agrees with other results reported in the literature [6,11,25]. We find that this is partially due to the fact that the collocation points are kept constant and not changed. The simplest approach to address this is to resample the collocation points when LBFGS stalls. We refer to this approach as Resampling. As we will discuss in the next section, we find this approach to be helpful for cases with moderate to large number of collocation points. Adaptive Sampling \u81ea\u9002\u5e94\u91c7\u6837 While the Resampling method is effective for large number of collocation points, we found it to be sub-optimal in the small collocation point regime, especially for problems with sharp/localized features. In this case, the Resampling method still uses a uniform distribution with which to sample the new the collocation points; and, in the presence of a small number of collocation points and/or sharp/localized features, this is not an optimal allocation of the points. Ideally, we want to find a probability distribution, as a replacement for the uniform distribution, that can improve trainability of PINNs for a given number of collocation points and/or computational budget. There are several possibilities to define this distribution. The first intuitive approach would be to use the value of the PDE residual (Eq. 3.2b), and normalize it as a probability distribution. This could then be used to sample collocation points based on the loss values in the domain. That is, more points would be sampled in areas with with higher PDE residual, and vice versa. We refer to this approach as ADAPTIVE-R sampling (withRreferring to the PDE residual). An alternative is to use the gradient of the loss function (PDE loss termLF) w.r.t. the spatial grid, using that as the probability distribution with which to sample the collocation points. We refer to this approach as ADAPTIVE-G (withG referring to gradient of the loss). As we will show in the next section, when combined with a cosine annealing scheme (discussed next), both of these adaptive schemes consistently perform better than the Resampling scheme. Progressive Adaptive Sampling \u6e10\u8fdb\u81ea\u9002\u5e94\u91c7\u6837 Given the non-convex nature of the problem, it might be important to not overly constrain the NN to be too locally focused, as the optimization procedure could get stuck in a local minima. However, this can be addressed by progressively incorporating adaptive sampling as training proceeds. In particular, we use a cosine annealing strategy. This allows the NN to periodically alternate between focusing on regions of high error as well as uniformly covering larger parts of the sample space, over a period of iterations. Specifically, in each annealing period, we start with a full uniform sampling, and progressively incorporate adaptively sampled points using a cosine schedule rule of \\(\\eta=\\dfrac{1}{2}(1+\\cos\\pi\\dfrac{T_c}{T})\\) , where \\(\\eta\\) is the fraction of points uniformly sampled, \\(T_c\\) is the number of epochs since the last restart, and \\(T\\) is the length of the cosine schedule. We use this schedule to resample the collocation every \\(e\\) epochs. Given the periodic nature of the cosine annealing, that approach balances local adaptivity without losing global information. We outline the adaptive sampling Algorithm 1 . Algorithm 1. Adaptive Sampling for Self-Supervision in PINNs Require : Loss \\(\\mathcal{L}\\) , NN model, number of collocation points \\(n_c\\) , PDE regularization \\(\\lambda_{\\mathcal{F}}\\) , \\(T\\) , \\(s_w\\) , momentum \\(\\gamma\\) , max epochs \\(i_{max}\\) 1: \\(i\\leftarrow 0\\) 2: while \\(i\\leq i_{max}\\) do 3: \\(\\qquad\\) Compute proxy function as the loss gradient ( ADAPTIVE-G ) or PDE residual ( ADAPTIVE-R ) 4: \\(\\qquad\\) Current proxy \\(\\mathcal{P}_i\\leftarrow\\mathcal{P} + \\gamma\\mathcal{P}_{i-1}\\) 5: \\(\\qquad\\) \\(T_c\\leftarrow i \\mod T\\) 6: \\(\\qquad\\) \\(\\eta \\leftarrow\\) cosine-schedule \\((T_c, T)\\) 7: \\(\\qquad\\) if \\(i \\mod e\\) is true then 8: \\(\\qquad\\qquad\\) Sample \\(\\eta n_c\\) points \\(\\pmb{x}_u\\) uniformly 9: \\(\\qquad\\qquad\\) Sample \\((1 - \\eta)n_c\\) points \\(\\pmb{x}_a\\) using proxy function \\(\\mathcal{P}_i\\) 10: \\(\\qquad\\) end if 11: \\(\\qquad\\) \\(\\pmb{x}_c\\leftarrow \\pmb{x}_u\\cup \\pmb{x}_a\\) 12: \\(\\qquad\\) Input \\(\\pmb{x}\\leftarrow\\pmb{x}_b\\cup\\pmb{x}_c\\) where \\(\\pmb{x}_b\\) are boundary points 13: \\(\\qquad\\) \\(u \\leftarrow NN(\\pmb{x}; \\theta)\\) 14: \\(\\qquad\\) \\(\\mathcal{L} \\leftarrow \\mathcal{L}_\\mathcal{B}+ \\lambda_\\mathcal{F}\\mathcal{L}_\\mathcal{F}\\) 15: \\(\\qquad\\) \\(\\theta \\leftarrow\\) optimizer-update \\((\\theta, \\mathcal{L})\\) 16: \\(\\qquad\\) if stopping-criterion is true then 17: \\(\\qquad\\qquad\\) reset cosine scheduler \\(T_c\\leftarrow 0\\) 18: \\(\\qquad\\) end if 19: \\(\\qquad\\) \\(i \\leftarrow i + 1\\) 20: end while 4. Experiments \u5b9e\u9a8c In this section, we show examples that highlight the prediction error improvement using our adaptive self-supervision method on two PDE systems: Poisson\u2019s equation \u00a74.1 and (steady state)diffusion-advection \u00a74.2.Problem setup. We use the same problem set up as in [11,17,25] for the NN model, which is a feed forward model with hyperbolic tangent activation function, trained with LBFGS optimizer. We focus on2D spatial domains in \u2126 = [0,1]2. The training data set contains points that are randomly sampled from a uniform 2562mesh on \u2126. The testing data set is the set of all 2562points, along with the true solution of the PDE\u02c6u(x) at these points. Furthermore, we use a constant regularization parameter of\u03bbF= 1e-4 for all the runs, which we found to be a good balance between the boundary term and the PDE residual. To ensure a fair comparison, we tune the rest of the hyperparameters both for the baseline model as well as for the adaptive schemes. For tuning the parameters, we use the validation loss computed by calculating the total loss over randomly sampled points in the domain (30K points for all the experiments). Note that we do not use any signal from the analytical solution, and instead only use the loss in Eq. 3.1.Metrics. We train the optimizer for 5000 epochs with a stall criterion when the loss does not change for 10 epochs. We keep track of the minimum validation loss in order to get the final model parameters for testing. We compute our prediction error using two metrics: the\u20182relative error,\u00b51=ku \u2212 \u02c6uk2/k\u02c6ukand the\u20181error\u00b52=ku \u2212 \u02c6uk1, whereuis the model prediction solution from the best validation loss epoch and\u02c6uis the true solution. All runs are trained on an A100 NVIDIA GPU on the Perlmutter supercomputer. 4.1. 2D Poisson's Equation \u4e8c\u7ef4\u6cca\u677e\u65b9\u7a0b Problem Formulation \u95ee\u9898\u5f62\u5f0f\u5316 We consider a prototypical elliptic system defined by the Poisson\u2019s equation with source function f(x). This system represents a steady state diffusion equation: \\[ (4.1)\u2212 div K\u2207u = f(x),x \u2208 \u2126, \\] whereKdenotes the diffusion tensor. For homogeneous diffusion tensors, the solution of the poisson\u2019s equation with doubly-periodic boundary conditions can be computed using the fast Fourier transform on a discrete grid as:(4.2)\u02c6u = F\u22121\ufffd\u22121\u2212(k2xk11+ k2yk22+ 2kxkyk12)F(f(x))\ufffd,whereFis the Fourier transform,kx, kyare the frequencies in the Fourier domain, andk11, k22, k12are the diagonal and off-diagonal coefficients of the diffusion tensorK. We enforce the boundary conditions as Dirichlet boundary conditions using the true solution to circumvent the ill-posedness of the doubly-periodic Poisson\u2019s equation.1We use a Gaussian function as the source with standard deviation\u03c3fand consider the diffusion tensork11= 1, k22= 8, k12= 4 to make the problem anisotropic. We consider two test-cases:(i) TC1: smooth source function with\u03c3f= 0.1 (ii) TC2: sharp source function with\u03c3f= 0.01. We show these source functions and the corresponding target solutions in Fig. 1. For any experiment, we sweep over all the hyperparameters and select the ones that show the lowest/best validation loss for the final model. Observations. We start by examining the performance of the difference methods for the relatively small number of collocation points ofnc= 1,000 (which corresponds to 1.5% of the 256\u00d7256 domain \u2126). We report the errors\u00b51and\u00b52for the different schemes in Tab. 1. We also visualize the predicted solution for each method in Fig. 2, along with the location of the collocation points overlayed for the final model. We can clearly see that the baseline model does not converge (despite hyperparameter tuning) due to the optimizer stalls. However, Resampling achieves significantly better errors, especially for the smooth source function setup (TC1). However, for the sharp source experiment (TC2), the resampling does not help and shows an error of about 50%.Overall, the adaptive schemes show much better (or comparable) prediction errors for both test-cases. In particular, ADAPTIVE-R and ADAPTIVE-G achieve about 2\u20135% relative error (\u00b51) for both TC1 and TC2. The visual reconstruction shown in Fig. 2 also shows the clearly improved prediction with the adaptive schemes (last two columns), as compared to the baseline with/without resampling (first two columns). Note that the ADAPTIVE-G method assigns more collocation points around the sharp features. This is due to the fact that it uses the gradient information for its probability distribution, instead of the residual of the PDE which is used in ADAPTIVE-R method. To show the effect of the scheduler, we show an ablation study with the cosine-annealing scheduler in the appendix (see Fig. A.4).We then repeat the same experiment, but now varying the number of collocation points,nc, from 500to 8K, and we report the relative error (\u00b51) in Tab. 2. We observe a consistent trend, where the Baseline(PINN training without resampling) does not converge to a good solution, whereas ADAPTIVE-G consistently achieves up to an order of magnitude better error. Also, note that the performance of the Resampling method significantly improves and becomes on par with ADAPTIVE-G as we increasenc. This is somewhat expected since at large number of collocation points resampling will have the chance to sample points near the areas with sharp features. In Fig. A.1, we show the errors for every test-case as a function of number of collocation points using 10 different random seed values to quantify the variance in the different methods. We observe that the adaptive schemes additionally show smaller variances across seeds, especially for the test-cases with sharp sources. 4.2. 2D Diffusion-Advection Equation We next look at a steady-state 2D diffusion-advection equation with source function f(x), diffusion tensor K, and velocity vector v: For homogeneous diffusion tensors and velocity vectors, the solution of the advection-diffusion equation with doubly-periodic boundary conditions can also be computed using the fast Fourier transform on a discrete grid as: \u02c6u = F\u22121\ufffd\u22121 g1\u2212 g2 F(f(x))\ufffd, g1= \u2212(k2xk11+ k2yk22+ 2kxkyk12), g2= ikxv1+ ikyv2, (4.4) whereFis the Fourier transform,i=\u221a\u22121,kx, kyare the frequencies in the Fourier domain,k11, k22, k12are the diagonal and off-diagonal coefficients of the diffusion tensorK, andv1, v2are the velocity components. As before, we enforce the boundary conditions as Dirichlet boundary conditions using the true solution,and we consider a Gaussian function as the source with standard deviation\u03c3f. We use a diffusion tensor k11= 1, k22= 8, k12= 4 and velocity vectorv1= 40, v2= 10 to simulate sufficient advection. We consider two test-cases as before: (i) TC3: smooth source function with\u03c3f= 0.1 (ii) TC4: sharp source function with \u03c3f= 0.01. We show the source functions and the target solutions in Fig. 3.Observations. We observe a very similar behaviour for the reconstruction errors as in the previous experiments. As before, we start withnc= 1,000 collocation points, and we report the results in Tab. 3and Fig. 4. Here, the baseline achieves slightly better performance for TC3 (14%) but completely fails (100%error) for the TC4 test case, which includes a sharper forcing function. However, the Resampling achieves better results for both cases. Furthermore, the best performance is achieved by the adaptive methods. Finally, in Tab. 4 we report the relative errors for the baseline and adaptive schemes with various numbers of collocation pointsnc. Similar to the Poisson system, larger values ofncshow good performance, but the baselines underperform in the low data regime for sharp sources. The resampling achieves better errors, while the adaptive methods once again consistently achieve the best performance for both data regimes. 5. Conclusions \u7ed3\u8bba We studied the impact of the location of the collocation points in training SciML models, focusing on the popular class of PINN models, and we showed that the vanilla PINN strategy of keeping the collocation points fixed throughout training often results in suboptimal solutions. This is particularly the case for PDE systems with sharp (or very localized) features. We showed that a simple strategy of resampling collocation points during optimization stalls can significantly improve the reconstruction error, especially for moderately large number of collocation points. We also proposed adaptive collocation schemes to obtain a better allocation of the collocation points. This is done by constructing a probability distribution derived from either the PDE residual ( ADAPTIVE-R ) or its gradient w.r.t. the input ( ADAPTIVE-G ). We found that by progressively incorporating the adaptive schemes, we can achieve up to an order of magnitude better solutions, as compared to the baseline, especially for the regime of a small number of collocation points and with problems that exhibit sharp (or very localized) features. Some limitations of this current work include the following: we did not change the NN architecture (it was fixed as a feed forward NN) or tune hyperparameters relating to the architecture (which can be a significant factor in any analysis); and we only focused on 2D spatial systems (it is known that time-dependent or 3D or higher-dimensional systems can show different behaviours and may benefit from different kinds of adaptivity in space and time). We leave these directions to future work. However, we expect that techniques such as those we used here that aim to combine in a more principled way domain-driven scientific methods and data-driven ML methods will help in these cases as well. Reference \u53c2\u8003\u6587\u732e [2] Rafael Bischof and Michael Kraus. Multi-Objective Loss Balancing for Physics-Informed Deep Learning. arXiv preprint arXiv:2110.09813, 2021. [3] (Book) Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge university press, 2004. [4] Steven L Brunton, Bernd R Noack, and Petros Koumoutsakos. Machine learning for fluid mechanics. Annual Review of Fluid Mechanics, 52:477\u2013508, 2020. [5] Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Optics express, 28(8):11618\u201311633, 2020. [6] C. Edwards. Neural networks learn to speed up simulations. Communications of the ACM, 65(5):27\u201329,2022. [7] Nicholas Geneva and Nicholas Zabaras. Modeling the dynamics of pde systems with physics-constrained deep auto-regressive networks. Journal of Computational Physics, 403:109056, 2020. [9] Xiaowei Jin, Shengze Cai, Hui Li, and George Em Karniadakis. Nsfnets (navier-stokes flow nets): Physics-informed neural networks for the incompressible navier-stokes equations. Journal of Computational Physics, 426:109951, 2021. [10] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422\u2013440, 2021. [11] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks. NeurIPS 2021. [12] Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. IEEE transactions on neural networks, 9(5):987\u20131000, 1998. [13] Dehao Liu and Yan Wang. A dual-dimer method for training physics-constrained neural networks with minimax architecture. Neural Networks, 136:112\u2013125, 2021. [14] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library for solving differential equations. SIAM Review, 63(1):208\u2013228, 2021. [15] Levi McClenny and Ulisses Braga-Neto. Self-adaptive physics-informed neural networks using a soft attention mechanism. arXiv preprint arXiv:2009.04544, 2020. [16] Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar, Dominic Skinner, Ali Ramadhan, and Alan Edelman. Universal differential equations for scientific machine learning. arXiv preprint arXiv:2001.04385, 2020. [18] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science, 367(6481):1026\u20131030, 2020. [19] Amuthan A Ramabathiran and Prabhu Ramachandran. Spinn: Sparse, physics-based, and partially interpretable neural networks for pdes. Journal of Computational Physics, 445:110600, 2021. [20] Francisco Sahli Costabal, Yibo Yang, Paris Perdikaris, Daniel E Hurtado, and Ellen Kuhl. Physics-informed neural networks for cardiac activation mapping. Frontiers in Physics, 8:42, 2020. [21] Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial differential equations. Journal of computational physics, 375:1339\u20131364, 2018. [22] Kejun Tang, Xiaoliang Wan, and Chao Yang. Das: A deep adaptive sampling method for solving partial differential equations. arXiv preprint arXiv:2112.14038, 2021. [23] Laura von Rueden, Sebastian Mayer, Katharina Beckh, Bogdan Georgiev, Sven Giesselbach, Raoul Heese, Birgit Kirsch, Julius Pfrommer, Annika Pick, Rajkumar Ramamurthy, et al. Informed machine learning\u2013a taxonomy and survey of integrating knowledge into learning systems. arXiv preprint arXiv:1903.12394,2019. [24] Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality is all you need for training physics-informed neural networks. arXiv preprint arXiv:2203.07404, 2022. Adaptive Self-supervision Algorithms for Physics-informed Neural Networks1. [25] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient pathologies in physics-informed neural networks. arXiv preprint arXiv:2001.04536, 2020. [26] Sifan Wang, Hanwen Wang, and Paris Perdikaris. On the eigenvector bias of fourier feature networks:From regression to solving multi-scale pdes with physics-informed neural networks. arXiv preprint arXiv:2012.10047, 2020. [27] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel perspective. arXiv preprint arXiv:2007.14527, 2020. [28] Jared Willard, Xiaowei Jia, Shaoming Xu, Michael Steinbach, and Vipin Kumar. Integrating physics-based modeling with machine learning: A survey. arXiv preprint arXiv:2003.04919, 2020. [29] Zixue Xiang, Wei Peng, Xiaohu Zheng, Xiaoyu Zhao, and Wen Yao. Self-adaptive loss balanced physics-informed neural networks for the incompressible navier-stokes equations. arXiv preprint arXiv:2104.06217,2021. [30] Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Journal of Computational Physics, 394:56\u201381, 2019. John Hanna, Jose V Aguado, Sebastien Comas-Cardona, Ramzi Askri, and Domenico Borzacchiello. Residual-based adaptivity for two-phase flow simulation in porous media using physics-informed neural networks. arXiv preprint arXiv:2109.14290, 2021. \u21a9 Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations . JCP 2019. \u21a9","title":"Adaptive Self-Supervision Algorithms for Physics-Informed Neural Networks <br> \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u9002\u5e94\u81ea\u76d1\u7763\u7b97\u6cd5"},{"location":"Models/PINNs/PN-2207.04084/#adaptive-self-supervision-algorithms-for-physics-informed-neural-networks","text":"\u4f5c\u8005: Shashank Subramanian | Robert M. Kirby | Michael W. Mahoney | Amir Gholami \u673a\u6784: \u65f6\u95f4: 2022-07-08 \u9884\u5370: arXiv:2207.04084v1 \u9886\u57df: \u6807\u7b7e: #PINN #\u5f00\u6e90 \u5f15\u7528: 29 \u7bc7 (\u81ea\u5f15 1 \u7bc7) \u4ee3\u7801: Github cur{ color:red; } model{ text-decoration: underline; text-decoration-color: orange; } term{ text-decoration: underline; text-decoration-color: purple; }","title":"Adaptive Self-Supervision Algorithms for Physics-Informed Neural Networks  \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u9002\u5e94\u81ea\u76d1\u7763\u7b97\u6cd5"},{"location":"Models/PINNs/PN-2207.04084/#abstract","text":"Physics-informed neural networks (PINNs) incorporate physical knowledge from the problem domain as a soft constraint on the loss function, but recent work has shown that this can lead to optimization difficulties. Here, we study the impact of the location of the collocation points on the trainability of these models. We find that the vanilla PINN performance can be significantly boosted by adapting the location of the collocation points as training proceeds. Specifically, we propose a novel adaptive collocation scheme which progressively allocates more collocation points (without increasing their number) to areas where the model is making higher errors (based on the gradient of the loss function in the domain). This, coupled with a judicious restarting of the training during any optimization stalls (by simply resampling the collocation points in order to adjust the loss landscape) leads to better estimates for the prediction error. We present results for several problems, including a 2D Poisson and diffusion-advection system with different forcing functions. We find that training vanilla PINNs for these problems can result in up to 70% prediction error in the solution, especially in the regime of low collocation points. In contrast, our adaptive schemes can achieve up to an order of magnitude smaller error, with similar computational complexity as the baseline. Furthermore, we find that the adaptive methods consistently perform on-par or slightly better than vanilla PINN method, even for large collocation point regimes. The code for all the experiments has been open sourced and available at Github.","title":"Abstract \u6458\u8981"},{"location":"Models/PINNs/PN-2207.04084/#1-introduction","text":"A key aspect that distinguishes scientific ML (SciML) [4,10,14,16,23,28] from other ML tasks is that scientists typically know a great deal about the underlying physical processes that generate their data. For example, while the Ordinary Differential Equations (ODEs) or Partial Differential Equations (PDEs) used to simulate the physical phenomena may not capture every detail of a physical system,they often provide a reasonably good approximation. In some cases, we know that physical systems have to obey conservation laws (mass, energy, momentum, etc.). In other cases, we can learn these constraints, either exactly or approximately, from the data. In either case, the main challenge in SciML lies in combining such scientific prior domain-driven knowledge with large-scale data-driven methods from ML in a principled manner. One popular method to incorporate scientific prior knowledge is to incorporate them as soft-constraints throughout training, as proposed with Physics Informed Neural Networks (PINNs) [10,12,17]. These models use penalty method techniques from optimization [3] and formulate the solution of the PDE as an unconstrained optimization problem that minimizes a self-supervision loss function that incorporates the domain physics (PDEs) as a penalty (regularization) term. Formulating the problem as a soft-constraint makes it very easy to use existing auto-differentiation frameworks for SciML tasks. However, training PINNs this way can be very difficult, and it is often difficult to solve the optimization problem [6,11,25]. This could be partly because the self-supervision term typically contains complex terms such as (higher-order) derivatives of spatial functions and other nonlinearities that cause the loss term to become ill-conditioned [11]. This is very different than unit\u2018pball or other such convex functions, more commonly used as regularization terms in ML. Several solutions such as loss scaling [25], curriculum or sequence-to-sequence learning [11], tuning of loss function weights [13], and novel network architectures [19] have been proposed to address this problem. One overlooked, but very important, parameter of the training process is the way that the self-supervision is performed in PINNs, and in particular which data points in the domain are used for enforcing the physical constraints (commonly referred to as collocation points in numerical analysis). In the original work of [17],the collocation points are randomly sampled in the beginning of the training and kept constant throughout the learning process. However, we find that this is sub-optimal, and instead we propose adaptive collocation schemes. We show that these schemes can result in an order of magnitude better performance, with similar computational overhead. Background. We focus on scientific systems that have a PDE constraint of the following form:(1.1)F(u(x)) = 0,x \u2208 \u2126 \u2282 Rd,whereFis a differential operator representing the PDE,u(x) is the state variable (i.e., physical quantity of interest), \u2126 is the physical domain, andxrepresents spatial domain (2D in all of our results). To ensure existence and uniqueness of an analytical solution, there are additional constraints specified on the boundary,d\u2126, as well (such as periodic or Dirichlet boundary conditions). One possible approach to learn a representation for the solution is to incorporate the PDE as a hard constraint, and formulate a loss function that measures the prediction error on the boundary (where data points are available):(1.2)min\u03b8L(u)s.t.F(u) = 0,whereL(u) is typically a data mismatch term (this includes initial/boundary conditions but can also include observational data points), and whereFis a constraint on the residual of the PDE system (i.e.,F(u) is the residual) under consideration. Since constrained optimization is typically more difficult than unconstrained optimization [3], this constraint is typically relaxed and added as a penalty term to the loss function. This yields the following unconstrained optimization problem, namely the PINNs (soft constrained) optimization problem:(1.3)min\u03b8L(u) + \u03bbFLF. In this problem formulation, the regularization parameter,\u03bbF, controls the weight given to the PDE constraints, as compared to the data misfit term;\u03b8denotes the parameters of the model that predictsu(x),which is often taken to be a neural network; and the PDE loss functional term,LF, can be considered as a self-supervised loss, as all the information comes from the PDE system we are interested in simulating,instead of using direct observations. Typically a Euclidean loss function is used to measure the residual of the PDE for this loss function,kF(u)k22, where the\u20182norm is computed at discrete points (collocation points) that are randomly sampled from \u2126. This loss term is often the source of the training difficulty with PINNs [6, 11], which is the focus of our paper. Main contributions. Unlike other work in the literature, which has focused on changing the training method or the neural network (NN) architecture, here we focus on the self-supervision component of PINNs,and specifically on the selection of the collocation points. In particular, we make the following contributions:\u2022We study the role of the collocation points for two PDE systems: steady state diffusion (Poisson);and diffusion-advection. We find that keeping the collocation points constant throughout training is a sub-optimal strategy and is an important source of the training difficulty with PINNs. This is particularly true for cases where the PDE problem exhibits local behaviour (e.g., in presence of sharp, or very localized, features).\u2022We propose an alternative strategy of resampling the collocation points when training stalls. Although this strategy is simple, it can lead to significantly better reconstruction (see Tab. 1 and Fig. 2). Im-portantly, this approach does not increase the computational complexity, and it is easy to implement.\u2022We propose to improve the basic resampling scheme with a gradient-based adaptive scheme. This adaptive scheme is designed to help to relocate the collocation points to areas with higher loss gradient,without increasing the total number of points (see Algorithm 1 for the algorithm). In particular,we progressively relocate the points to areas of high gradient as training proceeds. This is done through a cosine-annealing that gradually changes the sampling of collocation points from uniform to adaptive through training. We find that this scheme consistently achieves better performance than the basic resampling method, and it can lead to more than 10x improvements in the prediction error(see Tab. 1, Fig. 2).\u2022We extensively test our adaptive schemes for the two PDE systems of Poisson and diffusion-advection while varying the number of collocation points for problems with both smooth or sharp features. While the resampling and adaptive schemes perform similarly in the large collocation point regime,the adaptive approach shows significant improvement when the number of collocation points is small and the forcing function is sharp (see Tab. 2).","title":"1. Introduction \u4ecb\u7ecd "},{"location":"Models/PINNs/PN-2207.04084/#2-related-work","text":"There has been a large body of work studying PINNs [5,7,9,18,20,21,30] and the challenges associated with their training [6,11,24,25,26,27]. The work of [25] notes these challenges and proposes a loss scaling method to resolve the training difficulty. Similar to this approach, some works have treated the problem as a multi-objective optimization and tune the weights of the different loss terms[2,29]. A more formal approach was suggested in [13] where the weights are learned by solving a minimax optimization problem that ascends in the loss weight space and descends in the model parameter space. This approach was extended in [15] to shift the focus of the weights from the loss terms to the training data points instead, and the minimax forces the optimization to pay attention to specific regions of the domain. However, minimax optimization problems are known to be hard to optimize and introduce additional complexity and computational expense. Furthermore, it has been shown that using curriculum or sequence-to-sequence learning can ameliorate the training difficulty with PINNs [11]. More recently, the work of [24] shows that incorporating causality in time can help training for time-dependent PDEs. There is also recent work that studies the role of the collocation points. For instance, [22] refines the collocation point set without learnable weights. They propose an auxiliary NN that acts as a generative model to sample new collocation points that mimic the PDE residual. However, the auxiliary network also has to be trained in tandem with the PINN. The work of [14] proposes an adaptive collocation scheme where the points are densely sampled uniformly and trained for some number of iterations. Then the set is extended by adding points in increasing rank order of PDE residuals to refine in certain locations (of sharp fronts, for example) and the model is retrained. However, this method can increase the computational overhead, as the number of collocation points is progressively increased. Furthermore, in 1 the authors show that the latter approach can lead to excessive clustering of points throughout training. To address this, instead they propose to add points based on an underlying density function defined by the PDE residual. Both these schemes keep the original collocation set (the low residual points) and increase the training dataset sizes as the optimization proceeds. Unlike the work of [8,14], we focus on using gradient of the loss function, instead of the nominal loss value, as the proxy to guide the adaptive resampling of the collocation points. We show that this approach leads to better localization of the collocation points, especially for problems with sharp features. Furthermore, we incorporate a novel cosine-annealing scheme, which progressively incorporates adaptive sampling as training proceeds. Importantly, we also keep the number of collocation points the same. Not only does this not increase the computational overhead, but this is also easier to implement as well\uff0e","title":"2. Related Work \u76f8\u5173\u5de5\u4f5c"},{"location":"Models/PINNs/PN-2207.04084/#3-methods","text":"In PINNs , we use a feedforward NN, denoted \\(NN(\\pmb{x};\\theta)\\) , that is parameterized by weights and biases, \\(\\theta\\) , takes as input values for coordinate points, \\(\\pmb{x}\\) , and outputs the solution value \\(u(\\pmb{x})\\in\\mathbb{R}\\) at these points. As described in Section 1 , the model parameters \\(\\theta\\) are optimized through the loss function: \u5728 PINNs \u4e2d, \u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u6709\u6743\u91cd\u548c\u504f\u5dee \\(\\theta\\) \u53c2\u6570\u5316\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc \\(NN(\\pmb{x};\\theta)\\) , \u8f93\u5165\u503c\u4e3a\u914d\u7f6e\u70b9 \\(\\pmb{x}\\) \u5e76\u8f93\u51fa\u89e3\u5728\u8fd9\u4e9b\u70b9\u4e0a\u7684\u503c \\(u(\\pmb{x})\\in\\mathbb{R}\\) . \u6b63\u5982\u7b2c\u4e00\u8282\u6240\u63cf\u8ff0\u7684, \u6a21\u578b\u53c2\u6570 \\(\\theta\\) \u901a\u8fc7\u4ee5\u4e0b\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4f18\u5316: \\[ \\min_{\\theta}\\mathcal{L}_{\\mathcal{B}} + \\lambda_{\\mathcal{F}}\\mathcal{L}_{\\mathcal{F}}. \\tag{3.1} \\] We focus on boundary-value (steady state) problems and define the two loss terms as: \u6211\u4eec\u4e3b\u8981\u5173\u6ce8\u8fb9\u503c (\u7a33\u6001) \u95ee\u9898\u5e76\u5b9a\u4e49\u76f8\u5e94\u7684\u4e24\u4e2a\u635f\u5931\u9879\u5982\u4e0b: \\[ \\mathcal{L}_{\\mathcal{B}} =\\frac{1}{n_b} \\sum_{i=1}^{n_b}\\|u(\\pmb{x}_b^i)-\\hat{u}(\\pmb{x}_b^i)\\|_2^2, \\tag{3.2a} \\] \\[ \\mathcal{L}_{\\mathcal{F}} =\\frac{1}{n_c} \\sum_{i=1}^{n_c}\\|\\mathcal{F}(u(\\pmb{x}_c^i))\\|_2^2, \\tag{3.2b} \\] where \\(u\\) is the model predicted solution, \\(\\hat{u}\\) is the true solution or data, \\(\\pmb{x}_b^i\\) are points on the boundary, and \\(\\pmb{x}_c^i\\) are collocation points uniformly sampled from the domain \\(\\Omega\\) . Here, \\(n_b\\) and \\(n_c\\) are the number of boundary and collocation points, respectively; and the boundary loss term \\(\\mathcal{L}_{\\mathcal{B}}\\) implements a Dirichlet boundary condition, where we assume that the solution values are known on the boundary \\(d\\Omega\\) . \u5176\u4e2d \\(u\\) \u662f\u6a21\u578b\u9884\u6d4b\u89e3, \\(\\hat{u}\\) \u662f\u771f\u89e3\u6216\u6570\u636e, \\(\\pmb{x}_b^i\\) \u662f\u8fb9\u754c\u4e0a\u7684\u70b9, \\(\\pmb{x}_c^i\\) \u662f\u4ece\u5b9a\u4e49\u57df \\(\\Omega\\) \u4e2d\u5747\u5300\u91c7\u6837\u7684\u914d\u7f6e\u70b9.\u5f0f\u5b50\u4e2d\u7684 \\(n_b\\) \u548c \\(n_c\\) \u5206\u522b\u8868\u793a\u8fb9\u754c\u70b9\u548c\u914d\u7f6e\u70b9\u7684\u6570\u91cf, \u5e76\u4e14\u8fb9\u754c\u635f\u5931\u9879 \\(\\mathcal{L}_{\\mathcal{B}}\\) \u4f7f\u7528\u4e86 Dirichlet \u8fb9\u754c\u6761\u4ef6, \u5373\u6211\u4eec\u5047\u8bbe\u89e3\u5728\u8fb9\u754c \\(d\\Omega\\) \u4e0a\u7684\u503c\u5df2\u77e5. In PINNs 2 , the collocation points used in Eq.3.2b are randomly sampled with a uniform probability over the entire space \\(\\Omega\\) in the beginning of training and then kept constant afterwards (we refer to this approach as Baseline). While a uniformly distributed collocation point set may be sufficient for simple PDEs with smooth features, we find them to be sub-optimal when the problem exhibits sharp/local features, or even fail to train. To address this, we propose the following schemes. \u5728 PINNs \u4e2d, \u4e0a\u8ff0\u516c\u5f0f\u4f7f\u7528\u7684\u914d\u7f6e\u70b9\u662f\u5728\u8bad\u7ec3\u5f00\u59cb\u662f\u5728\u6574\u4e2a\u7a7a\u95f4 \\(\\Omega\\) \u4e0a\u6839\u636e\u5747\u5300\u5206\u5e03\u8fdb\u884c\u968f\u673a\u91c7\u6837\u7684, \u7136\u540e\u5728\u4e4b\u540e\u4fdd\u6301\u4e0d\u53d8 (\u6211\u4eec\u5c06\u8fd9\u4e00\u65b9\u6cd5\u4f5c\u4e3a\u57fa\u7ebf). \u867d\u7136\u5bf9\u4e8e\u6709\u5149\u6ed1\u6027\u8d28\u7684\u7b80\u5355\u504f\u5fae\u5206\u65b9\u7a0b\u6765\u8bf4, \u4e00\u4e2a\u5747\u5300\u5206\u5e03\u7684\u914d\u7f6e\u70b9\u96c6\u53ef\u80fd\u5df2\u7ecf\u8db3\u591f\u4e86, \u6211\u4eec\u53d1\u73b0\u5f53\u95ee\u9898\u51fa\u73b0 sharp/\u5c40\u90e8\u6027\u65f6, \u53ef\u80fd\u4f1a\u51fa\u73b0\u6b21\u4f18\u89e3\u751a\u81f3\u8bad\u7ec3\u5931\u8d25. \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898, \u6211\u4eec\u63d0\u51fa\u5982\u4e0b\u65b9\u6848.","title":"3. Methods \u65b9\u6cd5"},{"location":"Models/PINNs/PN-2207.04084/#resampling-collocation-points","text":"Current PINNs are typically optimized with LBFGS . In our experiments, we found that in the baseline approach LBFGS often fails to find a descent direction and training stalls, even after hyperparamter tuning. This agrees with other results reported in the literature [6,11,25]. We find that this is partially due to the fact that the collocation points are kept constant and not changed. The simplest approach to address this is to resample the collocation points when LBFGS stalls. We refer to this approach as Resampling. As we will discuss in the next section, we find this approach to be helpful for cases with moderate to large number of collocation points.","title":"Resampling Collocation Points \u91cd\u91c7\u6837\u914d\u7f6e\u70b9"},{"location":"Models/PINNs/PN-2207.04084/#adaptive-sampling","text":"While the Resampling method is effective for large number of collocation points, we found it to be sub-optimal in the small collocation point regime, especially for problems with sharp/localized features. In this case, the Resampling method still uses a uniform distribution with which to sample the new the collocation points; and, in the presence of a small number of collocation points and/or sharp/localized features, this is not an optimal allocation of the points. Ideally, we want to find a probability distribution, as a replacement for the uniform distribution, that can improve trainability of PINNs for a given number of collocation points and/or computational budget. There are several possibilities to define this distribution. The first intuitive approach would be to use the value of the PDE residual (Eq. 3.2b), and normalize it as a probability distribution. This could then be used to sample collocation points based on the loss values in the domain. That is, more points would be sampled in areas with with higher PDE residual, and vice versa. We refer to this approach as ADAPTIVE-R sampling (withRreferring to the PDE residual). An alternative is to use the gradient of the loss function (PDE loss termLF) w.r.t. the spatial grid, using that as the probability distribution with which to sample the collocation points. We refer to this approach as ADAPTIVE-G (withG referring to gradient of the loss). As we will show in the next section, when combined with a cosine annealing scheme (discussed next), both of these adaptive schemes consistently perform better than the Resampling scheme.","title":"Adaptive Sampling \u81ea\u9002\u5e94\u91c7\u6837"},{"location":"Models/PINNs/PN-2207.04084/#progressive-adaptive-sampling","text":"Given the non-convex nature of the problem, it might be important to not overly constrain the NN to be too locally focused, as the optimization procedure could get stuck in a local minima. However, this can be addressed by progressively incorporating adaptive sampling as training proceeds. In particular, we use a cosine annealing strategy. This allows the NN to periodically alternate between focusing on regions of high error as well as uniformly covering larger parts of the sample space, over a period of iterations. Specifically, in each annealing period, we start with a full uniform sampling, and progressively incorporate adaptively sampled points using a cosine schedule rule of \\(\\eta=\\dfrac{1}{2}(1+\\cos\\pi\\dfrac{T_c}{T})\\) , where \\(\\eta\\) is the fraction of points uniformly sampled, \\(T_c\\) is the number of epochs since the last restart, and \\(T\\) is the length of the cosine schedule. We use this schedule to resample the collocation every \\(e\\) epochs. Given the periodic nature of the cosine annealing, that approach balances local adaptivity without losing global information. We outline the adaptive sampling Algorithm 1 . Algorithm 1. Adaptive Sampling for Self-Supervision in PINNs Require : Loss \\(\\mathcal{L}\\) , NN model, number of collocation points \\(n_c\\) , PDE regularization \\(\\lambda_{\\mathcal{F}}\\) , \\(T\\) , \\(s_w\\) , momentum \\(\\gamma\\) , max epochs \\(i_{max}\\) 1: \\(i\\leftarrow 0\\) 2: while \\(i\\leq i_{max}\\) do 3: \\(\\qquad\\) Compute proxy function as the loss gradient ( ADAPTIVE-G ) or PDE residual ( ADAPTIVE-R ) 4: \\(\\qquad\\) Current proxy \\(\\mathcal{P}_i\\leftarrow\\mathcal{P} + \\gamma\\mathcal{P}_{i-1}\\) 5: \\(\\qquad\\) \\(T_c\\leftarrow i \\mod T\\) 6: \\(\\qquad\\) \\(\\eta \\leftarrow\\) cosine-schedule \\((T_c, T)\\) 7: \\(\\qquad\\) if \\(i \\mod e\\) is true then 8: \\(\\qquad\\qquad\\) Sample \\(\\eta n_c\\) points \\(\\pmb{x}_u\\) uniformly 9: \\(\\qquad\\qquad\\) Sample \\((1 - \\eta)n_c\\) points \\(\\pmb{x}_a\\) using proxy function \\(\\mathcal{P}_i\\) 10: \\(\\qquad\\) end if 11: \\(\\qquad\\) \\(\\pmb{x}_c\\leftarrow \\pmb{x}_u\\cup \\pmb{x}_a\\) 12: \\(\\qquad\\) Input \\(\\pmb{x}\\leftarrow\\pmb{x}_b\\cup\\pmb{x}_c\\) where \\(\\pmb{x}_b\\) are boundary points 13: \\(\\qquad\\) \\(u \\leftarrow NN(\\pmb{x}; \\theta)\\) 14: \\(\\qquad\\) \\(\\mathcal{L} \\leftarrow \\mathcal{L}_\\mathcal{B}+ \\lambda_\\mathcal{F}\\mathcal{L}_\\mathcal{F}\\) 15: \\(\\qquad\\) \\(\\theta \\leftarrow\\) optimizer-update \\((\\theta, \\mathcal{L})\\) 16: \\(\\qquad\\) if stopping-criterion is true then 17: \\(\\qquad\\qquad\\) reset cosine scheduler \\(T_c\\leftarrow 0\\) 18: \\(\\qquad\\) end if 19: \\(\\qquad\\) \\(i \\leftarrow i + 1\\) 20: end while","title":"Progressive Adaptive Sampling \u6e10\u8fdb\u81ea\u9002\u5e94\u91c7\u6837"},{"location":"Models/PINNs/PN-2207.04084/#4-experiments","text":"In this section, we show examples that highlight the prediction error improvement using our adaptive self-supervision method on two PDE systems: Poisson\u2019s equation \u00a74.1 and (steady state)diffusion-advection \u00a74.2.Problem setup. We use the same problem set up as in [11,17,25] for the NN model, which is a feed forward model with hyperbolic tangent activation function, trained with LBFGS optimizer. We focus on2D spatial domains in \u2126 = [0,1]2. The training data set contains points that are randomly sampled from a uniform 2562mesh on \u2126. The testing data set is the set of all 2562points, along with the true solution of the PDE\u02c6u(x) at these points. Furthermore, we use a constant regularization parameter of\u03bbF= 1e-4 for all the runs, which we found to be a good balance between the boundary term and the PDE residual. To ensure a fair comparison, we tune the rest of the hyperparameters both for the baseline model as well as for the adaptive schemes. For tuning the parameters, we use the validation loss computed by calculating the total loss over randomly sampled points in the domain (30K points for all the experiments). Note that we do not use any signal from the analytical solution, and instead only use the loss in Eq. 3.1.Metrics. We train the optimizer for 5000 epochs with a stall criterion when the loss does not change for 10 epochs. We keep track of the minimum validation loss in order to get the final model parameters for testing. We compute our prediction error using two metrics: the\u20182relative error,\u00b51=ku \u2212 \u02c6uk2/k\u02c6ukand the\u20181error\u00b52=ku \u2212 \u02c6uk1, whereuis the model prediction solution from the best validation loss epoch and\u02c6uis the true solution. All runs are trained on an A100 NVIDIA GPU on the Perlmutter supercomputer.","title":"4. Experiments \u5b9e\u9a8c"},{"location":"Models/PINNs/PN-2207.04084/#41-2d-poissons-equation","text":"","title":"4.1. 2D Poisson's Equation \u4e8c\u7ef4\u6cca\u677e\u65b9\u7a0b"},{"location":"Models/PINNs/PN-2207.04084/#problem-formulation","text":"We consider a prototypical elliptic system defined by the Poisson\u2019s equation with source function f(x). This system represents a steady state diffusion equation: \\[ (4.1)\u2212 div K\u2207u = f(x),x \u2208 \u2126, \\] whereKdenotes the diffusion tensor. For homogeneous diffusion tensors, the solution of the poisson\u2019s equation with doubly-periodic boundary conditions can be computed using the fast Fourier transform on a discrete grid as:(4.2)\u02c6u = F\u22121\ufffd\u22121\u2212(k2xk11+ k2yk22+ 2kxkyk12)F(f(x))\ufffd,whereFis the Fourier transform,kx, kyare the frequencies in the Fourier domain, andk11, k22, k12are the diagonal and off-diagonal coefficients of the diffusion tensorK. We enforce the boundary conditions as Dirichlet boundary conditions using the true solution to circumvent the ill-posedness of the doubly-periodic Poisson\u2019s equation.1We use a Gaussian function as the source with standard deviation\u03c3fand consider the diffusion tensork11= 1, k22= 8, k12= 4 to make the problem anisotropic. We consider two test-cases:(i) TC1: smooth source function with\u03c3f= 0.1 (ii) TC2: sharp source function with\u03c3f= 0.01. We show these source functions and the corresponding target solutions in Fig. 1. For any experiment, we sweep over all the hyperparameters and select the ones that show the lowest/best validation loss for the final model. Observations. We start by examining the performance of the difference methods for the relatively small number of collocation points ofnc= 1,000 (which corresponds to 1.5% of the 256\u00d7256 domain \u2126). We report the errors\u00b51and\u00b52for the different schemes in Tab. 1. We also visualize the predicted solution for each method in Fig. 2, along with the location of the collocation points overlayed for the final model. We can clearly see that the baseline model does not converge (despite hyperparameter tuning) due to the optimizer stalls. However, Resampling achieves significantly better errors, especially for the smooth source function setup (TC1). However, for the sharp source experiment (TC2), the resampling does not help and shows an error of about 50%.Overall, the adaptive schemes show much better (or comparable) prediction errors for both test-cases. In particular, ADAPTIVE-R and ADAPTIVE-G achieve about 2\u20135% relative error (\u00b51) for both TC1 and TC2. The visual reconstruction shown in Fig. 2 also shows the clearly improved prediction with the adaptive schemes (last two columns), as compared to the baseline with/without resampling (first two columns). Note that the ADAPTIVE-G method assigns more collocation points around the sharp features. This is due to the fact that it uses the gradient information for its probability distribution, instead of the residual of the PDE which is used in ADAPTIVE-R method. To show the effect of the scheduler, we show an ablation study with the cosine-annealing scheduler in the appendix (see Fig. A.4).We then repeat the same experiment, but now varying the number of collocation points,nc, from 500to 8K, and we report the relative error (\u00b51) in Tab. 2. We observe a consistent trend, where the Baseline(PINN training without resampling) does not converge to a good solution, whereas ADAPTIVE-G consistently achieves up to an order of magnitude better error. Also, note that the performance of the Resampling method significantly improves and becomes on par with ADAPTIVE-G as we increasenc. This is somewhat expected since at large number of collocation points resampling will have the chance to sample points near the areas with sharp features. In Fig. A.1, we show the errors for every test-case as a function of number of collocation points using 10 different random seed values to quantify the variance in the different methods. We observe that the adaptive schemes additionally show smaller variances across seeds, especially for the test-cases with sharp sources.","title":"Problem Formulation \u95ee\u9898\u5f62\u5f0f\u5316"},{"location":"Models/PINNs/PN-2207.04084/#42-2d-diffusion-advection-equation","text":"We next look at a steady-state 2D diffusion-advection equation with source function f(x), diffusion tensor K, and velocity vector v: For homogeneous diffusion tensors and velocity vectors, the solution of the advection-diffusion equation with doubly-periodic boundary conditions can also be computed using the fast Fourier transform on a discrete grid as: \u02c6u = F\u22121\ufffd\u22121 g1\u2212 g2 F(f(x))\ufffd, g1= \u2212(k2xk11+ k2yk22+ 2kxkyk12), g2= ikxv1+ ikyv2, (4.4) whereFis the Fourier transform,i=\u221a\u22121,kx, kyare the frequencies in the Fourier domain,k11, k22, k12are the diagonal and off-diagonal coefficients of the diffusion tensorK, andv1, v2are the velocity components. As before, we enforce the boundary conditions as Dirichlet boundary conditions using the true solution,and we consider a Gaussian function as the source with standard deviation\u03c3f. We use a diffusion tensor k11= 1, k22= 8, k12= 4 and velocity vectorv1= 40, v2= 10 to simulate sufficient advection. We consider two test-cases as before: (i) TC3: smooth source function with\u03c3f= 0.1 (ii) TC4: sharp source function with \u03c3f= 0.01. We show the source functions and the target solutions in Fig. 3.Observations. We observe a very similar behaviour for the reconstruction errors as in the previous experiments. As before, we start withnc= 1,000 collocation points, and we report the results in Tab. 3and Fig. 4. Here, the baseline achieves slightly better performance for TC3 (14%) but completely fails (100%error) for the TC4 test case, which includes a sharper forcing function. However, the Resampling achieves better results for both cases. Furthermore, the best performance is achieved by the adaptive methods. Finally, in Tab. 4 we report the relative errors for the baseline and adaptive schemes with various numbers of collocation pointsnc. Similar to the Poisson system, larger values ofncshow good performance, but the baselines underperform in the low data regime for sharp sources. The resampling achieves better errors, while the adaptive methods once again consistently achieve the best performance for both data regimes.","title":"4.2. 2D Diffusion-Advection Equation"},{"location":"Models/PINNs/PN-2207.04084/#5-conclusions","text":"We studied the impact of the location of the collocation points in training SciML models, focusing on the popular class of PINN models, and we showed that the vanilla PINN strategy of keeping the collocation points fixed throughout training often results in suboptimal solutions. This is particularly the case for PDE systems with sharp (or very localized) features. We showed that a simple strategy of resampling collocation points during optimization stalls can significantly improve the reconstruction error, especially for moderately large number of collocation points. We also proposed adaptive collocation schemes to obtain a better allocation of the collocation points. This is done by constructing a probability distribution derived from either the PDE residual ( ADAPTIVE-R ) or its gradient w.r.t. the input ( ADAPTIVE-G ). We found that by progressively incorporating the adaptive schemes, we can achieve up to an order of magnitude better solutions, as compared to the baseline, especially for the regime of a small number of collocation points and with problems that exhibit sharp (or very localized) features. Some limitations of this current work include the following: we did not change the NN architecture (it was fixed as a feed forward NN) or tune hyperparameters relating to the architecture (which can be a significant factor in any analysis); and we only focused on 2D spatial systems (it is known that time-dependent or 3D or higher-dimensional systems can show different behaviours and may benefit from different kinds of adaptivity in space and time). We leave these directions to future work. However, we expect that techniques such as those we used here that aim to combine in a more principled way domain-driven scientific methods and data-driven ML methods will help in these cases as well.","title":"5. Conclusions \u7ed3\u8bba"},{"location":"Models/PINNs/PN-2207.04084/#reference","text":"[2] Rafael Bischof and Michael Kraus. Multi-Objective Loss Balancing for Physics-Informed Deep Learning. arXiv preprint arXiv:2110.09813, 2021. [3] (Book) Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge university press, 2004. [4] Steven L Brunton, Bernd R Noack, and Petros Koumoutsakos. Machine learning for fluid mechanics. Annual Review of Fluid Mechanics, 52:477\u2013508, 2020. [5] Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Optics express, 28(8):11618\u201311633, 2020. [6] C. Edwards. Neural networks learn to speed up simulations. Communications of the ACM, 65(5):27\u201329,2022. [7] Nicholas Geneva and Nicholas Zabaras. Modeling the dynamics of pde systems with physics-constrained deep auto-regressive networks. Journal of Computational Physics, 403:109056, 2020. [9] Xiaowei Jin, Shengze Cai, Hui Li, and George Em Karniadakis. Nsfnets (navier-stokes flow nets): Physics-informed neural networks for the incompressible navier-stokes equations. Journal of Computational Physics, 426:109951, 2021. [10] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422\u2013440, 2021. [11] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks. NeurIPS 2021. [12] Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. IEEE transactions on neural networks, 9(5):987\u20131000, 1998. [13] Dehao Liu and Yan Wang. A dual-dimer method for training physics-constrained neural networks with minimax architecture. Neural Networks, 136:112\u2013125, 2021. [14] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library for solving differential equations. SIAM Review, 63(1):208\u2013228, 2021. [15] Levi McClenny and Ulisses Braga-Neto. Self-adaptive physics-informed neural networks using a soft attention mechanism. arXiv preprint arXiv:2009.04544, 2020. [16] Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar, Dominic Skinner, Ali Ramadhan, and Alan Edelman. Universal differential equations for scientific machine learning. arXiv preprint arXiv:2001.04385, 2020. [18] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science, 367(6481):1026\u20131030, 2020. [19] Amuthan A Ramabathiran and Prabhu Ramachandran. Spinn: Sparse, physics-based, and partially interpretable neural networks for pdes. Journal of Computational Physics, 445:110600, 2021. [20] Francisco Sahli Costabal, Yibo Yang, Paris Perdikaris, Daniel E Hurtado, and Ellen Kuhl. Physics-informed neural networks for cardiac activation mapping. Frontiers in Physics, 8:42, 2020. [21] Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial differential equations. Journal of computational physics, 375:1339\u20131364, 2018. [22] Kejun Tang, Xiaoliang Wan, and Chao Yang. Das: A deep adaptive sampling method for solving partial differential equations. arXiv preprint arXiv:2112.14038, 2021. [23] Laura von Rueden, Sebastian Mayer, Katharina Beckh, Bogdan Georgiev, Sven Giesselbach, Raoul Heese, Birgit Kirsch, Julius Pfrommer, Annika Pick, Rajkumar Ramamurthy, et al. Informed machine learning\u2013a taxonomy and survey of integrating knowledge into learning systems. arXiv preprint arXiv:1903.12394,2019. [24] Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality is all you need for training physics-informed neural networks. arXiv preprint arXiv:2203.07404, 2022. Adaptive Self-supervision Algorithms for Physics-informed Neural Networks1. [25] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient pathologies in physics-informed neural networks. arXiv preprint arXiv:2001.04536, 2020. [26] Sifan Wang, Hanwen Wang, and Paris Perdikaris. On the eigenvector bias of fourier feature networks:From regression to solving multi-scale pdes with physics-informed neural networks. arXiv preprint arXiv:2012.10047, 2020. [27] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel perspective. arXiv preprint arXiv:2007.14527, 2020. [28] Jared Willard, Xiaowei Jia, Shaoming Xu, Michael Steinbach, and Vipin Kumar. Integrating physics-based modeling with machine learning: A survey. arXiv preprint arXiv:2003.04919, 2020. [29] Zixue Xiang, Wei Peng, Xiaohu Zheng, Xiaoyu Zhao, and Wen Yao. Self-adaptive loss balanced physics-informed neural networks for the incompressible navier-stokes equations. arXiv preprint arXiv:2104.06217,2021. [30] Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Journal of Computational Physics, 394:56\u201381, 2019. John Hanna, Jose V Aguado, Sebastien Comas-Cardona, Ramzi Askri, and Domenico Borzacchiello. Residual-based adaptivity for two-phase flow simulation in porous media using physics-informed neural networks. arXiv preprint arXiv:2109.14290, 2021. \u21a9 Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations . JCP 2019. \u21a9","title":"Reference \u53c2\u8003\u6587\u732e"},{"location":"Models/PINNs/PN-2207.10289/","text":"A Comprehensive Study of Non-Adaptive and Residual-Based Adaptive Sampling for Physics-Informed Neural Networks \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u81ea\u9002\u5e94\u91c7\u6837\u548c\u57fa\u4e8e\u6b8b\u5dee\u7684\u81ea\u9002\u5e94\u91c7\u6837\u7684\u7efc\u5408\u7814\u7a76 \u4f5c\u8005: Chenxi Wu1,\u2020, Min Zhu1,\u2020, Qinyang Tan^2 , Yadhu Kartha^3 , and Lu Lu1,* \u673a\u6784: College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA \u65f6\u95f4: 2022-07-21 \u9884\u5370: arXiv:2207.10289v1 \u9886\u57df: physics.comp-ph \u6807\u7b7e: \u504f\u5fae\u5206\u65b9\u7a0b, \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc, \u6b8b\u5dee\u70b9\u5206\u5e03, \u975e\u81ea\u9002\u5e94\u5747\u5300\u91c7\u6837, \u5e26\u91cd\u91c7\u6837\u7684\u5747\u5300\u91c7\u6837, \u57fa\u4e8e\u6b8b\u5dee\u7684\u81ea\u9002\u5e94\u91c7\u6837 \u5f15\u7528: 47 \u7bc7 Abstract Physics-informed neural networks (PINNs) have shown to be an effective tool for solving both forward and inverse problems of partial differential equations (PDEs). PINNs embed the PDEs into the loss of the neural network using automatic differentiation, and this PDE loss is evaluated at a set of scattered spatio-temporal points (called residual points). The location and distribution of these residual points are highly important to the performance of PINNs. However, in the existing studies on PINNs, only a few simple residual point sampling methods have mainly been used. Here, we present a comprehensive study of two categories of sampling for PINNs: non-adaptive uniform sampling and adaptive nonuniform sampling. We consider six uniform sampling methods, including (1) equispaced uniform grid, (2) uniformly random sampling, (3) Latin hypercube sampling, (4) Halton sequence, (5) Hammersley sequence, and (6) Sobol sequence. We also consider a resampling strategy for uniform sampling. To improve the sampling efficiency and the accuracy of PINNs, we propose two new residual-based adaptive sampling methods: residual-based adaptive distribution (RAD) and residual-based adaptive refinement with distribution (RAR-D), which dynamically improve the distribution of residual points based on the PDE residuals during training. Hence, we have considered a total of 10 different sampling methods, including six non-adaptive uniform sampling, uniform sampling with resampling, two proposed adaptive sampling, and an existing adaptive sampling. We extensively tested the performance of these sampling methods for four forward problems and two inverse problems in many setups. Our numerical results presented in this study are summarized from more than 6000 simulations of PINNs. We show that the proposed adaptive sampling methods of RAD and RAR-D significantly improve the accuracy of PINNs with fewer residual points for both forward and inverse problems. The results obtained in this study can also be used as a practical guideline in choosing sampling methods. 1 Introduction Physics-informed neural networks (PINNs) [1] have emerged in recent years and quickly became a powerful tool for solving both forward and inverse problems of partial differential equations (PDEs) via deep neural networks (DNNs) [2, 3, 4]. PINNs embed the PDEs into the loss of the neural network using automatic differentiation. Compared with traditional numerical PDE solvers, such as the finite difference method (FDM) and the finite element method (FEM), PINNs are mesh free and therefore highly flexible. Moreover, PINNs can easily incorporate both physics-based constraints and data measurements into the loss function. PINNs have been applied to tackle diverse problems in computational science and engineering, such as inverse problems in nanooptics, metamaterials [5], and fluid dynamics [2], parameter estimation in systems biology [6, 7], and problems of inverse design and topology optimization [8]. In addition to standard PDEs, PINNs have also been extended to solve other types of PDEs, including integro-differential equations [3], fractional PDEs [9], and stochastic PDEs [10]. Despite the past success, addressing a wide range of PDE problems with increasing levels of complexity can be theoretically and practically challenging, and thus many aspects of PINNs still require further improvements to achieve more accurate prediction, higher computational efficiency, and training robustness [4]. A series of extensions to the vanilla PINN have been proposed to boost the performance of PINNs from various aspects. For example, better loss functions have been discovered via meta-learning [11], and gradient-enhanced PINNs (gPINNs) have been developed to embed the gradient information of the PDE residual into the loss [12]. In PINNs, the total loss is a weighted summation of multiple loss terms corresponding to the PDE and initial/boundary conditions, and different methods have been developed to automatically tune these weights and balance the losses [13, 14, 15]. Moreover, a different weight for each loss term could be set at every training point [16, 17, 8, 18]. For problems in a large domain, decomposition of the spatiotemporal domain accelerates the training of PINNs and improves their accuracy [19, 20, 21]. For time-dependent problems, it is usually helpful to first train PINNs within a short time domain and then gradually expand the time intervals of training until the entire time domain is covered [22, 23, 24, 25, 26]. In addition to these general methods, other problem-specific techniques have also been developed, e.g., enforcing Dirichlet or periodic boundary conditions exactly by constructing special neural network architectures [27, 28, 8]. PINNs are mainly optimized against the PDE loss, which guarantees that the trained network is consistent with the PDE to be solved. PDE loss is evaluated at a set of scattered residual points. Intuitively, the effect of residual points on PINNs is similar to the effect of mesh points on FEM, and thus the location and distribution of these residual points should be highly important to the performance of PINNs. However, in previous studies on PINNs, two simple residual point sampling methods (i.e., an equispaced uniform grid and uniformly random sampling) have mainly been used, and the importance of residual point sampling has largely been overlooked. 1.1 Related work and our contributions Different residual point sampling methods can be classified into two categories: uniform sampling and nonuniform sampling. Uniform sampling can be obtained in multiple ways. For example, we could use the nodes of an equispaced uniform grid as the residual points or randomly sample the points according to a continuous uniform distribution in the computational domain. Although these two sampling methods are simple and widely used, alternative sampling methods may be applied. The Latin hypercube sampling (LHS) [29, 30] was used in Ref. [1], and the Sobol sequence [31] was first used for PINNs in Ref. [9]. The Sobol sequence is one type of quasi random low-discrepancy sequences among other sequences, such as the Halton sequence [32], and the Hammersley sequence [33]. Low-discrepancy sequences usually perform better than uniformly distributed random numbers in many applications such as numerical integration; hence, a comprehensive comparison of these methods for PINNs is required. However, very few comparisons [34, 35] have been performed. In this study, we extensively compared the performance of different uniform sampling methods, including (1) equispaced uniform grid, (2) uniformly random sampling, (3) LHS, (4) Sobol sequence, (5) Halton sequence, and (6) Hammersley sequence. In supervised learning, the dataset is fixed during training, but in PINNs, we can select residual points at any location. Hence, instead of using the same residual points during training, in each optimization iteration, we could select a new set of residual points, as first emphasized in Ref. [3]. While this strategy has been used in some works, it has not yet been systematically tested. Thus, in this study, we tested the performance of such a resampling strategy and investigated the effect of the number of residual points and the resampling period for the first time. Uniform sampling works well for some simple PDEs, but it may not be efficient for those that are more complicated. To improve the accuracy, we could manually select the residual points in a nonuniform way, as was done in Ref. [36] for high-speed flows, but this approach is highly problem-dependent and usually tedious and time-consuming. In this study, we focus on automatic and adaptive nonuniform sampling. Motivated by the adaptive mesh refinement in FEM, Lu et al. [3] proposed the first adaptive nonuniform sampling for PINNs in 2019, the residual-based adaptive refinement (RAR) method, which adds new residual points in the locations with large PDE residuals. In 2021, another sampling strategy [37] was developed, where all the residual points were resampled according to a probability density function (PDF) proportional to the PDE residual. In this study, motivated by these two ideas, we proposed two new sampling strategies: residual-based adaptive distribution (RAD), where the PDF for sampling is a nonlinear func- tion of the PDE residual; residual-based adaptive refinement with distribution (RAR-D), which is a hybrid method of RAR and RAD, i.e., the new residual points are added according to a PDF. During the preparation of this paper, a few new studies appeared [38, 39, 40, 41, 42, 43, 44] that also proposed modified versions of RAR or PDF-based resampling. Most of these methods are special cases of the proposed RAD and RAR-D, and our methods can achieve better performance. We include a detailed comparison of these strategies in Section 2.4, after introducing several notations and our new proposed methods. In this study, we have considered a total of 10 different sampling methods, including seven non-adaptive sampling methods (six different uniform samplings and one uniform sampling with resampling) and three adaptive sampling approaches (RAR, RAD, and RAR-D). We compared the performance of these sampling methods for four forward problems of PDEs and investigated the effect of the number of residual points. We also compared their performance for two inverse problems that have not yet been consid- ered in the literature. We performed more than 6000 simulations of PINNs to obtain all the results shown in this study. 1.2 Organization This paper is organized as follows. In Section 2, after providing a brief overview of PINNs and different non-adaptive sampling strategies, two new adaptive nonuniform sampling strategies (RAD and RAR-D) are proposed. In Section 3, we compare the performance of 10 different methods for six different PDE problems, including four forward problems and two inverse problems. Section 4 summarizes the findings and concludes the paper. 2 Methods This section briefly reviews physics-informed neural networks (PINNs) in solving forward and inverse partial differential equations (PDEs). Then different types of uniformly sampling are introduced. Next, two nonuniform residual-based adaptive sampling methods are proposed to enhance the accuracy and training efficiency of PINNs. Finally, a comparison of related methods is presented. 2.1 PINNs in solving forward and inverse PDEs We consider the PDE parameterized by\u03bbdefined on a domain \u2126\u2282Rd, f(x;u(x)) =f ( x; \u2202u \u2202x 1 \u2202u \u2202xd \u2202^2 u \u2202x 1 \u2202x 1 \u2202^2 u \u2202x 1 \u2202xd ;...;\u03bb ) = 0, x= (x 1 ,...,xd)\u2208\u2126, with boundary conditions on\u2202\u2126 B(u,x) = 0, andu(x) denotes the solution atx. In PINNs, the initial condition is treated as the Dirichlet boundary condition. A forward problem is aimed to obtain the solutionu across the entire domain, where the model parameters\u03bbare known. In practice, the model parameters\u03bbmight be unknown, but some observations from the solutionuare available, which lead to an inverse problem. An inverse problem is aimed to discover parameters\u03bbthat best describe the observed data from the solution. PINNs are capable of addressing both forward and inverse problems. To solve a forward problem, the solutionuis represented with a neural network \u02c6u(x;\u03b8). The network parameters\u03b8are trained to approximate the solutionu, such that the loss function is minimized [1, 3]: L(\u03b8;T) =wfLf(\u03b8;Tf) +wbLb(\u03b8;Tb), where Lf(\u03b8;Tf) = 1 |Tf| \u2211 x\u2208Tf \u2223\u2223 \u2223\u2223f(x; \u2202u\u02c6 \u2202x 1 \u2202\u02c6u \u2202xd \u2202^2 u\u02c6 \u2202x 1 \u2202x 1 \u2202^2 u\u02c6 \u2202x 1 \u2202xd ;...;\u03bb) \u2223\u2223 \u2223\u2223 2 , (1) Lb(\u03b8;Tb) = 1 |Tb| \u2211 x\u2208Tb |B(\u02c6u,x)|^2 , andwfandwbare the weights. Two sets of points are samples both inside the domain (Tf) and on the boundaries (Tb). Here,TfandTbare referred to as the sets of \u201cresidual points\u201d, andT=Tf\u222aTb. To solve the inverse problem, an additional loss term corresponding to the misfit of the observed data at the locationsTi, defined as Li(\u03b8,\u03bb;Ti) = 1 |Ti| \u2211 x\u2208Ti |u\u02c6(x)\u2212u(x)|^2 , is added to the loss function. The loss function is then defined as L(\u03b8,\u03bb;T) =wfLf(\u03b8,\u03bb;Tf) +wbLb(\u03b8,\u03bb;Tb) +wiLi(\u03b8,\u03bb;Ti), with an additional weightwi. Then the network parameters\u03b8are trained simultaneously with\u03bb. For certain PDE problems, it is possible to enforce boundary conditions directly by constructing a special network architecture [27, 28, 8, 12], which eliminates the loss term of boundary conditions. In this study, the boundary conditions are enforced exactly and automatically. Hence, for a forward problem, the loss function is L(\u03b8,\u03bb;T) =Lf(\u03b8,\u03bb;Tf). For an inverse problem, the loss function is L(\u03b8,\u03bb;T) =wfLf(\u03b8,\u03bb;Tf) +wiLi(\u03b8,\u03bb;Ti), where we choosewf=wi= 1 for the diffusion-reaction equation in Section 3.6, andwf= 1,wi= 1000 for the Korteweg-de Vries equation in Section 3.7. 2.2 Uniformly-distributed non-adaptive sampling The training of PINNs requires a set of residual points (Tf). The sampling strategy ofTf plays a vital role in promoting the accuracy and computational efficiency of PINNs. Here, we discuss several sampling approaches. 2.2.1 Fixed residual points In most studies of PINNs, we specify the residual points at the beginning of training and never change them during the training process. Two simple sampling methods (equispaced uniform grids and uniformly random sampling) have been commonly used. Other sampling methods, such as the Latin hypercube sampling (LHS) [29, 30] and the Sobol sequence [31], have also been used in some studies [1, 9, 34]. The Sobol sequence is one type of quasi-random low-discrepancy sequences. Low-discrepancy sequences are commonly used as a replacement for uniformly distributed random numbers and usually perform better in many applications such as numerical integration. This study also considers other low-discrepancy sequences, including the Halton sequence [32] and the Hammersley sequence [33]. We list the six uniform sampling methods as follows, and the examples of 400 points generated in [0,1]^2 using different methods are shown in Fig. 1. Equispaced uniform grid (Grid): The residual points are chosen as the nodes of an equispaced uniform grid of the computational domain. Uniformly random sampling (Random): The residual points are randomly sampled according to a continuous uniform distribution over the domain. In practice, this is usually done using pseudo-random number generators such as the PCG-64 algorithm [45]. Latin hypercube sampling LHS : The LHS is a stratified Monte Carlo sampling method that generates random samples that occur within intervals on the basis of equal probability and with normal distribution for each range. Quasi-random low-discrepancy sequences: (a)Halton sequence Halton : The Halton samples are generated according to the reversing or flipping the base conversion of numbers using primes. (b)Hammersley sequence Hammersley : The Hammersley sequence is the same as the Halton sequence, except in the first dimension where points are located equidistant from each other. (c) Sobol sequence Sobol : The Sobol sequence is a base-2 digital sequence that fills in a highly uniform manner. Figure 1: Examples of 400 points generated in[0,1]^2 using different uniform sampling methods in Section 2.2.1. 2.2.2 Uniform points with resampling In PINNs, a point at any location can be used to evaluate the PDE loss. Instead of using the fixed residual points during training, we could also select a new set of residual points in every certain optimization iteration [3]. The specific method to sample the points each time can be chosen from those methods discussed in Section 2.2.1. We can even use different sampling methods at different times, so many possible implementations make it impossible to be completely covered in this study. In this study, we only consider Random sampling with resampling (Random-R). The RandomR method is the same as the Random method, except that the residual points are resampled for everyNiteration. Theresampling periodNis also an important hyperparameter for accuracy, as we demonstrate in our empirical experiments in Section 3. 2.3 Nonuniform adaptive sampling Although the uniform sampling strategies were predominantly employed, recent studies on the nonuniform adaptive sampling strategies [3, 37] have demonstrated promising improvement in the distribution of residual points during the training processes and achieved better accuracy. 2.3.1 Residual-based adaptive refinement with greed (RAR-G) The first adaptive sampling method for PINNs is the residual-based adaptive refinement method (RAR) proposed in Ref. [3]. RAR aims to improve the distribution of residual points during the training process by sampling more points in the locations where the PDE residual is large. Specifically, after every certain iteration, RAR adds new points in the locations with large PDE residuals (Algorithm 1). RAR only focuses on the points with large residual, and thus it is a greedy algorithm. To better distinguish from the other sampling methods, the RAR method is referred to as RAR-G in this study. Algorithm 1: RAR-G [3]. 1 Sample the initial residual pointsT using one of the methods in Section 2.2.1; 2 Train the PINN for a certain number of iterations; 3 repeat 4 Sample a set of dense pointsS 0 using one of the methods in Section 2.2.1; 5 Compute the PDE residuals for the points inS 0 ; 6 S \u2190mpoints with the largest residuals inS 0 ; 7 T \u2190T \u222aS; 8 Train the PINN for a certain number of iterations; 9 untilthe total number of iterations or the total number of residual points reaches the limit; 2.3.2 Residual-based adaptive distribution (RAD) RAR-G significantly improves the performance of PINNs when solving certain PDEs of solutions with steep gradients [3, 12]. Nevertheless, RAR-G focuses mainly on the location where the PDE residual is largest and disregards the locations of smaller residuals. Another sampling strategy was developed later in Ref. [37], where all the residual points are resampled according to a probability density function (PDF)p(x) proportional to the PDE residual. Specifically, for any pointx, we first compute the PDE residual\u03b5(x) =|f(x; \u02c6u(x))|, and then compute a probability as p(x)\u221d\u03b5(x), i.e., p(x) = \u03b5(x) A whereA= \u222b \u2126\u03b5(x)dxis a normalizing constant. Then all the residual points are sampled according top(x). This approach works for certain PDEs, but as we show in our numerical examples, it does not work well in some cases. Following this idea, we propose an improved version called the residualbased adaptive distribution (RAD) method (Algorithm 2), where we use a new PDF defined as p(x)\u221d \u03b5k(x) E[\u03b5k(x)] +c, (2) wherek\u22650 andc\u22650 are two hyperparameters. E[\u03b5k(x)] can be approximated by a numerical integration such as Monte Carlo integration. We note that the Random-R method in Section 2.2.2 is a special case of RAD by choosingk= 0 orc\u2192\u221e. Algorithm 2: RAD. 1 Sample the initial residual pointsT using one of the methods in Section 2.2.1; 2 Train the PINN for a certain number of iterations; 3 repeat 4 T \u2190A new set of points randomly sampled according to the PDF of Eq. (2); 5 Train the PINN for a certain number of iterations; 6 untilthe total number of iterations reaches the limit; In RAD (Algorithm 2 line 4), we need to sample a set of points according top(x), which can be done in a few ways. Whenxis low-dimensional, we can sample the points approximately in the following brute-force way: Sample a set of dense pointsS 0 using one of the methods in Section 2.2.1; Computep(x) for the points inS 0 ; Define a probability mass function \u0303p(x) =p(Ax)with the normalizing constantA= \u2211 x\u2208S 0 p(x); Sample a subset of points fromS 0 according to \u0303p(x). This method is simple, easy to implement, and sufficient for many PDE problems. For more complicated cases, we can use other methods such as inverse transform sampling, Markov chain Monte Carlo (MCMC) methods, and generative adversarial networks (GANs) [46]. The two hyperparameterskandcin Eq. (2) control the profile ofp(x) and thus the distribution of sampled points. We illustrate the effect ofkandcusing a simple 2D example, \u03b5(x,y) = 2^4 axa(1\u2212x)aya(1\u2212y)a, (3) witha= 10 in Fig. 2. Whenk= 0, it becomes a uniform distribution. As the value ofkincreases, more residual points will large PDE residuals are sampled. As the value ofcincreases, the residual points exhibit an inclination to be uniformly distributed. Compared with RAR, RAD provides more freedom to balance the points in the locations with large and small residuals by tuningkand c. The optimal values ofkandcare problem-dependent, and based on our numerical results, the combination ofk= 1 andc= 1 is usually a good default choice. 2.3.3 Residual-based adaptive refinement with distribution (RAR-D) We also propose a hybrid method of RAR-G and RAD, namely, residual-based adaptive refinement with distribution (RAR-D) (Algorithm 3). Similar to RAR-G, RAR-D repeatedly adds new points to the training dataset; similar to RAD, the new points are sampled based on the PDF in Eq. (2). We note that whenk\u2192 \u221e, only points with the largest PDE residual are added, which recovers RAR-G. The optimal values ofkandcare problem dependent, and based on our numerical results, the combination ofk= 2 andc= 0 is usually a good default choice. Figure 2:Examples of 1000 residual points sampled by RAD with different values ofk andcfor the PDE residual\u03b5(x,y)in Eq.(3). Algorithm 3: RAR-D. 1 Sample the initial residual pointsT using one of the methods in Section 2.2.1; 2 Train the PINN for a certain number of iterations; 3 repeat 4 S \u2190mpoints randomly sampled according to the PDF of Eq. (2); 5 T \u2190T \u222aS; 6 Train the PINN for a certain number of iterations; 7 untilthe total number of iterations or the total number of residual points reaches the limit; 2.4 Comparison with related work As discussed in Section 2.3, our proposed RAD and RAR-D are improved versions of the methods in Refs. [3, 37]. Here, we summarize the similarities between their methods and ours. Lu et al. [3] (in July 2019) proposed RAR (renamed to RAR-G here), which is a special case of RAR-D by choosing a large value ofk. The method proposed by Nabian et al. [37] (in April 2021) is a special case of RAD by choosingk= 1 andc= 0. During the preparation of this paper, a few new papers appeared [38, 39, 40, 41, 42, 43, 44] that also proposed similar methods. Here, we summarize the similarities and differences between these studies. The method proposed by Gao et al. [40] (in December 2021) is a special case of RAD by choosingc= 0. Tang et al. [41] (in December 2021) proposed two methods. One is a special case of RAD by choosingk= 2 andc= 0, and the other is a special case of RAR-D by choosingk= 2 and c= 0. Zeng et al. [43] (in April 2022) proposed a subdomain version of RAR-G. The entire domain is divided into many subdomains, and then new points are added to the several subdomains with large average PDE residual. Similar to RAR-G, Peng et al. [42] (in May 2022) proposed to add more points with large PDE residual, but they used the node generation technology proposed in Ref. [47]. We note that this method only works for a two-dimensional space. Zapf et al. [38] (in May 2022) proposed a modified version of RAR-G, where some points with small PDE residual are removed while adding points with large PDE residual. They show that compared with RAR, this reduces the computational cost, but the accuracy keeps similar. Hanna et al. [44] (in May 2022) proposed a similar method as RAR-D, but they chosep(x)\u221d max{log(\u03b5(x)/\u03b5 0 ), 0 }, where\u03b5 0 is a small tolerance. Similar to the work of Zapf et al., Daw et al. [39] (in July 2022) also proposed to remove the points with small PDE residual, but instead of adding new points with large PDE residual, they added new uniformly random sampled points. Thus all these methods are special cases of our proposed RAD and RAR-D (or with minor modification). However, in our study, two tunable variableskandcare introduced. As we show in our results, the values ofkandccould be crucial since they significantly influence the residual points distribution. By choosing proper values ofkandc, our methods would outperform the other methods. We also note that the point-wise weighting [16, 17, 8, 18] can be viewed as a special case of adaptive sampling, described as follows. When the residual points are randomly sampled from a uniform distributionU(\u2126), and the number of residual points is large, the PDE loss in Eq. (1) can be approximated byEU[\u03b5^2 (x)]. If we consider a point-wise weighting functionw(x), then the loss becomesEU[w(x)\u03b5^2 (x)], while for RAD the loss isEp[\u03b5^2 (x)]. If we choosew(x) (divided by a normalizing constant) as the PDFp(x), then the two losses are equal. 3 Results We apply PINNs with all the ten sampling methods in Section 2 to solve six forward and inverse PDE problems. In all examples, the hyperbolic tangent (tanh) is selected as the activation function. Table 1 summarizes the network width, depth, and optimizers used for each example. More details of the hyperparameters and training procedure can be found in each section of the specific problem. Table 1:The hyperparameters used for each numerical experiment.The learning rate of Adam optimizer is chosen as 0.001. Problems Depth Width Optimizer Section 3.2 Diffusion equation 4 32 Adam Section 3.3 Burgers\u2019 equation 4 64 Adam + L-BFGS Section 3.4 Allen-Cahn equation 4 64 Adam + L-BFGS Section 3.5 Wave equation 6 100 Adam + L-BFGS Section 3.6 Diffusion-reaction equation (inverse) 4 20 Adam Section 3.7 Korteweg-de Vries equation (inverse) 4 100 Adam For both forward and inverse problems, to evaluate the accuracy of the solution \u02c6u, theL^2 relative error is used: \u2016u\u02c6\u2212u\u2016 2 \u2016u\u2016 2 For inverse problems, to evaluate the accuracy of the predicted coefficients\u03bb\u02c6, the relative error is also computed: |\u03bb\u02c6\u2212\u03bb| |\u03bb| As the result of PINN has randomness due to the random sampling, network initialization, and optimization, thus, for each case, we run the same experiment at least 10 times and then compute the geometric mean and standard deviation of the errors. The code in this study is implemented by using the library DeepXDE [3] and is publicly available from the GitHub repositoryhttps: //github.com/lu-group/pinn-sampling. 3.1 Summary Here, we first present a summary of the accuracy of all the methods for the forward and inverse problems listed in Tables 2 and Table 3, respectively. A relatively small number of residual points is chosen to show the difference among different methods. In the specific section of each problem (Sections 3.2\u20133.7), we discuss all the detailed analyses, including the convergence of error during the training process, the convergence of error with respect to the number of residual points, and the effects of different hyperparameters (e.g., the period of resampling in Random-R, the values of kandcin RAD and RAR-D, and the number of new points added each time in RAR-D). We note that Random-R is a special case of RAD by choosingk= 0 orc\u2192 \u221e, and RAR-G is a special case of RAR-D by choosingk\u2192\u221e. Our main findings from the results are as follows. The proposed RAD method has always performed the best among the 10 sampling methods when solving all forward and inverse problems. For PDEs with complicated solutions, such as the Burgers\u2019 and multi-scale wave equation, the proposed RAD and RAR-D methods are predominately effective and yield errors magnitudes lower. For PDEs with smooth solutions, such as the diffusion equation and diffusion-reaction equa- tion, some uniform sampling methods, such as the Hammersley and Random-R, also produce sufficiently low errors. Compared with other uniform sampling methods, Random-R usually demonstrates better performance. Among the six uniform sampling methods with fixed residual points, the low-discrepancy sequences (Halton, Hammersley, and Sobol) generally perform better than Random and LHS, and both are better than Grid. Table 2: L^2 relative error of the PINN solution for the forward problems. Bold font indicates the smallest three errors for each problem. Underlined text indicates the smallest error for each problem. Diffusion Burgers\u2019 Allen-Cahn Wave No. of residual points 30 2000 1000 2000 Grid 0.66\u00b10.06% 13.7\u00b12.37% 93.4\u00b16.98% 81.3\u00b113.7% Random 0.74\u00b10.17% 13.3\u00b18.35% 22.2\u00b116.9% 68.4\u00b120.1% LHS 0.48\u00b10.24% 13.5\u00b19.05% 26.6\u00b115.8% 75.9\u00b133.1% Halton 0.24\u00b10.17% 4.51\u00b13.93% 0.29\u00b10.14% 60.2\u00b110.0% Hammersley 0.17\u00b10.07% 3.02\u00b12.98% 0.14\u00b10.14% 58.9\u00b18.52% Sobol 0.19\u00b10.07% 3.38\u00b13.21% 0.35\u00b10.24% 57.5\u00b114.7% Random-R 0.12\u00b10.06% 1.69\u00b11.67% 0.55\u00b10.34% 0.72\u00b10.90% RAR-G [3] 0.20\u00b10.07% 0.12\u00b10.04% 0.53\u00b10.19% 0.81\u00b10.11% RAD 0.11\u00b10.07% 0.02\u00b10.00% 0.08\u00b10.06% 0.09\u00b10.04% RAR-D 0.14\u00b10.11% 0.03\u00b10.01% 0.09\u00b10.03% 0.29\u00b10.04% 3.2 Diffusion equation We first consider the following one-dimensional diffusion equation: \u2202u \u2202t = \u2202^2 u \u2202x^2 +e\u2212t ( \u2212sin(\u03c0x) +\u03c0^2 sin(\u03c0x) ) , x\u2208[\u2212 1 ,1],t\u2208[0,1], u(x,0) = sin(\u03c0x), u(\u2212 1 ,t) =u(1,t) = 0, whereuis the concentration of the diffusing material. The exact solution isu(x,t) = sin(\u03c0x)e\u2212t. We first compare the performance of the six uniform sampling methods with fixed residual points (Fig. 3A). The number of residual points is ranged from 10 to 80 with an increment of 10 points each time. For each number of residual points, the maximum iteration is set to be 15 000 with Adam as the optimizer. When the number of points is large (e.g., more than 70), all these methods Figure 3:L^2 relative errors of different sampling methods for the diffusion equation in Section 3.2.(A) Six uniform sampling with fixed residual points. (B) Random-R with different periods of resampling when using 30 residual points. (CandD) The training trajectory of RAD with different values ofkandcwhen using 30 residual points. (C)k= 1. (D)c= 1. (EandF) RAR-D with different values ofkandc. Each time one new point is added. (E)k= 2. (F)c= The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. Table 3:L^2 relative error of the PINN solution and relative error of the inferred parameters for the inverse problems.Bold font indicates the smallest three errors for each problem. Underlined text indicates the smallest error for each problem. Diffusion-reaction Korteweg-de Vries u(x) k(x) u(x,t) \u03bb 1 \u03bb 2 No. of residual points 15 600 Grid 0.36\u00b10.12% 8.58\u00b12.14% 24.4\u00b111.1% 53.7\u00b130.7% 42.0\u00b122.3% Random 0.35\u00b10.17% 5.77\u00b12.05% 8.86\u00b12.80% 16.4\u00b17.33% 16.8\u00b17.40% LHS 0.36\u00b10.14% 7.00\u00b12.62% 10.9\u00b12.60% 22.0\u00b16.68% 22.6\u00b16.36% Halton 0.23\u00b10.08% 6.16\u00b11.08% 8.76\u00b13.33% 16.7\u00b16.16% 17.2\u00b16.20% Hammersley 0.28\u00b10.08% 6.37\u00b10.91% 4.49\u00b13.56% 5.24\u00b17.08% 5.71\u00b17.32% Sobol 0.21\u00b10.06% 3.09\u00b10.75% 8.59\u00b13.67% 15.8\u00b16.15% 15.6\u00b15.79% Random-R 0.19\u00b10.09% 3.43\u00b11.80% 0.97\u00b10.15% 0.41\u00b10.30% 1.14\u00b10.31% RAR-G [3] 1.12\u00b10.11% 15.9\u00b11.53% 8.83\u00b11.98% 15.4\u00b19.29% 14.5\u00b19.25% RAD 0.17\u00b10.09% 2.76\u00b11.32% 0.77\u00b10.11% 0.31\u00b10.19% 0.86\u00b10.25% RAR-D 0.76\u00b10.24% 10.3\u00b13.28% 2.36\u00b10.98% 3.49\u00b12.21% 3.18\u00b12.02% have similar performance. However, when the number of residual points is small such as 50, the Hammersley and Sobol sequences perform better than others, and the equispaced uniform grid and random sampling have the largest errors (about one order of magnitude larger than Hammersley and Sobol). We then test the Random-R method using 30 residual points (Fig. 3B). The accuracy of Random-R has a strong dependence on the period of resampling, and the optimal period of resampling in this problem is around 200. Compared with Random without resampling, the Random-R method always leads to lowerL^2 relative errors regardless of the period of resampling. The error can be lower by one order of magnitude by choosing a proper resampling period. Among all the non-adaptive methods, Random-R performs the best. Next, we test the performance of the nonuniform adaptive sampling methods. In Algorithms 2 and 3, the neural network is first trained using 10 000 steps of Adam. In the RAD method, we use 30 residual points and resample every 1000 iterations. The errors of RAD with different values of kandcare shown in Figs. 3C and D. We note that Random-R is a special case of RAD with either c\u2192 \u221eork= 0. Here, RAD with large values ofcor small values ofkleads to better accuracy, i.e., the points are almost uniformly distributed. For the RAR-D method (Figs. 3E and F), one residual point is added after every 1000 iterations starting from 10 points. When usingk= 2 and c= 0 (the two red lines in Figs. 3E red F), RAR-D performs the best. When using 30 residual points, the errors of all the methods are listed in Table 2. In this diffusion equation, all the methods achieve a good accuracy (<1%). Compared with Random-R (0.12%), RAD and RAR-D (0.11%) are not significantly better. The reason could be that the solution of this diffusion equation is very smooth, so uniformly distributed points are good enough. In our following examples, we show that RAD and RAR-D work significantly better and achieve an error of orders of magnitude smaller than the non-adaptive methods. 3.3 Burgers\u2019 equation The Burgers\u2019 equation is considered defined as: \u2202u \u2202t +u \u2202u \u2202x =\u03bd \u2202^2 u \u2202x^2 , x\u2208[\u2212 1 ,1],t\u2208[0,1], u(x,0) =\u2212sin(\u03c0x), u(\u2212 1 ,t) =u(1,t) = 0, whereuis the flow velocity and\u03bd is the viscosity of the fluid. In this study,\u03bdis set at 0. 01 /\u03c0. Different from the diffusion equation with a smooth solution, the solution of the Burgers\u2019 equation has a sharp front whenx= 0 andtis close to 1. We first test the uniform sampling methods by using the number of residual points ranging from 1,000 to 10,000 (Fig. 4A). The maximum iteration is 15,000 steps with Adam as optimizer followed by 15,000 steps of L-BFGS. Fig. 4A shows that the Hammersley method converges the fastest and reaches the lowestL^2 relative error among all the uniform sampling methods, while the Halton and Sobol sequences also perform adequately. Fig. 4B shows theL^2 relative error as a function of the period of resampling using the RandomR method with 2,000 residual points. Similar to the diffusion equation, the Random-R method always outperforms the Random method. However, the performance of Random-R is not sensitive to the period of resampling if the period is smaller than 100. Choosing a period of resampling too large can negatively affect its performance. When applying the nonuniform adaptive methods, the neural network is first trained using 15,000 steps of Adam and then 1,000 steps of L-BFGS. In the RAD method, we use 2000 residual points, which are resampled every 2,000 iterations (1,000 iterations using Adam followed by 1,000 iterations using L-BFGS). As indicated by Fig. 4C, the RAD method possesses significantly greater advantages over the Random-R method (a special case of RAD by choosingk= 0 orc\u2192 \u221e), whoseL^2 relative errors barely decrease during the training processes. This fact reflects that both extreme cases show worse performance. In contrast, fork= 1 andc= 1 (the red lines in Figs. 4C and D), theL^2 relative error declines rapidly and quickly reaches\u223c 2 \u00d7 10 \u2212^4. The RAD method is also effective when choosing a set ofkandcin a moderate range. For the RAR-D method, 1,000 residual points are selected in the pre-trained process, and 10 residual points are added every 2,000 iterations (1,000 iterations using Adam and 1,000 iterations using L-BFGS as optimizer) until the total number of residual points reaches 2,000. Shown by Figs. 4E and F, the optimal values forkandcare found to be 2 and 0, respectively. Since the solution of Burgers\u2019 equation has a very steep region, when using 2000 residual points, both RAD and RAR-D have competitive advantages over the uniform sampling methods in terms of accuracy and efficiency. For the following three forward PDE problems (Allen-Cahn equation in Section 3.4, wave equation in Section 3.5, and diffusion-reaction equation in Section 3.6), unless otherwise stated, the maximum iterations, the use of optimizer, and the training processes remain the same as the Burgers\u2019 equation. Table 2 summarizes theL^2 relative error for all methods when we fix the number of residual points at 2000. All uniform sampling methods fail to capture the solution well. TheL^2 relative errors given by the Halton, Hammersley, and Sobol methods (\u223c4%) are around one-fourth of that given by the Grid, Random, and LHS methods (>13%). Even though the Random-R performs the best among all uniform methods (1.69\u00b11.67%), the proposed RAD and RAR-D methods can achieve anL^2 relative error two orders of magnitude lower than that (0.02%). Figure 4:L^2 relative errors of different sampling methods for the Burgers\u2019 equation in Section 3.3.(A) Six uniform sampling with fixed residual points. (B) Random-R with different periods of resampling when using 2000 residual points. (CandD) The training trajectory of RAD with different values ofkandcwhen using 2000 residual points. (C)k= 1. (D)c= 1. (EandF) RAR-D with different values ofkandc. Each time 10 new points are added. (E)k= 2. (F)c= The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. 3.4 Allen-Cahn equation Next, we consider the Allen-Cahn equation in the following form: \u2202u \u2202t =D \u2202^2 u \u2202x^2 5(u\u2212u^3 ), x\u2208[\u2212 1 ,1],t\u2208[0,1], u(x,0) =x^2 cos(\u03c0x), u(\u2212 1 ,t) =u(1,t) =\u2212 1 , where the diffusion coefficientD= 0.001. Fig. 5 outlines theL^2 relative errors of different sampling methods for the Allen-Cahn equation. Similar patterns are found for the nonadaptive uniform sampling as in the previous examples. The Hammersley method has the best accuracy (Fig. 5A). As the number of residual points becomes significantly large, the difference between these uniform sampling methods becomes negligible. Except for the equispaced uniform grid method, other uniform sampling methods converge toL^2 relative errors of 10\u2212^3 , about the same magnitude as the number of residual points reaching 10^4. Fig. 5B shows that when using 1000 residual points for Random-R, lowerL^2 relative errors can be obtained if we select a period of resampling less than 500. We next test the performance of RAD for different values ofkandcwhen using a different number of residual points. In Figs. 5C and D, we resampled 500 residual points every 2000 iteration, while in Figs. 5E and F, we used 1000 residual points instead. For both cases, the combination of k= 1 andc= 1 (the red lines in Figs. 5C\u2013F) gives good accuracy. When fewer residual points (e.g., 500) are used, the RAD methods boost the performance of PINNs. Similarly, we also test RAR-D in Figs. 5G\u2013J. In Figs. 5G and H, we pre-train the neural network with 500 residual points and add 10 residual points after every 2000 iterations until the total number of residual points reaches 1000. In Figs. 5I and J, we pre-train the neural network using 1000 residual points and heading to 2000 residual points in the same fashion. We recognize that 2 and 0 are the bestkandcvalues for the RAR-D method for both scenarios, which outperform the RAR-G method. As proven in this example, when applying the RAD and the RAR-D methods, the optimal values ofkandcremain stable even though we choose a different number of residual points. In addition, we find that the optimalkandcfor the Burgers\u2019 and Allen Cahn equations are the same for both the RAD and the RAR-D methods. Thus, we could choose (k= 1,c= 1) for the RAD methods and (k= 2,c= 0) for the RAR-D methods by default when first applied these methods to a new PDE problem. To make a comparison across all sampling methods, Table 2 shows theL^2 relative error for the Allen-Cahn equation when we fix the number of residual points at 1000. The Grid, Random, and LHS methods are prone to substantial errors, which are all larger than 20%. Nevertheless, the other four uniform methods (Halton, Hammersley, Sobol, and Random-R) have greater performance and can achieveL^2 relative errors of less than 1%. Remarkably, the RAD and RAR-D methods we proposed can further bring down theL^2 relative error below 0.1%. Figure 5:L^2 relative errors of different sampling methods for the Allen-Cahn equation in Section 3.4.(A) Six uniform sampling with fixed residual points. (B) Random-R with different periods of resampling when using 1000 residual points. (C\u2013F) The training trajectory of RAD with different values ofkandc. (C and D) 500 residual points are used. (C)k= 1. (D)c= 1. (E and F) 1000 residual points are used. (E)k= 1. (F)c= 1. (G\u2013J) RAR-D with different values ofk andc. (G and H) The number of residual points is increased from 500 to 1000. Each time 10 new points are added. (G)k= 2. (H)c= 0. (I and J) The number of residual points is increased from 1000 to 2000. Each time 10 new points are added. (I)k= 2. (J)c= 0. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. 18 3.5 Wave equation In this example, the following one-dimensional wave equation is considered: \u2202^2 u \u2202t^2 \u2212 4 \u2202^2 u \u2202x^2 = 0, x\u2208[0,1],t\u2208[0,1], u(0,t) =u(1,t) = 0, t\u2208[0,1], u(x,0) = sin(\u03c0x) + 1 2 sin(4\u03c0x), x\u2208[0,1], \u2202u \u2202t (x,0) = 0, x\u2208[0,1], where the exact solution is given as: u(x,t) = sin(\u03c0x) cos(2\u03c0t) + 1 2 sin(4\u03c0x) cos(8\u03c0t). The solution has a multi-scale behavior in both spatial and temporal directions. When we test the six uniform sampling methods, the number of residual points are ranged from 1000 to 6000, with an increment of 1000 each time. The Hammersley method achieves the lowest L^2 relative error with the fastest rate (Fig. 6A). When the number of residual points approaches 6000, the Random, Halton, and Hammersley methods can all obtain anL^2 relative error\u223c 10 \u2212^3. To determine the effectiveness of Random-R when using different numbers of residual points, we test the following three scenarios: small (1000 points), medium (4000 points), and large (10.000) sets of residual points (Figs. 6B, C, and D). In the medium case (Fig. 6C), the Random-R attainsL^2 relative errors magnitudes lower than the Random method. However, in the small and large cases (Figs. 6B and D), the Random-R methods show no advantage over the Random method regardless of the period of resampling. This is because when the number of residual points is small, both the Random and Random-R methods fail to provide accurate predictions. On the other hand, if the number of residual points is large, the predictions by the Random method are already highly accurate, so the Random-R is unable to further improve the accuracy. Since the optimal sets ofkandcfor both RAD and RAR-D methods are found to be the same for the Burgers\u2019 and the Allen Cahn equations, in this numerical experiment, we only apply the default settings (i.e., RAD:k= 1 andc= 1; RAR-D:k= 2 andc= 0) to investigate the effect of other factors, including the number of residual points for the RAD method and the number of points added to the RAR-D method. In Fig. 6E, we compare the performance of three nonuniform adaptive sampling methods under the same number of residual points from 1000 to 10 000. We first train the network using 15 000 iterations of Adam and 1000 iterations of L-BFGS, and then after each resampling in RAD or adding new points in RAR-D/RAR-G, we train the network with 1000 iterations of L-BFGS. For the RAR-G and the RAR-D methods, we first train the network with 50% of the final number of the residual points and add 10 residual points each time until reaching the total number of residual points. As we can see from Fig. 6E, the RAD achieves much better results when the number of residual points is small. As the number of residual points increases, the RAR-D method acts more effectively and eventually reaches comparable accuracy to the RAD method. Since the RAD method is more computationally costly than the RAR-D methods with the same number of residual points, we suggest applying the RAD method when the number of residual points is small and the RAR-D method when the number of residual points is large. We next investigate the RAD method with a different number of residual points (i.e., 1000, 2000, 5000, and 10 000). Fig. 6F illustrates that if we increase the number of residual points, lower Figure 6: L^2 relative errors of different sampling methods for the wave equation in Section 3.5.(A) Six uniform sampling with fixed residual points. (B,C, andD) Random-R with different periods of resampling when using (B) 1000 residual points, (C) 4000 residual points, and (D) 10000 residual points. (E) Comparison among RAD (k= 1 andc= 1), RAR-D (k= 2 and c= 0), and RAR-G for different numbers of residual points. (F) The training trajectory of RAD (k= 1 andc= 1) uses different numbers of residual points. (GandH) Convergence of RAR-D (k= 2 andc= 0) when adding a different number of new points each time. (G) New points are added starting from 1000 residual points. (H) New points are added starting from 2500 residual points. (I) Convergence of RAR-G when adding a different number of new points each time. New points are added starting from 2500 residual points. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. L^2 relative error can be achieved but with diminishing marginal effect. We train the network for more than 500 000 iterations to see if theL^2 relative error can further decrease. However, theL^2 relative errors converge and remain relatively stable after 100 000 iterations. One important factor to consider in the RAR-D and the RAR-G methods is how new points are added. We can either add a small number of residual points each time and prolong the training process or add a large number of residual points each time and shorten the process. In Fig. 6G, we first train the network with 1000 residual points and then add new residual points at different rates until the total number of residual points reaches 2000. After adding new residual points each time, we train the network using 1000 steps of L-BFGS. Likewise, in Fig. 6H, we first train the network with 2500 residual points and add new points at different rates until the total number of residual points reaches 5000. In both cases (Figs. 6G and H) that use the RAR-D methods, we find that the best strategy is to add 10 points each time. However, shown by two red-shaded regions in Figs. 6G and H, the results are more stable when we use a larger number of residual points. Fig. 6I is set up the same way as Fig. 6H but tests the RAR-G method. The best strategy for the RAR-G is identical to that of the RAR-D. Table 2 outlines theL^2 relative error for the wave equation using all methods when the number of residual points equals 2000. All uniform methods with fixed residual points perform poorly (error >50%) and fail to approximate the truth values. Random-R, as a special case of the proposed RAD, givesL^2 relative errors of around 1%. The RAR-D method significantly enhances the prediction accuracy resulting inL^2 relative errors under 0.3%. In addition, the RAD with the default setting ofkandcconverges toL^2 relative errors under 0.1%. 3.6 Diffusion-reaction equation The first inverse problem we consider is the diffusion-reaction system as follows: \u03bb d^2 u dx^2 \u2212k(x)u=f, x\u2208[0,1], wheref = sin(2\u03c0x) is the source term. \u03bb= 0.01 is the diffusion coefficient, anduis the solute concentration. In this problem, we aim to infer the space-dependent reaction ratek(x) with given measurements on the solutionu. The exact unknown reaction rate is k(x) = 0.1 +e\u2212^0.^5 (x\u2212 0 .5)^2 (^152). We aim to learn the unknown functionk(x) and solve foru(x) by using eight observations ofu, which are uniformly distributed on the domainx\u2208[0,1], including two points on both sides of the boundaries. TheL^2 relative errors for both the solutionu(Figs. 7A, C, and E) and the unknown functionk(Figs. 7B, D, and F) are computed. The maximum number of iterations is 50 000 steps of Adam. Figs. 7A and B summarize the performance of all uniform sampling methods. We note that in 1D, the Hammersley and Halton sequences are identical and outperform other uniform methods. We fix the residual points at 15 and compare the Random method with the Random-R method. TheL^2 relative errors (Figs. 7C and D) given by the Random-R remain steady, disregarding the changes in the period of resampling, and are approximately the same as that produced by the Random method. This is because the reaction-diffusion system is fairly simple and can be easily handled by uniform sampling methods without resampling. Next, we compare the Random, RAD, RAR-G, and RAR-D methods with default settings (i.e., RAD:k= 1 andc= 1; RAR-D:k= 2 andc= 0) using a different number of residual points. For the random and RAD methods, the maximum number of iterations is 50 000 steps of Adam. For Figure 7:L^2 relative errors of different sampling methods foruandkin the diffusionreaction equation in Section 3.6.(AandB) Six uniform sampling with fixed residual points. (CandD) Random-R with different periods of resampling when using 15 residual points. (Eand F) Comparison among Random, RAD (k= 1 andc= 1), RAR-G, and RAR-D (k= 2 andc= 0) for different numbers of residual points. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. the RAR-G/RAR-D, we first train the neural network with 50% of the total number of residual points for 10 000 steps of Adam; then we add one point each time and train for 1000 steps of Adam until we meet the total number of residual points. As shown by Figs. 7E and F, the RAD method surpasses other methods and is able to produce lowL^2 relative error even when the number of residual points is very small. However, RAR-G and RAR-D are even worse than the Random sampling. To sum up, we fix the number of residual points at 15 and present theL^2 relative error for both the solution and unknown function in Table 3. The RAD yields the minimumL^2 relative error (0.17% foru(x); 2.76% fork(x)). However, due to the simplicity of this PDE problem, some uniform sampling methods, especially the Sobol and Random-R, have comparable performance to the RAD. Generally speaking, we recognize that the uniform sampling methods are adequate when solving this inverse PDE with smooth solutions. Still, the RAD method can further enhance the performance of PINNs, especially when the number of residual points is small. 3.7 Korteweg-de Vries equation The second inverse problem we solve is the Korteweg-de Vries (KdV) equation: \u2202u \u2202t +\u03bb 1 u \u2202u \u2202x +\u03bb 2 \u2202^3 u \u2202x^3 = 0, x\u2208[\u2212 1 ,1], t\u2208[0,1], where\u03bb 1 and\u03bb 2 are two unknown parameters. The exact values for\u03bb 1 and\u03bb 2 are 1 and 0.0025, respectively. The initial condition isu(x,t= 0) = cos(\u03c0x), and periodic boundary conditions are used. To infer\u03bb 1 and\u03bb 2 , we assume that we have the observations of two solution snapshots u(x,t= 0.2) andu(x,t= 0.8) at 64 uniformly distributed points at each time. In Fig. 8, the first column (Figs. 8A, D, and G) shows theL^2 relative error of the solutionu, while the second column (Figs. 8B, E, and H) and the third column (Figs. 8C, F, and I) illustrate the relative errors for\u03bb 1 and\u03bb 2 , respectively. The maximum iteration is 100 000 steps of Adam. Hammersley achieves better accuracy than the other uniform sampling methods. The Sobol and Halton methods behave comparably as these two curves (the yellow and green curves in Figs. 8A, B, and C) are almost overlapping. Shown in Figs. 8D, E and F, the Random-R method yields higher accuracy than the Random method by about one order of magnitude in all cases when using 1000 residual points. A smaller period of resampling leads to smaller errors. Figs. 8G, H, and I compare the Random-R, Random, RAD, RAR-G, and RAR-D methods using the same number of residual points and the total number of iterations. For the Random and the Random-R methods, we train the network for 100 000 steps of Adams. For the RAD methods, we first train the network using 50 000 steps of Adams; then, we resample the residual points and train for 1000 steps of Adams 50 times. In order to fix the total number of iterations for the RARG/RAR-D methods to 100 000, we accordingly adjust the number of new residual points added each time. For example, if the final number of residual points is 500, we first train the network using 250 residual points (i.e., 50% of the total number of residual points) with 50 000 steps of Adams; and we consequently add 5 points and train for 1000 steps of Adams each time. If the final number of residual points is 1000, we first train the network using 500 residual points with 50 000 steps of Adams; and then we add 10 points and train for 1000 steps of Adams each time. As demonstrated by Figs. 8G, H, and I, the RAD method is the best, while the Random-R method is also reasonably accurate. We show one example of the training process (Figs. 8J, K, and L) when the number of residual points is 600 to illustrate the convergence of the solution,\u03bb 1 , and\u03bb 2 during training. The resampling strategies, especially the RAD method, achieve the greatest success among all sampling methods. Figure 8:L^2 relative errors ofuand relative errors of\u03bb 1 and\u03bb 2 using different sampling methods for the Korteweg-de Vries equation in Section 3.7. (A,B, andC) Six uniform sampling with fixed residual points. (D,E, andF) Random-R with different periods of resampling when using 1000 residual points. (G,H, andI) Comparison among Random, Random-R, RAD (k= 1 andc= 1), RAR-G, and RAR-D (k= 2 andc= 0) for different number of residual points. (J,K, andL) Examples of the training trajectories using Random, Random-R, RAD (k= 1 and c= 1), RAR-G, and RAR-D (k= 2 andc= 0) with 600 residual points. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. Table 3 demonstrates theL^2 relative errors for the solutionu(x,t) and the relative error of two unknown parameters\u03bb 1 and\u03bb 2 , for all methods when the number of residual points is set at The lowestL^2 relative errors for uniform sampling with fixed points are given by Hammersley (\u223c 5%). The Random-R is the second-best method and providesL^2 relative errors of around 1%. With the smallest errors (<1%) and standard deviations, the RAD method has compelling advantages over all other methods in terms of accuracy and robustness. It is noteworthy that the RAR-D method provides adequate accuracy (\u223c3%) and is less expensive than the Random-R and RAD methods when the number of residual points is the same. Therefore, the RAR-D is also a valuable approach to consider. 4 Conclusions In this paper, we present a comprehensive study of two categories of sampling for physics-informed neural networks (PINNs), including non-adaptive uniform sampling and adaptive nonuniform sampling. For the non-adaptive uniform sampling, we have considered six methods: (1) equispaced uniform grid (Grid), (2) uniformly random sampling (Random), (3) Latin hypercube sampling (LHS), (4) Halton sequence (Halton), (5) Hammersley sequence (Hammersley), and (6) Sobol sequence (Sobol). We have also considered a resampling strategy for uniform sampling (Random-R). For the adaptive nonuniform sampling, motivated by the residual-based adaptive refinement with greed (RAR-G) [3], we proposed two new residual-based adaptive sampling methods: residual-based adaptive distribution (RAD) and residual-based adaptive refinement with distribution (RAR-D). We extensively investigated the performance of these ten sampling methods in solving four forward and two inverse problems of partial differential equations (PDEs) with many setups, such as a different number of residual points. Our results show that the proposed RAD and RAR-D significantly improve the accuracy of PINNs by orders of magnitude, especially when the number of residual points is small. RAD and RAR-D also have great advantages for the PDEs with complicated solutions, e.g., the solution of the Burgers\u2019 equation with steep gradients and the solution of the wave equation with a multi-scale behavior. A summary of the comparison of these methods can be found in Section 3.1. Based on our empirical results, we summarize the following suggestions as a practical guideline in choosing sampling methods for PINNs. RAD withk= 1 andc= 1 can be chosen as the default sampling method when solving a new PDE. The hyperparameterskandccan be tuned to balance the points in the locations with large and small PDE residuals. RAR-D can achieve comparable accuracy to RAD, but RAR-D is more computationally efficient as it gradually increases the number of residual points. Hence, RAR-D (k= 2 and c= 0 by default) is preferable for the case with limited computational resources. Random-R can be used in the situation where adaptive sampling is not allowed, e.g., it is difficult to sample residual points according to a probability density function. The period of resampling should not be chosen as too small or too large. A low-discrepancy sequence (e.g., Hammersley) should be considered rather than Grid, Ran- dom, or LHS, when we have to use a fixed set of residual points, such as in PINNs with the augmented Lagrangian method (hPINNs) [8]. In this study, we sample residual points in RAD and RAR-D by using a brute-force approach, which is simple, easy to implement, and sufficient for many PDEs. However, for high-dimensional problems, we need to use other methods, such as generative adversarial networks (GANs) [46], as was done in Ref. [41]. Moreover, the probability of sampling a pointxis only considered as p(x)\u221d \u03b5 k(x) E[\u03b5k(x)]+c. While this probability works very well in this study, it is possible that there exists another better choice. We can learn a new probability density function by meta-learning, as was done for loss functions of PINNs in Ref. [11]. References [1] M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.Journal of Computational Physics, 378:686\u2013707, 2019. [2] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations.Science, 367(6481):1026\u20131030, 2020. [3] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. DeepXDE: A deep learning library for solving differential equations.SIAM Review, 63(1):208\u2013228, 2021. [4] George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning.Nature Reviews Physics, 3(6):422\u2013440, 2021. [5] Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse problems in nano-optics and metamaterials.Optics Express, 28(8):11618, 2020. [6] Alireza Yazdani, Lu Lu, Maziar Raissi, and George Em Karniadakis. Systems biology informed deep learning for inferring parameters and hidden dynamics. PLOS Computational Biology, 16(11), 2020. [7] Mitchell Daneker, Zhen Zhang, George Em Kevrekidis, and Lu Lu. Systems biology: Identifiability analysis and parameter identification via systems-biology informed neural networks. arXiv preprint arXiv:2202.01723, 2022. [8] Lu Lu, Rapha \u0308el Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G. Johnson. Physics-informed neural networks with hard constraints for inverse design. SIAM Journal on Scientific Computing, 43(6), 2021. [9] Guofei Pang, Lu Lu, and George Em Karniadakis. fPINNs: Fractional physics-informed neural networks. SIAM Journal on Scientific Computing, 41(4), 2019. [10] Dongkun Zhang, Lu Lu, Ling Guo, and George Em Karniadakis. Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems.Journal of Computational Physics, 397:108850, 2019. [11] Apostolos F Psaros, Kenji Kawaguchi, and George Em Karniadakis. Meta-learning PINN loss functions. Journal of Computational Physics, 458:111121, 2022. [12] Jeremy Yu, Lu Lu, Xuhui Meng, and George Em Karniadakis. Gradient-enhanced physicsinformed neural networks for forward and inverse PDE problems.Computer Methods in Applied Mechanics and Engineering, 393:114823, 2022. [13] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow pathologies in physics-informed neural networks. SIAM Journal on Scientific Computing, 43(5):A3055\u2013A3081, 2021. [14] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why PINNs fail to train: A neural tangent kernel perspective. Journal of Computational Physics, 449:110768, 2022. [15] Zixue Xiang, Wei Peng, Xu Liu, and Wen Yao. Self-adaptive loss balanced physics-informed neural networks.Neurocomputing, 2022. [16] Levi McClenny and Ulisses Braga-Neto. Self-adaptive physics-informed neural networks using a soft attention mechanism.arXiv preprint arXiv:2009.04544, 2020. [17] Yiqi Gu, Haizhao Yang, and Chao Zhou. SelectNet: Self-paced learning for high-dimensional partial differential equations.Journal of Computational Physics, 441:110444, 2021. [18] Wensheng Li, Chao Zhang, Chuncheng Wang, Hanting Guan, and Dacheng Tao. Revisiting PINNs: Generative adversarial physics-informed neural networks and point-weighting method. arXiv preprint arXiv:2205.08754, 2022. [19] Xuhui Meng, Zhen Li, Dongkun Zhang, and George Em Karniadakis. PPINN: Parareal physicsinformed neural network for time-dependent pdes. Computer Methods in Applied Mechanics and Engineering, 370:113250, 2020. [20] Khemraj Shukla, Ameya D. Jagtap, and George Em Karniadakis. Parallel physics-informed neural networks via domain decomposition. Journal of Computational Physics, 447:110683, 2021. [21] Ameya D. Jagtap and George Em Karniadakis. Extended physics-informed neural networks (XPINNs): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations. Communications in Computational Physics, 28(5):2002\u20132041, 2020. [22] Colby L Wight and Jia Zhao. Solving Allen-Cahn and Cahn-Hilliard equations using the adaptive physics informed neural networks. arXiv preprint arXiv:2007.04542, 2020. [23] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks.Advances in Neural Information Processing Systems, 34:26548\u201326560, 2021. [24] Revanth Mattey and Susanta Ghosh. A novel sequential method to train physics informed neural networks for allen cahn and cahn hilliard equations. Computer Methods in Applied Mechanics and Engineering, 390:114474, 2022. [25] Katsiaryna Haitsiukevich and Alexander Ilin. Improved training of physics-informed neural networks with model ensembles. arXiv preprint arXiv:2204.05108, 2022. [26] Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality is all you need for training physics-informed neural networks. arXiv preprint arXiv:2203.07404, 2022. [27] Pola Lydia Lagari, Lefteri H Tsoukalas, Salar Safarkhani, and Isaac E Lagaris. Systematic construction of neural forms for solving partial differential equations inside rectangular domains, subject to initial, boundary and interface conditions. International Journal on Artificial Intelligence Tools, 29(05):2050009, 2020. [28] Suchuan Dong and Naxian Ni. A method for representing periodic functions and enforcing exactly periodic boundary conditions with deep neural networks. Journal of Computational Physics, 435:110242, 2021. [29] Michael D McKay, Richard J Beckman, and William J Conover. A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. Technometrics, 42(1):55\u201361, 2000. [30] Michael Stein. Large sample properties of simulations using Latin hypercube sampling.Technometrics, 29(2):143\u2013151, 1987. [31] Il\u2019ya Meerovich Sobol\u2019. On the distribution of points in a cube and the approximate evaluation of integrals.Zhurnal Vychislitel\u2019noi Matematiki i Matematicheskoi Fiziki, 7(4):784\u2013802, 1967. [32] John H Halton. On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals. Numerische Mathematik, 2(1):84\u201390, 1960. [33] JM Hammersley and DC Handscomb. Monte-Carlo methods, mathuen, 1964. [34] Hongwei Guo, Xiaoying Zhuang, Xiaoyu Meng, and Timon Rabczuk. Analysis of three dimensional potential problems in non-homogeneous media with deep learning based collocation method.arXiv preprint arXiv:2010.12060, 2020. [35] Sourav Das and Solomon Tesfamariam. State-of-the-art review of design of experiments for physics-informed deep learning.arXiv preprint arXiv:2202.06416, 2022. [36] Zhiping Mao, Ameya D Jagtap, and George Em Karniadakis. Physics-informed neural networks for high-speed flows. Computer Methods in Applied Mechanics and Engineering, 360:112789, 2020. [37] Mohammad Amin Nabian, Rini Jasmine Gladstone, and Hadi Meidani. Efficient training of physics-informed neural networks via importance sampling.Computer-Aided Civil and Infrastructure Engineering, 2021. [38] Bastian Zapf, Johannes Haubner, Miroslav Kuchta, Geir Ringstad, Per Kristian Eide, and Kent-Andre Mardal. Investigating molecular transport in the human brain from MRI with physics-informed neural networks.arXiv preprint arXiv:2205.02592, 2022. [39] Arka Daw, Jie Bu, Sifan Wang, Paris Perdikaris, and Anuj Karpatne. Rethinking the importance of sampling in physics-informed neural networks. arXiv preprint arXiv:2207.02338, 2022. [40] Wenhan Gao and Chunmei Wang. Active learning based sampling for high-dimensional nonlinear partial differential equations. arXiv preprint arXiv:2112.13988, 2021. [41] Kejun Tang, Xiaoliang Wan, and Chao Yang. DAS: A deep adaptive sampling method for solving partial differential equations.arXiv preprint arXiv:2112.14038, 2021. [42] Wei Peng, Weien Zhou, Xiaoya Zhang, Wen Yao, and Zheliang Liu. RANG: a residualbased adaptive node generation method for physics-informed neural networks. arXiv preprint arXiv:2205.01051, 2022. [43] Shaojie Zeng, Zong Zhang, and Qingsong Zou. Adaptive deep neural networks methods for high-dimensional partial differential equations.Journal of Computational Physics, 463:111232, 2022. [44] John M Hanna, Jose V Aguado, Sebastien Comas-Cardona, Ramzi Askri, and Domenico Borzacchiello. Residual-based adaptivity for two-phase flow simulation in porous media using physics-informed neural networks.Computer Methods in Applied Mechanics and Engineering, 396:115100, 2022. [45] Melissa E O\u2019Neill. Pcg: A family of simple fast space-efficient statistically good algorithms for random number generation.ACM Transactions on Mathematical Software, 2014. [46] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.Advances in neural information processing systems, 27, 2014. [47] Bengt Fornberg and Natasha Flyer. Fast generation of 2-D node distributions for mesh-free pde discretizations.Computers & Mathematics with Applications, 69(7):531\u2013544, 2015.","title":"A Comprehensive Study of Non-Adaptive and Residual-Based Adaptive Sampling for Physics-Informed Neural Networks <br> \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u81ea\u9002\u5e94\u91c7\u6837\u548c\u57fa\u4e8e\u6b8b\u5dee\u7684\u81ea\u9002\u5e94\u91c7\u6837\u7684\u7efc\u5408\u7814\u7a76"},{"location":"Models/PINNs/PN-2207.10289/#a-comprehensive-study-of-non-adaptive-and-residual-based-adaptive-sampling-for-physics-informed-neural-networks","text":"\u4f5c\u8005: Chenxi Wu1,\u2020, Min Zhu1,\u2020, Qinyang Tan^2 , Yadhu Kartha^3 , and Lu Lu1,* \u673a\u6784: College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA \u65f6\u95f4: 2022-07-21 \u9884\u5370: arXiv:2207.10289v1 \u9886\u57df: physics.comp-ph \u6807\u7b7e: \u504f\u5fae\u5206\u65b9\u7a0b, \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc, \u6b8b\u5dee\u70b9\u5206\u5e03, \u975e\u81ea\u9002\u5e94\u5747\u5300\u91c7\u6837, \u5e26\u91cd\u91c7\u6837\u7684\u5747\u5300\u91c7\u6837, \u57fa\u4e8e\u6b8b\u5dee\u7684\u81ea\u9002\u5e94\u91c7\u6837 \u5f15\u7528: 47 \u7bc7","title":"A Comprehensive Study of Non-Adaptive and Residual-Based Adaptive Sampling for Physics-Informed Neural Networks  \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u81ea\u9002\u5e94\u91c7\u6837\u548c\u57fa\u4e8e\u6b8b\u5dee\u7684\u81ea\u9002\u5e94\u91c7\u6837\u7684\u7efc\u5408\u7814\u7a76"},{"location":"Models/PINNs/PN-2207.10289/#abstract","text":"Physics-informed neural networks (PINNs) have shown to be an effective tool for solving both forward and inverse problems of partial differential equations (PDEs). PINNs embed the PDEs into the loss of the neural network using automatic differentiation, and this PDE loss is evaluated at a set of scattered spatio-temporal points (called residual points). The location and distribution of these residual points are highly important to the performance of PINNs. However, in the existing studies on PINNs, only a few simple residual point sampling methods have mainly been used. Here, we present a comprehensive study of two categories of sampling for PINNs: non-adaptive uniform sampling and adaptive nonuniform sampling. We consider six uniform sampling methods, including (1) equispaced uniform grid, (2) uniformly random sampling, (3) Latin hypercube sampling, (4) Halton sequence, (5) Hammersley sequence, and (6) Sobol sequence. We also consider a resampling strategy for uniform sampling. To improve the sampling efficiency and the accuracy of PINNs, we propose two new residual-based adaptive sampling methods: residual-based adaptive distribution (RAD) and residual-based adaptive refinement with distribution (RAR-D), which dynamically improve the distribution of residual points based on the PDE residuals during training. Hence, we have considered a total of 10 different sampling methods, including six non-adaptive uniform sampling, uniform sampling with resampling, two proposed adaptive sampling, and an existing adaptive sampling. We extensively tested the performance of these sampling methods for four forward problems and two inverse problems in many setups. Our numerical results presented in this study are summarized from more than 6000 simulations of PINNs. We show that the proposed adaptive sampling methods of RAD and RAR-D significantly improve the accuracy of PINNs with fewer residual points for both forward and inverse problems. The results obtained in this study can also be used as a practical guideline in choosing sampling methods.","title":"Abstract"},{"location":"Models/PINNs/PN-2207.10289/#1-introduction","text":"Physics-informed neural networks (PINNs) [1] have emerged in recent years and quickly became a powerful tool for solving both forward and inverse problems of partial differential equations (PDEs) via deep neural networks (DNNs) [2, 3, 4]. PINNs embed the PDEs into the loss of the neural network using automatic differentiation. Compared with traditional numerical PDE solvers, such as the finite difference method (FDM) and the finite element method (FEM), PINNs are mesh free and therefore highly flexible. Moreover, PINNs can easily incorporate both physics-based constraints and data measurements into the loss function. PINNs have been applied to tackle diverse problems in computational science and engineering, such as inverse problems in nanooptics, metamaterials [5], and fluid dynamics [2], parameter estimation in systems biology [6, 7], and problems of inverse design and topology optimization [8]. In addition to standard PDEs, PINNs have also been extended to solve other types of PDEs, including integro-differential equations [3], fractional PDEs [9], and stochastic PDEs [10]. Despite the past success, addressing a wide range of PDE problems with increasing levels of complexity can be theoretically and practically challenging, and thus many aspects of PINNs still require further improvements to achieve more accurate prediction, higher computational efficiency, and training robustness [4]. A series of extensions to the vanilla PINN have been proposed to boost the performance of PINNs from various aspects. For example, better loss functions have been discovered via meta-learning [11], and gradient-enhanced PINNs (gPINNs) have been developed to embed the gradient information of the PDE residual into the loss [12]. In PINNs, the total loss is a weighted summation of multiple loss terms corresponding to the PDE and initial/boundary conditions, and different methods have been developed to automatically tune these weights and balance the losses [13, 14, 15]. Moreover, a different weight for each loss term could be set at every training point [16, 17, 8, 18]. For problems in a large domain, decomposition of the spatiotemporal domain accelerates the training of PINNs and improves their accuracy [19, 20, 21]. For time-dependent problems, it is usually helpful to first train PINNs within a short time domain and then gradually expand the time intervals of training until the entire time domain is covered [22, 23, 24, 25, 26]. In addition to these general methods, other problem-specific techniques have also been developed, e.g., enforcing Dirichlet or periodic boundary conditions exactly by constructing special neural network architectures [27, 28, 8]. PINNs are mainly optimized against the PDE loss, which guarantees that the trained network is consistent with the PDE to be solved. PDE loss is evaluated at a set of scattered residual points. Intuitively, the effect of residual points on PINNs is similar to the effect of mesh points on FEM, and thus the location and distribution of these residual points should be highly important to the performance of PINNs. However, in previous studies on PINNs, two simple residual point sampling methods (i.e., an equispaced uniform grid and uniformly random sampling) have mainly been used, and the importance of residual point sampling has largely been overlooked.","title":"1 Introduction"},{"location":"Models/PINNs/PN-2207.10289/#11-related-work-and-our-contributions","text":"Different residual point sampling methods can be classified into two categories: uniform sampling and nonuniform sampling. Uniform sampling can be obtained in multiple ways. For example, we could use the nodes of an equispaced uniform grid as the residual points or randomly sample the points according to a continuous uniform distribution in the computational domain. Although these two sampling methods are simple and widely used, alternative sampling methods may be applied. The Latin hypercube sampling (LHS) [29, 30] was used in Ref. [1], and the Sobol sequence [31] was first used for PINNs in Ref. [9]. The Sobol sequence is one type of quasi random low-discrepancy sequences among other sequences, such as the Halton sequence [32], and the Hammersley sequence [33]. Low-discrepancy sequences usually perform better than uniformly distributed random numbers in many applications such as numerical integration; hence, a comprehensive comparison of these methods for PINNs is required. However, very few comparisons [34, 35] have been performed. In this study, we extensively compared the performance of different uniform sampling methods, including (1) equispaced uniform grid, (2) uniformly random sampling, (3) LHS, (4) Sobol sequence, (5) Halton sequence, and (6) Hammersley sequence. In supervised learning, the dataset is fixed during training, but in PINNs, we can select residual points at any location. Hence, instead of using the same residual points during training, in each optimization iteration, we could select a new set of residual points, as first emphasized in Ref. [3]. While this strategy has been used in some works, it has not yet been systematically tested. Thus, in this study, we tested the performance of such a resampling strategy and investigated the effect of the number of residual points and the resampling period for the first time. Uniform sampling works well for some simple PDEs, but it may not be efficient for those that are more complicated. To improve the accuracy, we could manually select the residual points in a nonuniform way, as was done in Ref. [36] for high-speed flows, but this approach is highly problem-dependent and usually tedious and time-consuming. In this study, we focus on automatic and adaptive nonuniform sampling. Motivated by the adaptive mesh refinement in FEM, Lu et al. [3] proposed the first adaptive nonuniform sampling for PINNs in 2019, the residual-based adaptive refinement (RAR) method, which adds new residual points in the locations with large PDE residuals. In 2021, another sampling strategy [37] was developed, where all the residual points were resampled according to a probability density function (PDF) proportional to the PDE residual. In this study, motivated by these two ideas, we proposed two new sampling strategies: residual-based adaptive distribution (RAD), where the PDF for sampling is a nonlinear func- tion of the PDE residual; residual-based adaptive refinement with distribution (RAR-D), which is a hybrid method of RAR and RAD, i.e., the new residual points are added according to a PDF. During the preparation of this paper, a few new studies appeared [38, 39, 40, 41, 42, 43, 44] that also proposed modified versions of RAR or PDF-based resampling. Most of these methods are special cases of the proposed RAD and RAR-D, and our methods can achieve better performance. We include a detailed comparison of these strategies in Section 2.4, after introducing several notations and our new proposed methods. In this study, we have considered a total of 10 different sampling methods, including seven non-adaptive sampling methods (six different uniform samplings and one uniform sampling with resampling) and three adaptive sampling approaches (RAR, RAD, and RAR-D). We compared the performance of these sampling methods for four forward problems of PDEs and investigated the effect of the number of residual points. We also compared their performance for two inverse problems that have not yet been consid- ered in the literature. We performed more than 6000 simulations of PINNs to obtain all the results shown in this study.","title":"1.1 Related work and our contributions"},{"location":"Models/PINNs/PN-2207.10289/#12-organization","text":"This paper is organized as follows. In Section 2, after providing a brief overview of PINNs and different non-adaptive sampling strategies, two new adaptive nonuniform sampling strategies (RAD and RAR-D) are proposed. In Section 3, we compare the performance of 10 different methods for six different PDE problems, including four forward problems and two inverse problems. Section 4 summarizes the findings and concludes the paper.","title":"1.2 Organization"},{"location":"Models/PINNs/PN-2207.10289/#2-methods","text":"This section briefly reviews physics-informed neural networks (PINNs) in solving forward and inverse partial differential equations (PDEs). Then different types of uniformly sampling are introduced. Next, two nonuniform residual-based adaptive sampling methods are proposed to enhance the accuracy and training efficiency of PINNs. Finally, a comparison of related methods is presented.","title":"2 Methods"},{"location":"Models/PINNs/PN-2207.10289/#21-pinns-in-solving-forward-and-inverse-pdes","text":"We consider the PDE parameterized by\u03bbdefined on a domain \u2126\u2282Rd, f(x;u(x)) =f","title":"2.1 PINNs in solving forward and inverse PDEs"},{"location":"Models/PINNs/PN-2207.10289/#_1","text":"x; \u2202u \u2202x 1","title":"("},{"location":"Models/PINNs/PN-2207.10289/#_2","text":"\u2202u \u2202xd","title":""},{"location":"Models/PINNs/PN-2207.10289/#_3","text":"\u2202^2 u \u2202x 1 \u2202x 1","title":""},{"location":"Models/PINNs/PN-2207.10289/#_4","text":"\u2202^2 u \u2202x 1 \u2202xd ;...;\u03bb","title":""},{"location":"Models/PINNs/PN-2207.10289/#_5","text":"= 0, x= (x 1 ,...,xd)\u2208\u2126, with boundary conditions on\u2202\u2126 B(u,x) = 0, andu(x) denotes the solution atx. In PINNs, the initial condition is treated as the Dirichlet boundary condition. A forward problem is aimed to obtain the solutionu across the entire domain, where the model parameters\u03bbare known. In practice, the model parameters\u03bbmight be unknown, but some observations from the solutionuare available, which lead to an inverse problem. An inverse problem is aimed to discover parameters\u03bbthat best describe the observed data from the solution. PINNs are capable of addressing both forward and inverse problems. To solve a forward problem, the solutionuis represented with a neural network \u02c6u(x;\u03b8). The network parameters\u03b8are trained to approximate the solutionu, such that the loss function is minimized [1, 3]: L(\u03b8;T) =wfLf(\u03b8;Tf) +wbLb(\u03b8;Tb), where Lf(\u03b8;Tf) =","title":")"},{"location":"Models/PINNs/PN-2207.10289/#1","text":"|Tf|","title":"1"},{"location":"Models/PINNs/PN-2207.10289/#_6","text":"x\u2208Tf","title":"\u2211"},{"location":"Models/PINNs/PN-2207.10289/#_7","text":"\u2223\u2223f(x; \u2202u\u02c6 \u2202x 1","title":"\u2223\u2223"},{"location":"Models/PINNs/PN-2207.10289/#_8","text":"\u2202\u02c6u \u2202xd","title":""},{"location":"Models/PINNs/PN-2207.10289/#_9","text":"\u2202^2 u\u02c6 \u2202x 1 \u2202x 1","title":""},{"location":"Models/PINNs/PN-2207.10289/#_10","text":"\u2202^2 u\u02c6 \u2202x 1 \u2202xd ;...;\u03bb)","title":""},{"location":"Models/PINNs/PN-2207.10289/#_11","text":"","title":"\u2223\u2223"},{"location":"Models/PINNs/PN-2207.10289/#_12","text":"2 , (1) Lb(\u03b8;Tb) =","title":"\u2223\u2223"},{"location":"Models/PINNs/PN-2207.10289/#1_1","text":"|Tb|","title":"1"},{"location":"Models/PINNs/PN-2207.10289/#_13","text":"x\u2208Tb |B(\u02c6u,x)|^2 , andwfandwbare the weights. Two sets of points are samples both inside the domain (Tf) and on the boundaries (Tb). Here,TfandTbare referred to as the sets of \u201cresidual points\u201d, andT=Tf\u222aTb. To solve the inverse problem, an additional loss term corresponding to the misfit of the observed data at the locationsTi, defined as Li(\u03b8,\u03bb;Ti) =","title":"\u2211"},{"location":"Models/PINNs/PN-2207.10289/#1_2","text":"|Ti|","title":"1"},{"location":"Models/PINNs/PN-2207.10289/#_14","text":"x\u2208Ti |u\u02c6(x)\u2212u(x)|^2 , is added to the loss function. The loss function is then defined as L(\u03b8,\u03bb;T) =wfLf(\u03b8,\u03bb;Tf) +wbLb(\u03b8,\u03bb;Tb) +wiLi(\u03b8,\u03bb;Ti), with an additional weightwi. Then the network parameters\u03b8are trained simultaneously with\u03bb. For certain PDE problems, it is possible to enforce boundary conditions directly by constructing a special network architecture [27, 28, 8, 12], which eliminates the loss term of boundary conditions. In this study, the boundary conditions are enforced exactly and automatically. Hence, for a forward problem, the loss function is L(\u03b8,\u03bb;T) =Lf(\u03b8,\u03bb;Tf). For an inverse problem, the loss function is L(\u03b8,\u03bb;T) =wfLf(\u03b8,\u03bb;Tf) +wiLi(\u03b8,\u03bb;Ti), where we choosewf=wi= 1 for the diffusion-reaction equation in Section 3.6, andwf= 1,wi= 1000 for the Korteweg-de Vries equation in Section 3.7.","title":"\u2211"},{"location":"Models/PINNs/PN-2207.10289/#22-uniformly-distributed-non-adaptive-sampling","text":"The training of PINNs requires a set of residual points (Tf). The sampling strategy ofTf plays a vital role in promoting the accuracy and computational efficiency of PINNs. Here, we discuss several sampling approaches. 2.2.1 Fixed residual points In most studies of PINNs, we specify the residual points at the beginning of training and never change them during the training process. Two simple sampling methods (equispaced uniform grids and uniformly random sampling) have been commonly used. Other sampling methods, such as the Latin hypercube sampling (LHS) [29, 30] and the Sobol sequence [31], have also been used in some studies [1, 9, 34]. The Sobol sequence is one type of quasi-random low-discrepancy sequences. Low-discrepancy sequences are commonly used as a replacement for uniformly distributed random numbers and usually perform better in many applications such as numerical integration. This study also considers other low-discrepancy sequences, including the Halton sequence [32] and the Hammersley sequence [33]. We list the six uniform sampling methods as follows, and the examples of 400 points generated in [0,1]^2 using different methods are shown in Fig. 1. Equispaced uniform grid (Grid): The residual points are chosen as the nodes of an equispaced uniform grid of the computational domain. Uniformly random sampling (Random): The residual points are randomly sampled according to a continuous uniform distribution over the domain. In practice, this is usually done using pseudo-random number generators such as the PCG-64 algorithm [45]. Latin hypercube sampling LHS : The LHS is a stratified Monte Carlo sampling method that generates random samples that occur within intervals on the basis of equal probability and with normal distribution for each range. Quasi-random low-discrepancy sequences: (a)Halton sequence Halton : The Halton samples are generated according to the reversing or flipping the base conversion of numbers using primes. (b)Hammersley sequence Hammersley : The Hammersley sequence is the same as the Halton sequence, except in the first dimension where points are located equidistant from each other. (c) Sobol sequence Sobol : The Sobol sequence is a base-2 digital sequence that fills in a highly uniform manner. Figure 1: Examples of 400 points generated in[0,1]^2 using different uniform sampling methods in Section 2.2.1. 2.2.2 Uniform points with resampling In PINNs, a point at any location can be used to evaluate the PDE loss. Instead of using the fixed residual points during training, we could also select a new set of residual points in every certain optimization iteration [3]. The specific method to sample the points each time can be chosen from those methods discussed in Section 2.2.1. We can even use different sampling methods at different times, so many possible implementations make it impossible to be completely covered in this study. In this study, we only consider Random sampling with resampling (Random-R). The RandomR method is the same as the Random method, except that the residual points are resampled for everyNiteration. Theresampling periodNis also an important hyperparameter for accuracy, as we demonstrate in our empirical experiments in Section 3.","title":"2.2 Uniformly-distributed non-adaptive sampling"},{"location":"Models/PINNs/PN-2207.10289/#23-nonuniform-adaptive-sampling","text":"Although the uniform sampling strategies were predominantly employed, recent studies on the nonuniform adaptive sampling strategies [3, 37] have demonstrated promising improvement in the distribution of residual points during the training processes and achieved better accuracy. 2.3.1 Residual-based adaptive refinement with greed (RAR-G) The first adaptive sampling method for PINNs is the residual-based adaptive refinement method (RAR) proposed in Ref. [3]. RAR aims to improve the distribution of residual points during the training process by sampling more points in the locations where the PDE residual is large. Specifically, after every certain iteration, RAR adds new points in the locations with large PDE residuals (Algorithm 1). RAR only focuses on the points with large residual, and thus it is a greedy algorithm. To better distinguish from the other sampling methods, the RAR method is referred to as RAR-G in this study. Algorithm 1: RAR-G [3]. 1 Sample the initial residual pointsT using one of the methods in Section 2.2.1; 2 Train the PINN for a certain number of iterations; 3 repeat 4 Sample a set of dense pointsS 0 using one of the methods in Section 2.2.1; 5 Compute the PDE residuals for the points inS 0 ; 6 S \u2190mpoints with the largest residuals inS 0 ; 7 T \u2190T \u222aS; 8 Train the PINN for a certain number of iterations; 9 untilthe total number of iterations or the total number of residual points reaches the limit; 2.3.2 Residual-based adaptive distribution (RAD) RAR-G significantly improves the performance of PINNs when solving certain PDEs of solutions with steep gradients [3, 12]. Nevertheless, RAR-G focuses mainly on the location where the PDE residual is largest and disregards the locations of smaller residuals. Another sampling strategy was developed later in Ref. [37], where all the residual points are resampled according to a probability density function (PDF)p(x) proportional to the PDE residual. Specifically, for any pointx, we first compute the PDE residual\u03b5(x) =|f(x; \u02c6u(x))|, and then compute a probability as p(x)\u221d\u03b5(x), i.e., p(x) = \u03b5(x) A","title":"2.3 Nonuniform adaptive sampling"},{"location":"Models/PINNs/PN-2207.10289/#_15","text":"whereA=","title":""},{"location":"Models/PINNs/PN-2207.10289/#_16","text":"\u2126\u03b5(x)dxis a normalizing constant. Then all the residual points are sampled according top(x). This approach works for certain PDEs, but as we show in our numerical examples, it does not work well in some cases. Following this idea, we propose an improved version called the residualbased adaptive distribution (RAD) method (Algorithm 2), where we use a new PDF defined as p(x)\u221d \u03b5k(x) E[\u03b5k(x)] +c, (2) wherek\u22650 andc\u22650 are two hyperparameters. E[\u03b5k(x)] can be approximated by a numerical integration such as Monte Carlo integration. We note that the Random-R method in Section 2.2.2 is a special case of RAD by choosingk= 0 orc\u2192\u221e. Algorithm 2: RAD. 1 Sample the initial residual pointsT using one of the methods in Section 2.2.1; 2 Train the PINN for a certain number of iterations; 3 repeat 4 T \u2190A new set of points randomly sampled according to the PDF of Eq. (2); 5 Train the PINN for a certain number of iterations; 6 untilthe total number of iterations reaches the limit; In RAD (Algorithm 2 line 4), we need to sample a set of points according top(x), which can be done in a few ways. Whenxis low-dimensional, we can sample the points approximately in the following brute-force way: Sample a set of dense pointsS 0 using one of the methods in Section 2.2.1; Computep(x) for the points inS 0 ; Define a probability mass function \u0303p(x) =p(Ax)with the normalizing constantA=","title":"\u222b"},{"location":"Models/PINNs/PN-2207.10289/#_17","text":"x\u2208S 0 p(x); Sample a subset of points fromS 0 according to \u0303p(x). This method is simple, easy to implement, and sufficient for many PDE problems. For more complicated cases, we can use other methods such as inverse transform sampling, Markov chain Monte Carlo (MCMC) methods, and generative adversarial networks (GANs) [46]. The two hyperparameterskandcin Eq. (2) control the profile ofp(x) and thus the distribution of sampled points. We illustrate the effect ofkandcusing a simple 2D example, \u03b5(x,y) = 2^4 axa(1\u2212x)aya(1\u2212y)a, (3) witha= 10 in Fig. 2. Whenk= 0, it becomes a uniform distribution. As the value ofkincreases, more residual points will large PDE residuals are sampled. As the value ofcincreases, the residual points exhibit an inclination to be uniformly distributed. Compared with RAR, RAD provides more freedom to balance the points in the locations with large and small residuals by tuningkand c. The optimal values ofkandcare problem-dependent, and based on our numerical results, the combination ofk= 1 andc= 1 is usually a good default choice. 2.3.3 Residual-based adaptive refinement with distribution (RAR-D) We also propose a hybrid method of RAR-G and RAD, namely, residual-based adaptive refinement with distribution (RAR-D) (Algorithm 3). Similar to RAR-G, RAR-D repeatedly adds new points to the training dataset; similar to RAD, the new points are sampled based on the PDF in Eq. (2). We note that whenk\u2192 \u221e, only points with the largest PDE residual are added, which recovers RAR-G. The optimal values ofkandcare problem dependent, and based on our numerical results, the combination ofk= 2 andc= 0 is usually a good default choice. Figure 2:Examples of 1000 residual points sampled by RAD with different values ofk andcfor the PDE residual\u03b5(x,y)in Eq.(3). Algorithm 3: RAR-D. 1 Sample the initial residual pointsT using one of the methods in Section 2.2.1; 2 Train the PINN for a certain number of iterations; 3 repeat 4 S \u2190mpoints randomly sampled according to the PDF of Eq. (2); 5 T \u2190T \u222aS; 6 Train the PINN for a certain number of iterations; 7 untilthe total number of iterations or the total number of residual points reaches the limit;","title":"\u2211"},{"location":"Models/PINNs/PN-2207.10289/#24-comparison-with-related-work","text":"As discussed in Section 2.3, our proposed RAD and RAR-D are improved versions of the methods in Refs. [3, 37]. Here, we summarize the similarities between their methods and ours. Lu et al. [3] (in July 2019) proposed RAR (renamed to RAR-G here), which is a special case of RAR-D by choosing a large value ofk. The method proposed by Nabian et al. [37] (in April 2021) is a special case of RAD by choosingk= 1 andc= 0. During the preparation of this paper, a few new papers appeared [38, 39, 40, 41, 42, 43, 44] that also proposed similar methods. Here, we summarize the similarities and differences between these studies. The method proposed by Gao et al. [40] (in December 2021) is a special case of RAD by choosingc= 0. Tang et al. [41] (in December 2021) proposed two methods. One is a special case of RAD by choosingk= 2 andc= 0, and the other is a special case of RAR-D by choosingk= 2 and c= 0. Zeng et al. [43] (in April 2022) proposed a subdomain version of RAR-G. The entire domain is divided into many subdomains, and then new points are added to the several subdomains with large average PDE residual. Similar to RAR-G, Peng et al. [42] (in May 2022) proposed to add more points with large PDE residual, but they used the node generation technology proposed in Ref. [47]. We note that this method only works for a two-dimensional space. Zapf et al. [38] (in May 2022) proposed a modified version of RAR-G, where some points with small PDE residual are removed while adding points with large PDE residual. They show that compared with RAR, this reduces the computational cost, but the accuracy keeps similar. Hanna et al. [44] (in May 2022) proposed a similar method as RAR-D, but they chosep(x)\u221d max{log(\u03b5(x)/\u03b5 0 ), 0 }, where\u03b5 0 is a small tolerance. Similar to the work of Zapf et al., Daw et al. [39] (in July 2022) also proposed to remove the points with small PDE residual, but instead of adding new points with large PDE residual, they added new uniformly random sampled points. Thus all these methods are special cases of our proposed RAD and RAR-D (or with minor modification). However, in our study, two tunable variableskandcare introduced. As we show in our results, the values ofkandccould be crucial since they significantly influence the residual points distribution. By choosing proper values ofkandc, our methods would outperform the other methods. We also note that the point-wise weighting [16, 17, 8, 18] can be viewed as a special case of adaptive sampling, described as follows. When the residual points are randomly sampled from a uniform distributionU(\u2126), and the number of residual points is large, the PDE loss in Eq. (1) can be approximated byEU[\u03b5^2 (x)]. If we consider a point-wise weighting functionw(x), then the loss becomesEU[w(x)\u03b5^2 (x)], while for RAD the loss isEp[\u03b5^2 (x)]. If we choosew(x) (divided by a normalizing constant) as the PDFp(x), then the two losses are equal.","title":"2.4 Comparison with related work"},{"location":"Models/PINNs/PN-2207.10289/#3-results","text":"We apply PINNs with all the ten sampling methods in Section 2 to solve six forward and inverse PDE problems. In all examples, the hyperbolic tangent (tanh) is selected as the activation function. Table 1 summarizes the network width, depth, and optimizers used for each example. More details of the hyperparameters and training procedure can be found in each section of the specific problem. Table 1:The hyperparameters used for each numerical experiment.The learning rate of Adam optimizer is chosen as 0.001. Problems Depth Width Optimizer Section 3.2 Diffusion equation 4 32 Adam Section 3.3 Burgers\u2019 equation 4 64 Adam + L-BFGS Section 3.4 Allen-Cahn equation 4 64 Adam + L-BFGS Section 3.5 Wave equation 6 100 Adam + L-BFGS Section 3.6 Diffusion-reaction equation (inverse) 4 20 Adam Section 3.7 Korteweg-de Vries equation (inverse) 4 100 Adam For both forward and inverse problems, to evaluate the accuracy of the solution \u02c6u, theL^2 relative error is used: \u2016u\u02c6\u2212u\u2016 2 \u2016u\u2016 2","title":"3 Results"},{"location":"Models/PINNs/PN-2207.10289/#_18","text":"For inverse problems, to evaluate the accuracy of the predicted coefficients\u03bb\u02c6, the relative error is also computed: |\u03bb\u02c6\u2212\u03bb| |\u03bb|","title":""},{"location":"Models/PINNs/PN-2207.10289/#_19","text":"As the result of PINN has randomness due to the random sampling, network initialization, and optimization, thus, for each case, we run the same experiment at least 10 times and then compute the geometric mean and standard deviation of the errors. The code in this study is implemented by using the library DeepXDE [3] and is publicly available from the GitHub repositoryhttps: //github.com/lu-group/pinn-sampling.","title":""},{"location":"Models/PINNs/PN-2207.10289/#31-summary","text":"Here, we first present a summary of the accuracy of all the methods for the forward and inverse problems listed in Tables 2 and Table 3, respectively. A relatively small number of residual points is chosen to show the difference among different methods. In the specific section of each problem (Sections 3.2\u20133.7), we discuss all the detailed analyses, including the convergence of error during the training process, the convergence of error with respect to the number of residual points, and the effects of different hyperparameters (e.g., the period of resampling in Random-R, the values of kandcin RAD and RAR-D, and the number of new points added each time in RAR-D). We note that Random-R is a special case of RAD by choosingk= 0 orc\u2192 \u221e, and RAR-G is a special case of RAR-D by choosingk\u2192\u221e. Our main findings from the results are as follows. The proposed RAD method has always performed the best among the 10 sampling methods when solving all forward and inverse problems. For PDEs with complicated solutions, such as the Burgers\u2019 and multi-scale wave equation, the proposed RAD and RAR-D methods are predominately effective and yield errors magnitudes lower. For PDEs with smooth solutions, such as the diffusion equation and diffusion-reaction equa- tion, some uniform sampling methods, such as the Hammersley and Random-R, also produce sufficiently low errors. Compared with other uniform sampling methods, Random-R usually demonstrates better performance. Among the six uniform sampling methods with fixed residual points, the low-discrepancy sequences (Halton, Hammersley, and Sobol) generally perform better than Random and LHS, and both are better than Grid. Table 2: L^2 relative error of the PINN solution for the forward problems. Bold font indicates the smallest three errors for each problem. Underlined text indicates the smallest error for each problem. Diffusion Burgers\u2019 Allen-Cahn Wave No. of residual points 30 2000 1000 2000 Grid 0.66\u00b10.06% 13.7\u00b12.37% 93.4\u00b16.98% 81.3\u00b113.7% Random 0.74\u00b10.17% 13.3\u00b18.35% 22.2\u00b116.9% 68.4\u00b120.1% LHS 0.48\u00b10.24% 13.5\u00b19.05% 26.6\u00b115.8% 75.9\u00b133.1% Halton 0.24\u00b10.17% 4.51\u00b13.93% 0.29\u00b10.14% 60.2\u00b110.0% Hammersley 0.17\u00b10.07% 3.02\u00b12.98% 0.14\u00b10.14% 58.9\u00b18.52% Sobol 0.19\u00b10.07% 3.38\u00b13.21% 0.35\u00b10.24% 57.5\u00b114.7% Random-R 0.12\u00b10.06% 1.69\u00b11.67% 0.55\u00b10.34% 0.72\u00b10.90% RAR-G [3] 0.20\u00b10.07% 0.12\u00b10.04% 0.53\u00b10.19% 0.81\u00b10.11% RAD 0.11\u00b10.07% 0.02\u00b10.00% 0.08\u00b10.06% 0.09\u00b10.04% RAR-D 0.14\u00b10.11% 0.03\u00b10.01% 0.09\u00b10.03% 0.29\u00b10.04%","title":"3.1 Summary"},{"location":"Models/PINNs/PN-2207.10289/#32-diffusion-equation","text":"We first consider the following one-dimensional diffusion equation: \u2202u \u2202t","title":"3.2 Diffusion equation"},{"location":"Models/PINNs/PN-2207.10289/#_20","text":"\u2202^2 u \u2202x^2 +e\u2212t","title":"="},{"location":"Models/PINNs/PN-2207.10289/#_21","text":"\u2212sin(\u03c0x) +\u03c0^2 sin(\u03c0x)","title":"("},{"location":"Models/PINNs/PN-2207.10289/#_22","text":", x\u2208[\u2212 1 ,1],t\u2208[0,1], u(x,0) = sin(\u03c0x), u(\u2212 1 ,t) =u(1,t) = 0, whereuis the concentration of the diffusing material. The exact solution isu(x,t) = sin(\u03c0x)e\u2212t. We first compare the performance of the six uniform sampling methods with fixed residual points (Fig. 3A). The number of residual points is ranged from 10 to 80 with an increment of 10 points each time. For each number of residual points, the maximum iteration is set to be 15 000 with Adam as the optimizer. When the number of points is large (e.g., more than 70), all these methods Figure 3:L^2 relative errors of different sampling methods for the diffusion equation in Section 3.2.(A) Six uniform sampling with fixed residual points. (B) Random-R with different periods of resampling when using 30 residual points. (CandD) The training trajectory of RAD with different values ofkandcwhen using 30 residual points. (C)k= 1. (D)c= 1. (EandF) RAR-D with different values ofkandc. Each time one new point is added. (E)k= 2. (F)c= The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. Table 3:L^2 relative error of the PINN solution and relative error of the inferred parameters for the inverse problems.Bold font indicates the smallest three errors for each problem. Underlined text indicates the smallest error for each problem. Diffusion-reaction Korteweg-de Vries u(x) k(x) u(x,t) \u03bb 1 \u03bb 2 No. of residual points 15 600 Grid 0.36\u00b10.12% 8.58\u00b12.14% 24.4\u00b111.1% 53.7\u00b130.7% 42.0\u00b122.3% Random 0.35\u00b10.17% 5.77\u00b12.05% 8.86\u00b12.80% 16.4\u00b17.33% 16.8\u00b17.40% LHS 0.36\u00b10.14% 7.00\u00b12.62% 10.9\u00b12.60% 22.0\u00b16.68% 22.6\u00b16.36% Halton 0.23\u00b10.08% 6.16\u00b11.08% 8.76\u00b13.33% 16.7\u00b16.16% 17.2\u00b16.20% Hammersley 0.28\u00b10.08% 6.37\u00b10.91% 4.49\u00b13.56% 5.24\u00b17.08% 5.71\u00b17.32% Sobol 0.21\u00b10.06% 3.09\u00b10.75% 8.59\u00b13.67% 15.8\u00b16.15% 15.6\u00b15.79% Random-R 0.19\u00b10.09% 3.43\u00b11.80% 0.97\u00b10.15% 0.41\u00b10.30% 1.14\u00b10.31% RAR-G [3] 1.12\u00b10.11% 15.9\u00b11.53% 8.83\u00b11.98% 15.4\u00b19.29% 14.5\u00b19.25% RAD 0.17\u00b10.09% 2.76\u00b11.32% 0.77\u00b10.11% 0.31\u00b10.19% 0.86\u00b10.25% RAR-D 0.76\u00b10.24% 10.3\u00b13.28% 2.36\u00b10.98% 3.49\u00b12.21% 3.18\u00b12.02% have similar performance. However, when the number of residual points is small such as 50, the Hammersley and Sobol sequences perform better than others, and the equispaced uniform grid and random sampling have the largest errors (about one order of magnitude larger than Hammersley and Sobol). We then test the Random-R method using 30 residual points (Fig. 3B). The accuracy of Random-R has a strong dependence on the period of resampling, and the optimal period of resampling in this problem is around 200. Compared with Random without resampling, the Random-R method always leads to lowerL^2 relative errors regardless of the period of resampling. The error can be lower by one order of magnitude by choosing a proper resampling period. Among all the non-adaptive methods, Random-R performs the best. Next, we test the performance of the nonuniform adaptive sampling methods. In Algorithms 2 and 3, the neural network is first trained using 10 000 steps of Adam. In the RAD method, we use 30 residual points and resample every 1000 iterations. The errors of RAD with different values of kandcare shown in Figs. 3C and D. We note that Random-R is a special case of RAD with either c\u2192 \u221eork= 0. Here, RAD with large values ofcor small values ofkleads to better accuracy, i.e., the points are almost uniformly distributed. For the RAR-D method (Figs. 3E and F), one residual point is added after every 1000 iterations starting from 10 points. When usingk= 2 and c= 0 (the two red lines in Figs. 3E red F), RAR-D performs the best. When using 30 residual points, the errors of all the methods are listed in Table 2. In this diffusion equation, all the methods achieve a good accuracy (<1%). Compared with Random-R (0.12%), RAD and RAR-D (0.11%) are not significantly better. The reason could be that the solution of this diffusion equation is very smooth, so uniformly distributed points are good enough. In our following examples, we show that RAD and RAR-D work significantly better and achieve an error of orders of magnitude smaller than the non-adaptive methods.","title":")"},{"location":"Models/PINNs/PN-2207.10289/#33-burgers-equation","text":"The Burgers\u2019 equation is considered defined as: \u2202u \u2202t +u \u2202u \u2202x =\u03bd \u2202^2 u \u2202x^2 , x\u2208[\u2212 1 ,1],t\u2208[0,1], u(x,0) =\u2212sin(\u03c0x), u(\u2212 1 ,t) =u(1,t) = 0, whereuis the flow velocity and\u03bd is the viscosity of the fluid. In this study,\u03bdis set at 0. 01 /\u03c0. Different from the diffusion equation with a smooth solution, the solution of the Burgers\u2019 equation has a sharp front whenx= 0 andtis close to 1. We first test the uniform sampling methods by using the number of residual points ranging from 1,000 to 10,000 (Fig. 4A). The maximum iteration is 15,000 steps with Adam as optimizer followed by 15,000 steps of L-BFGS. Fig. 4A shows that the Hammersley method converges the fastest and reaches the lowestL^2 relative error among all the uniform sampling methods, while the Halton and Sobol sequences also perform adequately. Fig. 4B shows theL^2 relative error as a function of the period of resampling using the RandomR method with 2,000 residual points. Similar to the diffusion equation, the Random-R method always outperforms the Random method. However, the performance of Random-R is not sensitive to the period of resampling if the period is smaller than 100. Choosing a period of resampling too large can negatively affect its performance. When applying the nonuniform adaptive methods, the neural network is first trained using 15,000 steps of Adam and then 1,000 steps of L-BFGS. In the RAD method, we use 2000 residual points, which are resampled every 2,000 iterations (1,000 iterations using Adam followed by 1,000 iterations using L-BFGS). As indicated by Fig. 4C, the RAD method possesses significantly greater advantages over the Random-R method (a special case of RAD by choosingk= 0 orc\u2192 \u221e), whoseL^2 relative errors barely decrease during the training processes. This fact reflects that both extreme cases show worse performance. In contrast, fork= 1 andc= 1 (the red lines in Figs. 4C and D), theL^2 relative error declines rapidly and quickly reaches\u223c 2 \u00d7 10 \u2212^4. The RAD method is also effective when choosing a set ofkandcin a moderate range. For the RAR-D method, 1,000 residual points are selected in the pre-trained process, and 10 residual points are added every 2,000 iterations (1,000 iterations using Adam and 1,000 iterations using L-BFGS as optimizer) until the total number of residual points reaches 2,000. Shown by Figs. 4E and F, the optimal values forkandcare found to be 2 and 0, respectively. Since the solution of Burgers\u2019 equation has a very steep region, when using 2000 residual points, both RAD and RAR-D have competitive advantages over the uniform sampling methods in terms of accuracy and efficiency. For the following three forward PDE problems (Allen-Cahn equation in Section 3.4, wave equation in Section 3.5, and diffusion-reaction equation in Section 3.6), unless otherwise stated, the maximum iterations, the use of optimizer, and the training processes remain the same as the Burgers\u2019 equation. Table 2 summarizes theL^2 relative error for all methods when we fix the number of residual points at 2000. All uniform sampling methods fail to capture the solution well. TheL^2 relative errors given by the Halton, Hammersley, and Sobol methods (\u223c4%) are around one-fourth of that given by the Grid, Random, and LHS methods (>13%). Even though the Random-R performs the best among all uniform methods (1.69\u00b11.67%), the proposed RAD and RAR-D methods can achieve anL^2 relative error two orders of magnitude lower than that (0.02%). Figure 4:L^2 relative errors of different sampling methods for the Burgers\u2019 equation in Section 3.3.(A) Six uniform sampling with fixed residual points. (B) Random-R with different periods of resampling when using 2000 residual points. (CandD) The training trajectory of RAD with different values ofkandcwhen using 2000 residual points. (C)k= 1. (D)c= 1. (EandF) RAR-D with different values ofkandc. Each time 10 new points are added. (E)k= 2. (F)c= The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted.","title":"3.3 Burgers\u2019 equation"},{"location":"Models/PINNs/PN-2207.10289/#34-allen-cahn-equation","text":"Next, we consider the Allen-Cahn equation in the following form: \u2202u \u2202t","title":"3.4 Allen-Cahn equation"},{"location":"Models/PINNs/PN-2207.10289/#d","text":"\u2202^2 u \u2202x^2 5(u\u2212u^3 ), x\u2208[\u2212 1 ,1],t\u2208[0,1], u(x,0) =x^2 cos(\u03c0x), u(\u2212 1 ,t) =u(1,t) =\u2212 1 , where the diffusion coefficientD= 0.001. Fig. 5 outlines theL^2 relative errors of different sampling methods for the Allen-Cahn equation. Similar patterns are found for the nonadaptive uniform sampling as in the previous examples. The Hammersley method has the best accuracy (Fig. 5A). As the number of residual points becomes significantly large, the difference between these uniform sampling methods becomes negligible. Except for the equispaced uniform grid method, other uniform sampling methods converge toL^2 relative errors of 10\u2212^3 , about the same magnitude as the number of residual points reaching 10^4. Fig. 5B shows that when using 1000 residual points for Random-R, lowerL^2 relative errors can be obtained if we select a period of resampling less than 500. We next test the performance of RAD for different values ofkandcwhen using a different number of residual points. In Figs. 5C and D, we resampled 500 residual points every 2000 iteration, while in Figs. 5E and F, we used 1000 residual points instead. For both cases, the combination of k= 1 andc= 1 (the red lines in Figs. 5C\u2013F) gives good accuracy. When fewer residual points (e.g., 500) are used, the RAD methods boost the performance of PINNs. Similarly, we also test RAR-D in Figs. 5G\u2013J. In Figs. 5G and H, we pre-train the neural network with 500 residual points and add 10 residual points after every 2000 iterations until the total number of residual points reaches 1000. In Figs. 5I and J, we pre-train the neural network using 1000 residual points and heading to 2000 residual points in the same fashion. We recognize that 2 and 0 are the bestkandcvalues for the RAR-D method for both scenarios, which outperform the RAR-G method. As proven in this example, when applying the RAD and the RAR-D methods, the optimal values ofkandcremain stable even though we choose a different number of residual points. In addition, we find that the optimalkandcfor the Burgers\u2019 and Allen Cahn equations are the same for both the RAD and the RAR-D methods. Thus, we could choose (k= 1,c= 1) for the RAD methods and (k= 2,c= 0) for the RAR-D methods by default when first applied these methods to a new PDE problem. To make a comparison across all sampling methods, Table 2 shows theL^2 relative error for the Allen-Cahn equation when we fix the number of residual points at 1000. The Grid, Random, and LHS methods are prone to substantial errors, which are all larger than 20%. Nevertheless, the other four uniform methods (Halton, Hammersley, Sobol, and Random-R) have greater performance and can achieveL^2 relative errors of less than 1%. Remarkably, the RAD and RAR-D methods we proposed can further bring down theL^2 relative error below 0.1%. Figure 5:L^2 relative errors of different sampling methods for the Allen-Cahn equation in Section 3.4.(A) Six uniform sampling with fixed residual points. (B) Random-R with different periods of resampling when using 1000 residual points. (C\u2013F) The training trajectory of RAD with different values ofkandc. (C and D) 500 residual points are used. (C)k= 1. (D)c= 1. (E and F) 1000 residual points are used. (E)k= 1. (F)c= 1. (G\u2013J) RAR-D with different values ofk andc. (G and H) The number of residual points is increased from 500 to 1000. Each time 10 new points are added. (G)k= 2. (H)c= 0. (I and J) The number of residual points is increased from 1000 to 2000. Each time 10 new points are added. (I)k= 2. (J)c= 0. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. 18","title":"=D"},{"location":"Models/PINNs/PN-2207.10289/#35-wave-equation","text":"In this example, the following one-dimensional wave equation is considered: \u2202^2 u \u2202t^2","title":"3.5 Wave equation"},{"location":"Models/PINNs/PN-2207.10289/#4","text":"\u2202^2 u \u2202x^2 = 0, x\u2208[0,1],t\u2208[0,1], u(0,t) =u(1,t) = 0, t\u2208[0,1], u(x,0) = sin(\u03c0x) +","title":"\u2212 4"},{"location":"Models/PINNs/PN-2207.10289/#1_3","text":"","title":"1"},{"location":"Models/PINNs/PN-2207.10289/#2","text":"sin(4\u03c0x), x\u2208[0,1], \u2202u \u2202t (x,0) = 0, x\u2208[0,1], where the exact solution is given as: u(x,t) = sin(\u03c0x) cos(2\u03c0t) +","title":"2"},{"location":"Models/PINNs/PN-2207.10289/#1_4","text":"","title":"1"},{"location":"Models/PINNs/PN-2207.10289/#2_1","text":"sin(4\u03c0x) cos(8\u03c0t). The solution has a multi-scale behavior in both spatial and temporal directions. When we test the six uniform sampling methods, the number of residual points are ranged from 1000 to 6000, with an increment of 1000 each time. The Hammersley method achieves the lowest L^2 relative error with the fastest rate (Fig. 6A). When the number of residual points approaches 6000, the Random, Halton, and Hammersley methods can all obtain anL^2 relative error\u223c 10 \u2212^3. To determine the effectiveness of Random-R when using different numbers of residual points, we test the following three scenarios: small (1000 points), medium (4000 points), and large (10.000) sets of residual points (Figs. 6B, C, and D). In the medium case (Fig. 6C), the Random-R attainsL^2 relative errors magnitudes lower than the Random method. However, in the small and large cases (Figs. 6B and D), the Random-R methods show no advantage over the Random method regardless of the period of resampling. This is because when the number of residual points is small, both the Random and Random-R methods fail to provide accurate predictions. On the other hand, if the number of residual points is large, the predictions by the Random method are already highly accurate, so the Random-R is unable to further improve the accuracy. Since the optimal sets ofkandcfor both RAD and RAR-D methods are found to be the same for the Burgers\u2019 and the Allen Cahn equations, in this numerical experiment, we only apply the default settings (i.e., RAD:k= 1 andc= 1; RAR-D:k= 2 andc= 0) to investigate the effect of other factors, including the number of residual points for the RAD method and the number of points added to the RAR-D method. In Fig. 6E, we compare the performance of three nonuniform adaptive sampling methods under the same number of residual points from 1000 to 10 000. We first train the network using 15 000 iterations of Adam and 1000 iterations of L-BFGS, and then after each resampling in RAD or adding new points in RAR-D/RAR-G, we train the network with 1000 iterations of L-BFGS. For the RAR-G and the RAR-D methods, we first train the network with 50% of the final number of the residual points and add 10 residual points each time until reaching the total number of residual points. As we can see from Fig. 6E, the RAD achieves much better results when the number of residual points is small. As the number of residual points increases, the RAR-D method acts more effectively and eventually reaches comparable accuracy to the RAD method. Since the RAD method is more computationally costly than the RAR-D methods with the same number of residual points, we suggest applying the RAD method when the number of residual points is small and the RAR-D method when the number of residual points is large. We next investigate the RAD method with a different number of residual points (i.e., 1000, 2000, 5000, and 10 000). Fig. 6F illustrates that if we increase the number of residual points, lower Figure 6: L^2 relative errors of different sampling methods for the wave equation in Section 3.5.(A) Six uniform sampling with fixed residual points. (B,C, andD) Random-R with different periods of resampling when using (B) 1000 residual points, (C) 4000 residual points, and (D) 10000 residual points. (E) Comparison among RAD (k= 1 andc= 1), RAR-D (k= 2 and c= 0), and RAR-G for different numbers of residual points. (F) The training trajectory of RAD (k= 1 andc= 1) uses different numbers of residual points. (GandH) Convergence of RAR-D (k= 2 andc= 0) when adding a different number of new points each time. (G) New points are added starting from 1000 residual points. (H) New points are added starting from 2500 residual points. (I) Convergence of RAR-G when adding a different number of new points each time. New points are added starting from 2500 residual points. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. L^2 relative error can be achieved but with diminishing marginal effect. We train the network for more than 500 000 iterations to see if theL^2 relative error can further decrease. However, theL^2 relative errors converge and remain relatively stable after 100 000 iterations. One important factor to consider in the RAR-D and the RAR-G methods is how new points are added. We can either add a small number of residual points each time and prolong the training process or add a large number of residual points each time and shorten the process. In Fig. 6G, we first train the network with 1000 residual points and then add new residual points at different rates until the total number of residual points reaches 2000. After adding new residual points each time, we train the network using 1000 steps of L-BFGS. Likewise, in Fig. 6H, we first train the network with 2500 residual points and add new points at different rates until the total number of residual points reaches 5000. In both cases (Figs. 6G and H) that use the RAR-D methods, we find that the best strategy is to add 10 points each time. However, shown by two red-shaded regions in Figs. 6G and H, the results are more stable when we use a larger number of residual points. Fig. 6I is set up the same way as Fig. 6H but tests the RAR-G method. The best strategy for the RAR-G is identical to that of the RAR-D. Table 2 outlines theL^2 relative error for the wave equation using all methods when the number of residual points equals 2000. All uniform methods with fixed residual points perform poorly (error >50%) and fail to approximate the truth values. Random-R, as a special case of the proposed RAD, givesL^2 relative errors of around 1%. The RAR-D method significantly enhances the prediction accuracy resulting inL^2 relative errors under 0.3%. In addition, the RAD with the default setting ofkandcconverges toL^2 relative errors under 0.1%.","title":"2"},{"location":"Models/PINNs/PN-2207.10289/#36-diffusion-reaction-equation","text":"The first inverse problem we consider is the diffusion-reaction system as follows: \u03bb d^2 u dx^2 \u2212k(x)u=f, x\u2208[0,1], wheref = sin(2\u03c0x) is the source term. \u03bb= 0.01 is the diffusion coefficient, anduis the solute concentration. In this problem, we aim to infer the space-dependent reaction ratek(x) with given measurements on the solutionu. The exact unknown reaction rate is k(x) = 0.1 +e\u2212^0.^5 (x\u2212 0 .5)^2 (^152). We aim to learn the unknown functionk(x) and solve foru(x) by using eight observations ofu, which are uniformly distributed on the domainx\u2208[0,1], including two points on both sides of the boundaries. TheL^2 relative errors for both the solutionu(Figs. 7A, C, and E) and the unknown functionk(Figs. 7B, D, and F) are computed. The maximum number of iterations is 50 000 steps of Adam. Figs. 7A and B summarize the performance of all uniform sampling methods. We note that in 1D, the Hammersley and Halton sequences are identical and outperform other uniform methods. We fix the residual points at 15 and compare the Random method with the Random-R method. TheL^2 relative errors (Figs. 7C and D) given by the Random-R remain steady, disregarding the changes in the period of resampling, and are approximately the same as that produced by the Random method. This is because the reaction-diffusion system is fairly simple and can be easily handled by uniform sampling methods without resampling. Next, we compare the Random, RAD, RAR-G, and RAR-D methods with default settings (i.e., RAD:k= 1 andc= 1; RAR-D:k= 2 andc= 0) using a different number of residual points. For the random and RAD methods, the maximum number of iterations is 50 000 steps of Adam. For Figure 7:L^2 relative errors of different sampling methods foruandkin the diffusionreaction equation in Section 3.6.(AandB) Six uniform sampling with fixed residual points. (CandD) Random-R with different periods of resampling when using 15 residual points. (Eand F) Comparison among Random, RAD (k= 1 andc= 1), RAR-G, and RAR-D (k= 2 andc= 0) for different numbers of residual points. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. the RAR-G/RAR-D, we first train the neural network with 50% of the total number of residual points for 10 000 steps of Adam; then we add one point each time and train for 1000 steps of Adam until we meet the total number of residual points. As shown by Figs. 7E and F, the RAD method surpasses other methods and is able to produce lowL^2 relative error even when the number of residual points is very small. However, RAR-G and RAR-D are even worse than the Random sampling. To sum up, we fix the number of residual points at 15 and present theL^2 relative error for both the solution and unknown function in Table 3. The RAD yields the minimumL^2 relative error (0.17% foru(x); 2.76% fork(x)). However, due to the simplicity of this PDE problem, some uniform sampling methods, especially the Sobol and Random-R, have comparable performance to the RAD. Generally speaking, we recognize that the uniform sampling methods are adequate when solving this inverse PDE with smooth solutions. Still, the RAD method can further enhance the performance of PINNs, especially when the number of residual points is small.","title":"3.6 Diffusion-reaction equation"},{"location":"Models/PINNs/PN-2207.10289/#37-korteweg-de-vries-equation","text":"The second inverse problem we solve is the Korteweg-de Vries (KdV) equation: \u2202u \u2202t +\u03bb 1 u \u2202u \u2202x +\u03bb 2 \u2202^3 u \u2202x^3 = 0, x\u2208[\u2212 1 ,1], t\u2208[0,1], where\u03bb 1 and\u03bb 2 are two unknown parameters. The exact values for\u03bb 1 and\u03bb 2 are 1 and 0.0025, respectively. The initial condition isu(x,t= 0) = cos(\u03c0x), and periodic boundary conditions are used. To infer\u03bb 1 and\u03bb 2 , we assume that we have the observations of two solution snapshots u(x,t= 0.2) andu(x,t= 0.8) at 64 uniformly distributed points at each time. In Fig. 8, the first column (Figs. 8A, D, and G) shows theL^2 relative error of the solutionu, while the second column (Figs. 8B, E, and H) and the third column (Figs. 8C, F, and I) illustrate the relative errors for\u03bb 1 and\u03bb 2 , respectively. The maximum iteration is 100 000 steps of Adam. Hammersley achieves better accuracy than the other uniform sampling methods. The Sobol and Halton methods behave comparably as these two curves (the yellow and green curves in Figs. 8A, B, and C) are almost overlapping. Shown in Figs. 8D, E and F, the Random-R method yields higher accuracy than the Random method by about one order of magnitude in all cases when using 1000 residual points. A smaller period of resampling leads to smaller errors. Figs. 8G, H, and I compare the Random-R, Random, RAD, RAR-G, and RAR-D methods using the same number of residual points and the total number of iterations. For the Random and the Random-R methods, we train the network for 100 000 steps of Adams. For the RAD methods, we first train the network using 50 000 steps of Adams; then, we resample the residual points and train for 1000 steps of Adams 50 times. In order to fix the total number of iterations for the RARG/RAR-D methods to 100 000, we accordingly adjust the number of new residual points added each time. For example, if the final number of residual points is 500, we first train the network using 250 residual points (i.e., 50% of the total number of residual points) with 50 000 steps of Adams; and we consequently add 5 points and train for 1000 steps of Adams each time. If the final number of residual points is 1000, we first train the network using 500 residual points with 50 000 steps of Adams; and then we add 10 points and train for 1000 steps of Adams each time. As demonstrated by Figs. 8G, H, and I, the RAD method is the best, while the Random-R method is also reasonably accurate. We show one example of the training process (Figs. 8J, K, and L) when the number of residual points is 600 to illustrate the convergence of the solution,\u03bb 1 , and\u03bb 2 during training. The resampling strategies, especially the RAD method, achieve the greatest success among all sampling methods. Figure 8:L^2 relative errors ofuand relative errors of\u03bb 1 and\u03bb 2 using different sampling methods for the Korteweg-de Vries equation in Section 3.7. (A,B, andC) Six uniform sampling with fixed residual points. (D,E, andF) Random-R with different periods of resampling when using 1000 residual points. (G,H, andI) Comparison among Random, Random-R, RAD (k= 1 andc= 1), RAR-G, and RAR-D (k= 2 andc= 0) for different number of residual points. (J,K, andL) Examples of the training trajectories using Random, Random-R, RAD (k= 1 and c= 1), RAR-G, and RAR-D (k= 2 andc= 0) with 600 residual points. The curves and shaded regions represent the geometric mean and one standard deviation of 10 runs. For clarity, only some standard deviations are plotted. Table 3 demonstrates theL^2 relative errors for the solutionu(x,t) and the relative error of two unknown parameters\u03bb 1 and\u03bb 2 , for all methods when the number of residual points is set at The lowestL^2 relative errors for uniform sampling with fixed points are given by Hammersley (\u223c 5%). The Random-R is the second-best method and providesL^2 relative errors of around 1%. With the smallest errors (<1%) and standard deviations, the RAD method has compelling advantages over all other methods in terms of accuracy and robustness. It is noteworthy that the RAR-D method provides adequate accuracy (\u223c3%) and is less expensive than the Random-R and RAD methods when the number of residual points is the same. Therefore, the RAR-D is also a valuable approach to consider.","title":"3.7 Korteweg-de Vries equation"},{"location":"Models/PINNs/PN-2207.10289/#4-conclusions","text":"In this paper, we present a comprehensive study of two categories of sampling for physics-informed neural networks (PINNs), including non-adaptive uniform sampling and adaptive nonuniform sampling. For the non-adaptive uniform sampling, we have considered six methods: (1) equispaced uniform grid (Grid), (2) uniformly random sampling (Random), (3) Latin hypercube sampling (LHS), (4) Halton sequence (Halton), (5) Hammersley sequence (Hammersley), and (6) Sobol sequence (Sobol). We have also considered a resampling strategy for uniform sampling (Random-R). For the adaptive nonuniform sampling, motivated by the residual-based adaptive refinement with greed (RAR-G) [3], we proposed two new residual-based adaptive sampling methods: residual-based adaptive distribution (RAD) and residual-based adaptive refinement with distribution (RAR-D). We extensively investigated the performance of these ten sampling methods in solving four forward and two inverse problems of partial differential equations (PDEs) with many setups, such as a different number of residual points. Our results show that the proposed RAD and RAR-D significantly improve the accuracy of PINNs by orders of magnitude, especially when the number of residual points is small. RAD and RAR-D also have great advantages for the PDEs with complicated solutions, e.g., the solution of the Burgers\u2019 equation with steep gradients and the solution of the wave equation with a multi-scale behavior. A summary of the comparison of these methods can be found in Section 3.1. Based on our empirical results, we summarize the following suggestions as a practical guideline in choosing sampling methods for PINNs. RAD withk= 1 andc= 1 can be chosen as the default sampling method when solving a new PDE. The hyperparameterskandccan be tuned to balance the points in the locations with large and small PDE residuals. RAR-D can achieve comparable accuracy to RAD, but RAR-D is more computationally efficient as it gradually increases the number of residual points. Hence, RAR-D (k= 2 and c= 0 by default) is preferable for the case with limited computational resources. Random-R can be used in the situation where adaptive sampling is not allowed, e.g., it is difficult to sample residual points according to a probability density function. The period of resampling should not be chosen as too small or too large. A low-discrepancy sequence (e.g., Hammersley) should be considered rather than Grid, Ran- dom, or LHS, when we have to use a fixed set of residual points, such as in PINNs with the augmented Lagrangian method (hPINNs) [8]. In this study, we sample residual points in RAD and RAR-D by using a brute-force approach, which is simple, easy to implement, and sufficient for many PDEs. However, for high-dimensional problems, we need to use other methods, such as generative adversarial networks (GANs) [46], as was done in Ref. [41]. Moreover, the probability of sampling a pointxis only considered as p(x)\u221d \u03b5 k(x) E[\u03b5k(x)]+c. While this probability works very well in this study, it is possible that there exists another better choice. We can learn a new probability density function by meta-learning, as was done for loss functions of PINNs in Ref. [11].","title":"4 Conclusions"},{"location":"Models/PINNs/PN-2207.10289/#references","text":"[1] M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.Journal of Computational Physics, 378:686\u2013707, 2019. [2] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations.Science, 367(6481):1026\u20131030, 2020. [3] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. DeepXDE: A deep learning library for solving differential equations.SIAM Review, 63(1):208\u2013228, 2021. [4] George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning.Nature Reviews Physics, 3(6):422\u2013440, 2021. [5] Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse problems in nano-optics and metamaterials.Optics Express, 28(8):11618, 2020. [6] Alireza Yazdani, Lu Lu, Maziar Raissi, and George Em Karniadakis. Systems biology informed deep learning for inferring parameters and hidden dynamics. PLOS Computational Biology, 16(11), 2020. [7] Mitchell Daneker, Zhen Zhang, George Em Kevrekidis, and Lu Lu. Systems biology: Identifiability analysis and parameter identification via systems-biology informed neural networks. arXiv preprint arXiv:2202.01723, 2022. [8] Lu Lu, Rapha \u0308el Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G. Johnson. Physics-informed neural networks with hard constraints for inverse design. SIAM Journal on Scientific Computing, 43(6), 2021. [9] Guofei Pang, Lu Lu, and George Em Karniadakis. fPINNs: Fractional physics-informed neural networks. SIAM Journal on Scientific Computing, 41(4), 2019. [10] Dongkun Zhang, Lu Lu, Ling Guo, and George Em Karniadakis. Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems.Journal of Computational Physics, 397:108850, 2019. [11] Apostolos F Psaros, Kenji Kawaguchi, and George Em Karniadakis. Meta-learning PINN loss functions. Journal of Computational Physics, 458:111121, 2022. [12] Jeremy Yu, Lu Lu, Xuhui Meng, and George Em Karniadakis. Gradient-enhanced physicsinformed neural networks for forward and inverse PDE problems.Computer Methods in Applied Mechanics and Engineering, 393:114823, 2022. [13] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow pathologies in physics-informed neural networks. SIAM Journal on Scientific Computing, 43(5):A3055\u2013A3081, 2021. [14] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why PINNs fail to train: A neural tangent kernel perspective. Journal of Computational Physics, 449:110768, 2022. [15] Zixue Xiang, Wei Peng, Xu Liu, and Wen Yao. Self-adaptive loss balanced physics-informed neural networks.Neurocomputing, 2022. [16] Levi McClenny and Ulisses Braga-Neto. Self-adaptive physics-informed neural networks using a soft attention mechanism.arXiv preprint arXiv:2009.04544, 2020. [17] Yiqi Gu, Haizhao Yang, and Chao Zhou. SelectNet: Self-paced learning for high-dimensional partial differential equations.Journal of Computational Physics, 441:110444, 2021. [18] Wensheng Li, Chao Zhang, Chuncheng Wang, Hanting Guan, and Dacheng Tao. Revisiting PINNs: Generative adversarial physics-informed neural networks and point-weighting method. arXiv preprint arXiv:2205.08754, 2022. [19] Xuhui Meng, Zhen Li, Dongkun Zhang, and George Em Karniadakis. PPINN: Parareal physicsinformed neural network for time-dependent pdes. Computer Methods in Applied Mechanics and Engineering, 370:113250, 2020. [20] Khemraj Shukla, Ameya D. Jagtap, and George Em Karniadakis. Parallel physics-informed neural networks via domain decomposition. Journal of Computational Physics, 447:110683, 2021. [21] Ameya D. Jagtap and George Em Karniadakis. Extended physics-informed neural networks (XPINNs): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations. Communications in Computational Physics, 28(5):2002\u20132041, 2020. [22] Colby L Wight and Jia Zhao. Solving Allen-Cahn and Cahn-Hilliard equations using the adaptive physics informed neural networks. arXiv preprint arXiv:2007.04542, 2020. [23] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks.Advances in Neural Information Processing Systems, 34:26548\u201326560, 2021. [24] Revanth Mattey and Susanta Ghosh. A novel sequential method to train physics informed neural networks for allen cahn and cahn hilliard equations. Computer Methods in Applied Mechanics and Engineering, 390:114474, 2022. [25] Katsiaryna Haitsiukevich and Alexander Ilin. Improved training of physics-informed neural networks with model ensembles. arXiv preprint arXiv:2204.05108, 2022. [26] Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality is all you need for training physics-informed neural networks. arXiv preprint arXiv:2203.07404, 2022. [27] Pola Lydia Lagari, Lefteri H Tsoukalas, Salar Safarkhani, and Isaac E Lagaris. Systematic construction of neural forms for solving partial differential equations inside rectangular domains, subject to initial, boundary and interface conditions. International Journal on Artificial Intelligence Tools, 29(05):2050009, 2020. [28] Suchuan Dong and Naxian Ni. A method for representing periodic functions and enforcing exactly periodic boundary conditions with deep neural networks. Journal of Computational Physics, 435:110242, 2021. [29] Michael D McKay, Richard J Beckman, and William J Conover. A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. Technometrics, 42(1):55\u201361, 2000. [30] Michael Stein. Large sample properties of simulations using Latin hypercube sampling.Technometrics, 29(2):143\u2013151, 1987. [31] Il\u2019ya Meerovich Sobol\u2019. On the distribution of points in a cube and the approximate evaluation of integrals.Zhurnal Vychislitel\u2019noi Matematiki i Matematicheskoi Fiziki, 7(4):784\u2013802, 1967. [32] John H Halton. On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals. Numerische Mathematik, 2(1):84\u201390, 1960. [33] JM Hammersley and DC Handscomb. Monte-Carlo methods, mathuen, 1964. [34] Hongwei Guo, Xiaoying Zhuang, Xiaoyu Meng, and Timon Rabczuk. Analysis of three dimensional potential problems in non-homogeneous media with deep learning based collocation method.arXiv preprint arXiv:2010.12060, 2020. [35] Sourav Das and Solomon Tesfamariam. State-of-the-art review of design of experiments for physics-informed deep learning.arXiv preprint arXiv:2202.06416, 2022. [36] Zhiping Mao, Ameya D Jagtap, and George Em Karniadakis. Physics-informed neural networks for high-speed flows. Computer Methods in Applied Mechanics and Engineering, 360:112789, 2020. [37] Mohammad Amin Nabian, Rini Jasmine Gladstone, and Hadi Meidani. Efficient training of physics-informed neural networks via importance sampling.Computer-Aided Civil and Infrastructure Engineering, 2021. [38] Bastian Zapf, Johannes Haubner, Miroslav Kuchta, Geir Ringstad, Per Kristian Eide, and Kent-Andre Mardal. Investigating molecular transport in the human brain from MRI with physics-informed neural networks.arXiv preprint arXiv:2205.02592, 2022. [39] Arka Daw, Jie Bu, Sifan Wang, Paris Perdikaris, and Anuj Karpatne. Rethinking the importance of sampling in physics-informed neural networks. arXiv preprint arXiv:2207.02338, 2022. [40] Wenhan Gao and Chunmei Wang. Active learning based sampling for high-dimensional nonlinear partial differential equations. arXiv preprint arXiv:2112.13988, 2021. [41] Kejun Tang, Xiaoliang Wan, and Chao Yang. DAS: A deep adaptive sampling method for solving partial differential equations.arXiv preprint arXiv:2112.14038, 2021. [42] Wei Peng, Weien Zhou, Xiaoya Zhang, Wen Yao, and Zheliang Liu. RANG: a residualbased adaptive node generation method for physics-informed neural networks. arXiv preprint arXiv:2205.01051, 2022. [43] Shaojie Zeng, Zong Zhang, and Qingsong Zou. Adaptive deep neural networks methods for high-dimensional partial differential equations.Journal of Computational Physics, 463:111232, 2022. [44] John M Hanna, Jose V Aguado, Sebastien Comas-Cardona, Ramzi Askri, and Domenico Borzacchiello. Residual-based adaptivity for two-phase flow simulation in porous media using physics-informed neural networks.Computer Methods in Applied Mechanics and Engineering, 396:115100, 2022. [45] Melissa E O\u2019Neill. Pcg: A family of simple fast space-efficient statistically good algorithms for random number generation.ACM Transactions on Mathematical Software, 2014. [46] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.Advances in neural information processing systems, 27, 2014. [47] Bengt Fornberg and Natasha Flyer. Fast generation of 2-D node distributions for mesh-free pde discretizations.Computers & Mathematics with Applications, 69(7):531\u2013544, 2015.","title":"References"},{"location":"Models/PINNs/PN-2210.00279/","text":"Failure-Informed Adaptive Sampling for PINNs \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u5931\u8d25\u4fe1\u606f\u81ea\u9002\u5e94\u91c7\u6837","title":"Failure-Informed Adaptive Sampling for PINNs <br> \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u5931\u8d25\u4fe1\u606f\u81ea\u9002\u5e94\u91c7\u6837"},{"location":"Models/PINNs/PN-2210.00279/#failure-informed-adaptive-sampling-for-pinns","text":"","title":"Failure-Informed Adaptive Sampling for PINNs  \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u5931\u8d25\u4fe1\u606f\u81ea\u9002\u5e94\u91c7\u6837"},{"location":"Models/PINNs/PN-2210.12914/","text":"A Novel Adaptive Causal Sampling Method for PINNs \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u4e00\u79cd\u65b0\u81ea\u9002\u5e94\u56e0\u679c\u91c7\u6837\u65b9\u6cd5","title":"A Novel Adaptive Causal Sampling Method for PINNs <br> \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u4e00\u79cd\u65b0\u81ea\u9002\u5e94\u56e0\u679c\u91c7\u6837\u65b9\u6cd5"},{"location":"Models/PINNs/PN-2210.12914/#a-novel-adaptive-causal-sampling-method-for-pinns","text":"","title":"A Novel Adaptive Causal Sampling Method for PINNs  \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u4e00\u79cd\u65b0\u81ea\u9002\u5e94\u56e0\u679c\u91c7\u6837\u65b9\u6cd5"},{"location":"Models/PINNs/PN-JCP201810045/","text":"Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations \u4f5c\u8005: \u673a\u6784: \u65f6\u95f4: 2018-06-13 \u53d1\u8868: JCP \u9886\u57df: \u6807\u7b7e: #PINN \u5f15\u7528: \u4ee3\u7801:","title":"Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations"},{"location":"Models/PINNs/PN-JCP201810045/#physics-informed-neural-networks-a-deep-learning-framework-for-solving-forward-and-inverse-problems-involving-nonlinear-partial-differential-equations","text":"\u4f5c\u8005: \u673a\u6784: \u65f6\u95f4: 2018-06-13 \u53d1\u8868: JCP \u9886\u57df: \u6807\u7b7e: #PINN \u5f15\u7528: \u4ee3\u7801:","title":"Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations"},{"location":"Models/Transformers/PN-2005.12872/","text":"DETR","title":"DETR"},{"location":"Models/Transformers/PN-2005.12872/#detr","text":"","title":"DETR"},{"location":"Models/Transformers/Transformers/","text":"Transformers \u7cfb\u5217","title":"Transformers \u7cfb\u5217"},{"location":"Models/Transformers/Transformers/#transformers","text":"","title":"Transformers \u7cfb\u5217"},{"location":"Models/YOLOs/YOLOs/","text":"","title":"YOLOs"},{"location":"Scholars/PN-2207.13266/","text":"Sparse Deep Neural Network for Nonlinear Partial Differential Equations \u4f5c\u8005: \u8bb8\u8dc3\u751f, Zeng Taishan \u94fe\u63a5: arXiv:2207.13266v1 \u65f6\u95f4: 2022-07-27 \u6807\u7b7e: Sparse Approximation, \u6df1\u5ea6\u5b66\u4e60, \u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b, Sparse Regularization, Adaptive Approximation \u76ee\u5f55 [Toc] author{ color: red; } Abstarct More competent learning models are demanded for data processing due to increasingly greater amounts of data available in applications. Data that we encounter often have certain embedded sparsity structures. That is, if they are represented in an appropriate basis, their energies can concentrate on a small number of basis functions. This paper is devoted to a numerical study of adaptive approximation of solutions of nonlinear partial differential equations whose solutions may have singularities, by deep neural networks (DNNs) with a sparse regularization with multiple parameters. Noting that DNNs have an intrinsic multi-scale structure which is favorable for adaptive representation of functions, by employing a penalty with multiple parameters, we develop DNNs with a multi-scale sparse regularization ( SDNN ) for effectively representing functions having certain singularities. We then apply the proposed SDNN to numerical solutions of the Burgers equation and the Schr\u00f6dinger equation. Numerical examples confirm that solutions generated by the proposed SDNN are sparse and accurate. \u7531\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u7528\u7684\u6570\u636e\u8d8a\u6765\u8d8a\u591a, \u56e0\u6b64\u9700\u8981\u66f4\u6709\u80fd\u529b\u7684\u5b66\u4e60\u6a21\u578b\u6765\u8fdb\u884c\u6570\u636e\u5904\u7406. \u6211\u4eec\u9047\u5230\u7684\u6570\u636e\u901a\u5e38\u5177\u6709\u67d0\u79cd\u5d4c\u5165\u5f0f\u7a00\u758f\u7ed3\u6784. \u4e5f\u5c31\u662f\u8bf4, \u5982\u679c\u7528\u9002\u5f53\u7684\u57fa\u51fd\u6570\u8868\u793a\u5b83\u4eec, \u5b83\u4eec\u7684\u80fd\u91cf\u5c31\u53ef\u4ee5\u96c6\u4e2d\u5728\u5c11\u91cf\u7684\u57fa\u51fd\u6570\u4e0a. \u672c\u6587\u5229\u7528\u5177\u6709\u591a\u53c2\u6570\u7a00\u758f\u6b63\u5219\u5316\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc, \u5bf9\u89e3\u5177\u6709\u5947\u5f02\u6027\u7684\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u89e3\u7684\u81ea\u9002\u5e94\u903c\u8fd1\u8fdb\u884c\u4e86\u6570\u503c\u7814\u7a76. \u6ce8\u610f\u5230\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u5185\u5728\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784, \u6709\u5229\u4e8e\u51fd\u6570\u7684\u81ea\u9002\u5e94\u8868\u793a, \u901a\u8fc7\u4f7f\u7528\u591a\u53c2\u6570\u60e9\u7f5a, \u6211\u4eec\u5efa\u7acb\u4e86\u5177\u6709\u591a\u5c3a\u5ea6\u7a00\u758f\u6b63\u5219\u5316\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (SDNN) \u6765\u6709\u6548\u5730\u8868\u793a\u5177\u6709\u67d0\u4e9b\u5947\u5f02\u6027\u7684\u51fd\u6570. \u7136\u540e\u6211\u4eec\u5c06\u6240\u63d0\u51fa\u7684 SDNN \u5e94\u7528\u4e8e\u6c42\u89e3 Burgers \u65b9\u7a0b\u548c Schr\u00f6dinger \u65b9\u7a0b\u7684\u6570\u503c\u89e3. \u6570\u503c\u7b97\u4f8b\u8868\u660e, \u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u751f\u6210\u7684\u89e3\u662f\u7a00\u758f\u4e14\u51c6\u786e\u7684. Introduction The goal of this paper is to develop a sparse regularization deep neural network model for numerical solutions of nonlinear partial differential equations whose solutions may have singularities. We will mainly focus on designing a sparse regularization model by employing multiple parameters to balance sparsity of different layers and the overall accuracy. The proposed ideas are tested in this paper numerically to confirm our intuition and more in-depth theoretical studies will be followed in a future paper. \u672c\u6587\u7684\u76ee\u7684\u662f\u5efa\u7acb\u4e00\u4e2a\u7a00\u758f\u6b63\u5219\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b, \u7528\u4e8e\u6c42\u89e3\u90a3\u4e9b\u53ef\u80fd\u5177\u6709\u5947\u5f02\u6027\u7684\u89e3\u7684\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u6570\u503c\u89e3. \u6211\u4eec\u5c06\u7740\u91cd\u4e8e\u8bbe\u8ba1\u4e00\u4e2a\u7a00\u758f\u6b63\u5219\u5316\u6a21\u578b, \u91c7\u7528\u591a\u4e2a\u53c2\u6570\u6765\u5e73\u8861\u4e0d\u540c\u5c42\u7684\u7a00\u758f\u6027\u548c\u6574\u4f53\u7cbe\u5ea6. \u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u7684\u76f4\u89c9, \u672c\u6587\u5bf9\u6240\u63d0\u51fa\u7684\u89c2\u70b9\u8fdb\u884c\u4e86\u6570\u503c\u5b9e\u9a8c, \u5e76\u5c06\u5728\u4eca\u540e\u7684\u8bba\u6587\u4e2d\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u7406\u8bba\u7814\u7a76. Artificial intelligence especially deep neural networks (DNN) has received great attention in many research fields. From the approximation theory point of view, a neural network is built by functional composition to approximate a continuous function with arbitrary accuracy. Deep neural networks are proven to have better approximation by practice and theory due to their relatively large number of hidden layers. Deep neural network has achieved state-of-the-art performance in a wide range of applications, including speech recognition 11 , computer vision 28 , natural language processing 14 , and finance 8 . For an overview of deep learning the readers are referred to monograph 20 . Recently, there was great interest in applying deep neural networks to the field of scientific computing, such as discovering the differential equations from observed data 34 , solving the partial differential equation (PDE) 21 29 30 35 , and problem aroused in physics 16 . Mathematical understanding of deep neural networks received much attention in the applied mathematics community. A universal approximation theory of neural network for Borel measurable function on compact domain is established in 9 . Some recent research studies the expressivity of deep neural networks for different function spaces 15 , for example, Sobolev spaces, Barron functions, and H\u00f6lder spaces. There are close connections between deep neural network and traditional approximation methods, such as splines 13 37 , compressed sensing 1 , and finite elements 22 26 . Convergence of deep neural networks and deep convolutional neural networks are studied in 40 and 41 respectively. Some work aims at understanding the training process of DNN. For instance, in paper 10 , the training process of DNN is interpreted as learning adaptive basis from data. \u4eba\u5de5\u667a\u80fd, \u7279\u522b\u662f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5df2\u7ecf\u5728\u8bb8\u591a\u9886\u57df\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u5173\u6ce8. \u4ece\u903c\u8fd1\u7406\u8bba\u7684\u89d2\u5ea6\u6765\u770b, \u795e\u7ecf\u7f51\u7edc\u662f\u901a\u8fc7\u51fd\u6570\u7ec4\u5408\u6765\u903c\u8fd1\u4efb\u610f\u7cbe\u5ea6\u7684\u8fde\u7eed\u51fd\u6570. \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7531\u4e8e\u5176\u76f8\u5bf9\u8f83\u5927\u7684\u9690\u5c42\u6570, \u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u88ab\u8bc1\u660e\u5177\u6709\u8f83\u597d\u7684\u903c\u8fd1\u6027\u80fd. \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bed\u97f3\u8bc6\u522b, \u8ba1\u7b97\u673a\u89c6\u89c9, \u81ea\u7136\u8bed\u8a00\u5904\u7406 (BERT) \u548c\u91d1\u878d\u7b49\u5e7f\u6cdb\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd. \u5bf9\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6982\u8ff0, \u8bfb\u8005\u53ef\u4ee5\u53c2\u8003\u4e13\u8457 Deep Learning . \u6700\u8fd1, \u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u4e8e\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u5f15\u8d77\u4e86\u6781\u5927\u7684\u5174\u8da3, \u4f8b\u5982\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u53d1\u73b0\u5fae\u5206\u65b9\u7a0b, \u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b, \u4ee5\u53ca\u7269\u7406\u5b66\u4e2d\u51fa\u73b0\u7684\u76f8\u5173\u95ee\u9898. \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u5b66\u7406\u89e3\u5728\u5e94\u7528\u6570\u5b66\u754c\u5907\u53d7\u5173\u6ce8. \u6587\u732e 9 \u4e2d\u5efa\u7acb\u4e86\u4e00\u4e2a\u7528\u4e8e\u7d27\u81f4\u57df\u4e0a Borel \u53ef\u6d4b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u901a\u7528\u903c\u8fd1\u7406\u8bba. \u6700\u8fd1\u7684\u4e00\u4e9b\u5de5\u4f5c\u7814\u7a76\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u4e0d\u540c\u51fd\u6570\u7a7a\u95f4\u7684\u8868\u8fbe\u80fd\u529b, \u4f8b\u5982 Sobolev \u7a7a\u95f4, Barron \u51fd\u6570\u548c H\u00f6lder \u7a7a\u95f4. \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0e\u4f20\u7edf\u903c\u8fd1\u65b9\u6cd5\u6709\u7740\u5bc6\u5207\u7684\u8054\u7cfb, \u4f8b\u5982\u6837\u6761\u51fd\u6570, \u538b\u7f29\u611f\u77e5\u548c\u6709\u9650\u5143. \u6587\u732e 40 \u548c 41 \u4e2d\u5206\u522b\u7814\u7a76\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6536\u655b\u6027. \u4e00\u4e9b\u5de5\u4f5c\u8bd5\u56fe\u7406\u89e3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b. \u4f8b\u5982, \u6587\u732e 10 \u4e2d\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u88ab\u89e3\u91ca\u4e3a\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u81ea\u9002\u5e94\u57fa. Traditionally, deep neural networks are dense and over-parameterized. A dense network model requires more memory and other computational resources during training and inference of the model. Increasingly greater amounts of data and related model sizes demand the availability of more competent learning models. Compared to dense models, sparse deep neural networks require less memory, less computing time and have better interpretability. Hence, sparse deep neural network models are desirable. On the other hand, animal brains are found to have hierarchical and sparse structures 19 . The connectivity of an animal brain becomes sparser as the size of the brain grows larger. Therefore, it is not only necessary but also natural to design sparse networks. In fact, it was pointed out in 24 that the future of deep learning relies on sparsity. Further more, over-parameterized and dense models tend to lead to overfitting and weakening the ability to generalize over unseen examples. Sparse models can improve accuracy of approximation. Sparse regularization is a popular way to learn the sparse solutions 5 38 39 42 . The readers are referred to 25 for an overview of sparse deep learning. \u4e00\u822c\u6765\u8bf4, \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bc6\u96c6\u4e14\u8fc7\u53c2\u6570\u5316. \u5728\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d, \u5bc6\u96c6\u7f51\u7edc\u6a21\u578b\u9700\u8981\u66f4\u591a\u7684\u5185\u5b58\u548c\u5176\u4ed6\u8ba1\u7b97\u8d44\u6e90. \u8d8a\u6765\u8d8a\u591a\u7684\u6570\u636e\u548c\u76f8\u5173\u6a21\u578b\u7684\u5927\u5c0f\u8981\u6c42\u63d0\u4f9b\u66f4\u6709\u80fd\u529b\u7684\u5b66\u4e60\u6a21\u578b. \u4e0e\u5bc6\u96c6\u6a21\u578b\u76f8\u6bd4, \u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u5185\u5b58\u66f4\u5c11, \u8ba1\u7b97\u65f6\u95f4\u66f4\u77ed, \u53ef\u89e3\u91ca\u6027\u66f4\u597d\u7b49\u4f18\u70b9. \u56e0\u6b64, \u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u662f\u53ef\u53d6\u7684. \u53e6\u4e00\u65b9\u9762, \u52a8\u7269\u7684\u5927\u8111\u88ab\u53d1\u73b0\u5177\u6709\u5c42\u6b21\u548c\u7a00\u758f\u7ed3\u6784. \u52a8\u7269\u5927\u8111\u7684\u8fde\u63a5\u6027\u968f\u7740\u5927\u8111\u5c3a\u5bf8\u7684\u589e\u5927\u800c\u53d8\u5f97\u7a00\u758f. \u56e0\u6b64\u8bbe\u8ba1\u7a00\u758f\u7f51\u7edc\u4e0d\u4ec5\u662f\u5fc5\u8981\u7684, \u800c\u4e14\u662f\u81ea\u7136\u7684. \u4e8b\u5b9e\u4e0a, \u5728\u6587\u732e 24 \u4e2d\u6307\u51fa, \u6df1\u5ea6\u5b66\u4e60\u7684\u672a\u6765\u4f9d\u8d56\u4e8e\u7a00\u758f\u6027. \u6b64\u5916, \u8fc7\u53c2\u6570\u5316\u548c\u5bc6\u96c6\u6a21\u578b\u5f80\u5f80\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u524a\u5f31\u5728\u672a\u77e5\u6837\u672c\u4e0a\u6cdb\u5316\u7684\u80fd\u529b. \u7a00\u758f\u6a21\u578b\u53ef\u4ee5\u63d0\u9ad8\u903c\u8fd1\u7cbe\u5ea6. \u7a00\u758f\u6b63\u5219\u5316\u662f\u5b66\u4e60\u7a00\u758f\u89e3\u7684\u4e00\u79cd\u6d41\u884c\u65b9\u6cd5. \u8bfb\u8005\u53ef\u53c2\u8003 25 \u4ee5\u4e86\u89e3\u7a00\u758f\u6df1\u5ea6\u5b66\u4e60. Although much progress has been made in theoretical research of deep learning, it remains a challenging issue to construct an effective neural network approximation for general function spaces using as few neuron connections or neurons as possible. Most of existing network structures are specific for a particular class of functions. In this paper, we aim to propose a multi-scale sparse regularized neural network to approximate the function effectively. A neural network with multiple hidden layers can be viewed as a multi-scale transformation from simple features to complex features. The layer-by-layer composite of functions can be seen as a generalization of wavelet transforms 7 12 33 . For neurons in different layers, corresponding to different transformation scales, the corresponding features have different levels of importance. Imposing different regularization parameters for different scales was proved to be an effective way to deal with multi-scale regularization problems 3 6 32 . Inspired by multi-scale analysis, we propose a sparse regularization network model by applying different sparse regularization penalties to the neuron connections in different layers. During the training process, the neural network adaptively learns matrix weights from given data. By sparse optimization, many weight connections are automatically zero. The remaining neural networks composed of non-zero weights form the sparse deep neural network that we desire. \u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u7684\u7406\u8bba\u7814\u7a76\u5df2\u7ecf\u53d6\u5f97\u4e86\u5f88\u5927\u7684\u8fdb\u5c55, \u4f46\u662f\u5982\u4f55\u5229\u7528\u5c3d\u53ef\u80fd\u5c11\u7684\u795e\u7ecf\u5143\u8fde\u63a5\u6216\u795e\u7ecf\u5143\u6765\u6784\u9020\u4e00\u4e2a\u6709\u6548\u7684\u7528\u4e8e\u4e00\u822c\u51fd\u6570\u7a7a\u95f4\u7684\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898. \u5927\u591a\u6570\u73b0\u6709\u7684\u7f51\u7edc\u7ed3\u6784\u90fd\u662f\u7279\u5b9a\u4e8e\u67d0\u4e00\u7c7b\u51fd\u6570\u7684. \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u591a\u5c3a\u5ea6\u7a00\u758f\u6b63\u5219\u5316\u795e\u7ecf\u7f51\u7edc\u6765\u6709\u6548\u5730\u903c\u8fd1\u51fd\u6570. \u5177\u6709\u591a\u4e2a\u9690\u85cf\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u770b\u4f5c\u662f\u7531\u7b80\u5355\u7279\u5f81\u5411\u590d\u6742\u7279\u5f81\u7684\u591a\u5c3a\u5ea6\u53d8\u6362. \u51fd\u6570\u7684\u9010\u5c42\u590d\u5408\u53ef\u4ee5\u770b\u4f5c\u662f\u5c0f\u6ce2\u53d8\u6362\u7684\u4e00\u79cd\u63a8\u5e7f. \u5bf9\u4e8e\u4e0d\u540c\u5c42\u7684\u795e\u7ecf\u5143, \u5bf9\u5e94\u4e8e\u4e0d\u540c\u7684\u53d8\u6362\u5c3a\u5ea6, \u76f8\u5e94\u7684\u7279\u5f81\u5177\u6709\u4e0d\u540c\u7ea7\u522b\u7684\u91cd\u8981\u6027. \u9488\u5bf9\u4e0d\u540c\u5c3a\u5ea6\u8bbe\u7f6e\u4e0d\u540c\u7684\u6b63\u5219\u5316\u53c2\u6570\u662f\u5904\u7406\u591a\u5c3a\u5ea6\u6b63\u5219\u5316\u95ee\u9898\u7684\u4e00\u79cd\u6709\u6548\u65b9\u6cd5. \u53d7\u591a\u5c3a\u5ea6\u5206\u6790\u7684\u542f\u53d1, \u6211\u4eec\u901a\u8fc7\u5bf9\u4e0d\u540c\u5c42\u7684\u795e\u7ecf\u5143\u8fde\u63a5\u65bd\u52a0\u4e0d\u540c\u7684\u7a00\u758f\u6b63\u5219\u5316\u60e9\u7f5a, \u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u6b63\u5219\u5316\u7f51\u7edc\u6a21\u578b. \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u795e\u7ecf\u7f51\u7edc\u4ece\u7ed9\u5b9a\u7684\u6570\u636e\u4e2d\u81ea\u9002\u5e94\u5730\u5b66\u4e60\u77e9\u9635\u6743\u91cd. \u901a\u8fc7\u7a00\u758f\u4f18\u5316, \u8bb8\u591a\u6743\u91cd\u8fde\u63a5\u81ea\u52a8\u4e3a\u96f6. \u5176\u4f59\u7531\u975e\u96f6\u6743\u91cd\u7ec4\u6210\u7684\u795e\u7ecf\u7f51\u7edc\u6784\u6210\u6211\u4eec\u6240\u9700\u8981\u7684\u7a00\u758f\u6df1\u5c42\u795e\u7ecf\u7f51\u7edc. This paper is organized in five sections. In Section 2, we describe a multi-parameter regularization model for solving partial differential equations by using deep neural networks. We study in Section 3 the capacity of the proposed multi-parameter regularization in adaptive representing functions having certain singularities. In Section 4, we investigate numerical solutions of nonlinear partial differential equations by using the proposed SDNN model. Specifically, we consider two equations: the Burgers equation and the Schr\u00f6dinger equation since solutions of these two equations exhibit certain types of singularities. Finally, a conclusion is drawn in Section 5. \u672c\u6587\u5206\u4e3a\u4e94\u4e2a\u90e8\u5206. \u5728\u7b2c\u4e8c\u8282\u4e2d, \u6211\u4eec\u63cf\u8ff0\u4e86\u4e00\u4e2a\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u591a\u53c2\u6570\u6b63\u5219\u5316\u6a21\u578b. \u5728\u7b2c\u4e09\u8282\u4e2d, \u6211\u4eec\u7814\u7a76\u4e86\u6240\u63d0\u51fa\u7684\u591a\u53c2\u6570\u6b63\u5219\u5316\u6a21\u578b\u5728\u81ea\u9002\u5e94\u8868\u793a\u5177\u6709\u4e00\u5b9a\u5947\u6027\u7684\u51fd\u6570\u80fd\u529b. \u5728\u7b2c\u56db\u8282\u4e2d, \u6211\u4eec\u4f7f\u7528\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u6c42\u89e3\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u6570\u503c\u89e3. \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u8003\u8651\u4e24\u4e2a\u65b9\u7a0b: Burgers \u65b9\u7a0b\u548c Schr\u00f6dinger \u65b9\u7a0b, \u56e0\u4e3a\u8fd9\u4e24\u4e2a\u65b9\u7a0b\u7684\u89e3\u663e\u793a\u51fa\u67d0\u79cd\u7c7b\u578b\u7684\u5947\u6027. \u5728\u7b2c\u4e94\u8282\u4e2d, \u5f97\u51fa\u6700\u540e\u7684\u7ed3\u8bba. A Sparse DNN Model for Solving Partial Differential Equations In this section, we propose a sparse DNN model for solving nonlinear partial differential equations (PDEs). \u5728\u672c\u8282\u4e2d, \u6211\u4eec\u63d0\u51fa\u4e00\u4e2a\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7528\u4e8e\u6c42\u89e3\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b. We begin with describing the PDE and its boundary, initial conditions to be considered in this paper. Suppose that \\(\\Omega\\) is an open domain in \\(\\mathbb{R}^d\\) . By \\(\\Gamma\\) we denote the boundary of the domain \\(\\Omega\\) . Let \\(\\mathcal{F}\\) denote a nonlinear differential operator, \\(\\mathcal{I}\\) the initial condition operator, and \\(\\mathcal{B}\\) the boundary operator. We consider the following boundary/initial value problem of the nonlinear partial differential equation: \u9996\u5148\u4ece\u63cf\u8ff0\u672c\u6587\u8003\u8651\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u53ca\u5176\u521d\u8fb9\u503c\u6761\u4ef6\u5f00\u59cb. \u8bbe \\(\\Omega\\) \u662f \\(\\mathbb{R}^d\\) \u4e0a\u7684\u4e00\u4e2a\u5f00\u96c6. \u7528 \\(\\Gamma\\) \u8868\u793a\u5b9a\u4e49\u57df \\(\\Omega\\) \u7684\u8fb9\u754c. \u7528 \\(\\mathcal{F}\\) \u8868\u793a\u975e\u7ebf\u6027\u5fae\u5206\u7b97\u5b50, \\(\\mathcal{I}\\) \u8868\u793a\u521d\u59cb\u6761\u4ef6\u7b97\u5b50, \\(\\mathcal{B}\\) \u8868\u793a\u8fb9\u754c\u6761\u4ef6\u7b97\u5b50. \u6211\u4eec\u8003\u8651\u5982\u4e0b\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u8fb9\u503c\u95ee\u9898: $$ \\begin{align} \\mathcal{F}(u(t,x)) &= 0, &x\\in\\Omega,\\ &t\\in [0,T],\\tag{1}\\ \\mathcal{I}(u(t,x)) &= 0, &x\\in\\Omega,\\ &t=0,\\tag{2}\\ \\mathcal{B}(u(t,x)) &= 0, &x\\in\\Gamma,\\ &t\\in [0,T],\\tag{3}\\ \\end{align} $$ where \\(T > 0\\) , the data \\(u\\) on \\(\\Gamma\\) and \\(t = 0\\) are given and \\(u\\) in \\(\\Omega\\) is the solution to be learned. The formulation (1) - (3) covers a broad range of problems including conservation laws, reaction-diffusion equations, and Navier-Stokes equations. \u5176\u4e2d \\(T>0\\) , \u5728 \\(\\Gamma\\) \u548c \\(t=0\\) \u5904\u7684\u6570\u636e \\(u\\) \u7ed9\u5b9a, \u5728 \\(\\Omega\\) \u4e0a\u7684 \\(u\\) \u662f\u9700\u8981\u5b66\u4e60\u7684\u89e3. \u516c\u5f0f 1-3 \u6db5\u76d6\u4e86\u5f88\u5927\u8303\u56f4\u7684\u95ee\u9898\u5982\u5b88\u6052\u5f8b, \u53cd\u5e94\u6269\u6563\u65b9\u7a0b, Navier-Stokes \u65b9\u7a0b\u7b49. For example, the one dimensional Burgers equation can be recognized as \u4f8b\u5982, \u4e00\u7ef4 Burgers \u65b9\u7a0b\u53ef\u4ee5\u8868\u793a\u4e3a \\[ \\mathcal{F}(u) := \\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\frac{\\partial^2 u}{\\partial x^2}. \\] The goal of this paper is to develop a sparse DNN model for solving problem(1). We will conduct numerical study of the proposed model by applying it to two equations, the Burgers equation and the Schr\u00f6dinger equation, of practical importance. \u672c\u6587\u7684\u76ee\u6807\u662f\u5efa\u7acb\u4e00\u4e2a\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6c42\u89e3\u95ee\u9898 (1) . \u6211\u4eec\u5c06\u901a\u8fc7\u628a\u6a21\u578b\u5e94\u7528\u5230\u4e24\u4e2a\u5177\u6709\u5b9e\u9645\u91cd\u8981\u6027\u7684\u65b9\u7a0b, Burgers \u65b9\u7a0b\u548c Schr\u00f6dinger \u65b9\u7a0b\u6765\u5bf9\u8fd9\u4e2a\u6a21\u578b\u8fdb\u884c\u6570\u503c\u7814\u7a76. Now, we present the sparse DNN model with multi-parameter regularization. We first recall the the feed forward neural network (FNN). A neural network can be viewed as a composition of functions. A FNN of depth \\(D\\) is defined to be a neural network with an input layer, \\(D - 1\\) hidden layers, and an output layer. A neural network with more than two hidden layers is usually called a deep neural network (DNN). Suppose that there are \\(d_i\\) neurons in the \\(i\\) -th hidden layer. Let \\(W_i\\in \\mathbb{R}^{d_i\\times d_{i-1}}\\) and \\(b_i\\in\\mathbb{R}^{d_i}\\) denote, respectively, the weight matrix and bias vector of the \\(i\\) -th layer. By \\(x_0:= x \\in \\mathbb{R}^{d_0}\\) we denote the input vector and by \\(x_{i-1}\\in \\mathbb{R}^{d_{i-1}}\\) we denote the output vector of the ( \\(i - 1\\) )-th layer. For the \\(i\\) -th hidden layer, we define the affine transform \\(L_i: \\mathbb{R}^{d_{i-1}}\\mapsto \\mathbb{R}^{d_i}\\) by \u73b0\u5728\u6211\u4eec\u4ecb\u7ecd\u5e26\u6709\u591a\u53d8\u91cf\u6b63\u5219\u5316\u7684\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b. \u6211\u4eec\u9996\u5148\u56de\u5fc6\u524d\u9988\u795e\u7ecf\u7f51\u7edc FNN. \u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u89c6\u4e3a\u51fd\u6570\u7684\u590d\u5408. \u4e00\u4e2a \\(D\\) \u5c42\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u662f\u6307\u5177\u6709\u4e00\u5c42\u8f93\u5165\u5c42, \\(D-1\\) \u5c42\u9690\u85cf\u5c42\u548c\u4e00\u5c42\u8f93\u51fa\u5c42\u7684\u795e\u7ecf\u7f51\u7edc. \u5177\u6709\u8d85\u8fc7\u4e24\u5c42\u9690\u85cf\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u79f0\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc DNN. \u5047\u8bbe\u7b2c \\(i\\) \u5c42\u9690\u85cf\u5c42\u4e2d\u6709 \\(d_i\\) \u4e2a\u795e\u7ecf\u7f51\u7edc. \u7528 \\(W_i,b_i\\) \u5206\u522b\u8868\u793a\u7b2c \\(i\\) \u5c42\u7684\u6743\u91cd\u77e9\u9635\u548c\u504f\u5dee\u5411\u91cf. \\(x_0\\) \u8868\u793a\u8f93\u5165\u5411\u91cf, \\(x_{i-1}\\) \u8868\u793a\u7b2c \\(i-1\\) \u5c42\u7684\u8f93\u51fa\u5411\u91cf. \u5bf9\u4e8e\u7b2c \\(i\\) \u5c42\u9690\u85cf\u5c42, \u6211\u4eec\u5b9a\u4e49\u4eff\u5c04\u53d8\u6362 \\(L_i\\) \u5982\u4e0b: \\[ L_i(x_{i-1}) := W_i x_{i-1}+ b_i, i = 1,2,\\cdots,D. \\] For an activation function \\(\\sigma_i\\) , the output vector of the \\(i\\) -th hidden layer is defined as \u5bf9\u4e8e\u6fc0\u6d3b\u51fd\u6570 \\(\\sigma_i\\) , \u7b2c \\(i\\) \u5c42\u9690\u85cf\u5c42\u7684\u8f93\u51fa\u5411\u91cf\u5b9a\u4e49\u4e3a \\[ x_i:= \\sigma_i(L_i(x_{i-1})). \\] Given nonlinear activation functions \\(\\sigma_i, i = 1, 2,\\cdots, D-1\\) , the feed forward neural network \\(N_\\Theta(x)\\) of depth \\(D\\) is defined as \u7ed9\u5b9a \\(D-1\\) \u4e2a\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570 \\(\\sigma_i\\) , \u6df1\u5ea6\u4e3a \\(D\\) \u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc \\(N_\\Theta(x)\\) \u5b9a\u4e49\u4e3a \\[ N_\\Theta(x) := L_D\\circ \\sigma_{D-1} \\circ L_{D-1}\\circ \\cdots\\circ \\sigma_1\\circ L_1(x),\\tag{4} \\] where \\(\\circ\\) denotes the composition operator and \\(\\Theta :=\\{W_i, b_i\\}^D_{i=1}\\) is the set of trainable parameters in the network. \u5176\u4e2d \\(\\circ\\) \u8868\u793a\u590d\u5408\u7b97\u5b50, \\(\\Theta\\) \u662f\u7f51\u7edc\u53ef\u8bad\u7ec3\u53c2\u6570\u96c6\u5408. We first describe the physics-informed neural network ( PINN ) model introduced in 35 for solving the partial differential equation (1) . We denote by \\(Loss_{PDE}\\) the loss of training data on the partial differential equation (1) . We choose \\(N_f\\) collocation points \\((t^i_f, x^i_f)\\) by randomly sampling in domain \\(\\Omega\\) using a sampling method such as Latin hypercube sampling 23 . We then evaluate \\(\\mathcal{F}(\\mathcal{N}_\\Theta(t^i_f, x^i_f))\\) for \\(i = 1, 2,\\cdots N_f\\) and define \u6211\u4eec\u9996\u5148\u4ecb\u7ecd\u7528\u4e8e\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b (1) \u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc PINN. \u6211\u4eec\u4f7f\u7528\u67d0\u79cd\u91c7\u6837\u65b9\u6cd5\u5982\u62c9\u4e01\u8d85\u7acb\u65b9\u91c7\u6837 (LHS) \u4ece\u5b9a\u4e49\u57df \\(\\Omega\\) \u4e2d\u968f\u673a\u91c7\u6837 \\(N_f\\) \u4e2a\u914d\u7f6e\u70b9, \u7136\u540e\u5728\u8fd9\u4e9b\u70b9\u4e0a\u8ba1\u7b97 \\(\\mathcal{F}(\\mathcal{N}_\\Theta(t^i_f, x^i_f))\\) \u7684\u503c\u5e76\u5b9a\u4e49 \\[ Loss_{PDE}:= \\frac{1}{N_f} \\sum_{i=1}^{N_f} |\\mathcal{F}(\\mathcal{N}_\\Theta(t^i_f, x^i_f))|^2, \\] where \\(\\mathcal{F}\\) is the operator for the partial differential equation (1) . \u5176\u4e2d \\(\\mathcal{F}\\) \u662f\u504f\u5fae\u5206\u65b9\u7a0b (1) \u7684\u7b97\u5b50. We next describe the loss function for the boundary/initial condition. We randomly sample \\(N_0\\) points \\(x^i_0\\) for the initial condition (2) , \\(N_b\\) points \\(\\{t^i_b, x^i_b\\}\\) for the boundary condition (3) . The loss function \\(Loss_0\\) related to the initial value condition is given by \u7136\u540e\u6211\u4eec\u4ecb\u7ecd\u8fb9\u754c\u6761\u4ef6/\u521d\u59cb\u6761\u4ef6\u7684\u635f\u5931\u51fd\u6570. \u6211\u4eec\u4e3a\u521d\u59cb\u6761\u4ef6\u968f\u673a\u91c7\u6837 \\(N_0\\) \u4e2a\u70b9, \u4e3a\u8fb9\u754c\u6761\u4ef6\u968f\u673a\u91c7\u6837 \\(N_b\\) \u4e2a\u70b9. \u7136\u540e\u4e0e\u521d\u503c\u6761\u4ef6\u76f8\u5173\u7684\u635f\u5931\u51fd\u6570 \\(Loss_0\\) \u5b9a\u4e49\u5982\u4e0b \\[ Loss_0 := \\frac{1}{N_0} \\sum_{i=1}^{N_0} |\\mathcal{I}(\\mathcal{N}_\\Theta(0, x^i_0))|. \\] The loss function \\(Loss_b\\) pertaining to the boundary value is given as \u4e0e\u8fb9\u503c\u6761\u4ef6\u76f8\u5173\u7684\u635f\u5931\u51fd\u6570 \\(Loss_b\\) \u5b9a\u4e49\u5982\u4e0b \\[ Loss_b := \\frac{1}{N_b} \\sum_{i=1}^{N_b}|\\mathcal{B}(\\mathcal{N}_\\Theta(t^i_b, x^i_b))|, x^i_b\\in \\Gamma. \\] Adding the three loss functions \\(Loss_{PDE}\\) , \\(Loss_0\\) , and \\(Loss_b\\) together gives rise to the PINN model \u5c06\u4e09\u4e2a\u635f\u5931\u51fd\u6570 \\(Loss_{PDE}\\) , \\(Loss_0\\) , \\(Loss_b\\) \u76f8\u52a0\u5c31\u5f97\u5230\u4e86 PINN \u6a21\u578b \\[ \\min_\\Theta\\bigg\\{\\frac{1}{N_f} \\sum_{i=1}^{N_f} |\\mathcal{F}(\\mathcal{N}_\\Theta(t^i_f, x^i_f))|^2 + \\frac{1}{N_0} \\sum_{i=1}^{N_0} |\\mathcal{I}(\\mathcal{N}_\\Theta(0, x^i_0))| + \\frac{1}{N_b} \\sum_{i=1}^{N_b}|\\mathcal{B}(\\mathcal{N}_\\Theta(t^i_b, x^i_b))|\\bigg\\}\\tag{5} \\] where \\(\\Theta:=\\{W_i, b_i\\}^D_{i=1}\\) . \u5176\u4e2d \\(\\Theta:=\\{W_i, b_i\\}^D_{i=1}\\) . The neural network learned from (5) is often dense and may be over-parameterized. Moreover, training data are often contaminated with noise. When noise presents, over-parameterized models may overfit training data samples and result in bad generalization to the unseen samples. The problem of overfitting is often overcome by adding a regularization term: \u4ece\u4e0a\u8ff0\u635f\u5931\u51fd\u6570\u5b66\u4e60\u5230\u7684\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u662f\u5bc6\u96c6\u7684\u4e14\u53ef\u80fd\u8fc7\u53c2\u6570\u5316. \u6b64\u5916, \u8bad\u7ec3\u6570\u636e\u7ecf\u5e38\u88ab\u566a\u58f0\u6c61\u67d3. \u5f53\u566a\u58f0\u51fa\u73b0, \u8fc7\u53c2\u6570\u5316\u7684\u6a21\u578b\u53ef\u80fd\u5bf9\u8bad\u7ec3\u6570\u636e\u8fc7\u62df\u5408\u4ece\u800c\u5728\u672a\u77e5\u6837\u672c\u4e0a\u6cdb\u5316\u5f97\u5f88\u5dee. \u8fc7\u62df\u5408\u95ee\u9898\u901a\u5e38\u901a\u8fc7\u6dfb\u52a0\u4e00\u4e2a\u6b63\u5219\u5316\u9879\u6765\u514b\u670d. \\[ Loss := Loss_{PDE}+ \\beta (Loss_0 + Loss_b) + \\text{Regularization}. \\] The \\(l_1\\) - and \\(l_2\\) -norms are popular choices for regularization. Design of the regularization often makes use of prior information of the solution to be learned. It is known 4 17 42 that the \\(l_1\\) -norm can promote sparsity. Hence, the \\(l_1\\) -norm regularization not only has many advantages over the \\(l_2\\) -norm regularization, but also leads to sparse models which can be more easily interpreted. Therefore, we choose to use the \\(l_1\\) -norm as the regularizer in this study. Furthermore, we observe that DNNs have an intrinsic multiscale structure whose different layers represent different scales of information, which will be validated later by numerical studies. \\(l_1\\) \u8303\u6570\u548c \\(l_2\\) \u8303\u6570\u662f\u6b63\u5219\u5316\u7684\u5e38\u7528\u9009\u62e9. \u6b63\u5219\u5316\u7684\u8bbe\u8ba1\u901a\u5e38\u4f7f\u7528\u9700\u8981\u5b66\u4e60\u7684\u89e3\u7684\u5148\u9a8c\u4fe1\u606f. \u4ece\u5148\u524d\u7684\u5de5\u4f5c\u4e2d\u76f4\u5230 \\(l_1\\) \u8303\u6570\u53ef\u4ee5\u4fc3\u8fdb\u7a00\u758f\u6027. \u56e0\u6b64 \\(l_1\\) \u8303\u6570\u6b63\u5219\u5316\u4e0d\u4ec5\u6bd4 \\(l_2\\) \u8303\u6570\u6b63\u5219\u5316\u6709\u8bf8\u591a\u4f18\u70b9, \u8fd8\u80fd\u591f\u5bfc\u51fa\u66f4\u6613\u4e8e\u89e3\u91ca\u7684\u7a00\u758f\u6a21\u578b. \u56e0\u6b64\u6211\u4eec\u5728\u672c\u9879\u5de5\u4f5c\u4e2d\u9009\u62e9 \\(l_1\\) \u8303\u6570\u4f5c\u4e3a\u6b63\u5219\u5316\u9879. \u6b64\u5916, \u6211\u4eec\u89c2\u5bdf\u5230\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u5185\u5728\u591a\u5c3a\u5ea6\u7ed3\u6784, \u4e0d\u540c\u5c42\u8868\u793a\u4fe1\u606f\u7684\u4e0d\u540c\u5c3a\u5ea6, \u8fd9\u5c06\u5728\u4e4b\u540e\u7684\u6570\u503c\u7b97\u4f8b\u4e2d\u5f97\u5230\u9a8c\u8bc1. In fact, we will demonstrate in the next section that a smooth function or smooth parts of a function can be represented by a DNN with sparse weight matrices. This is because a smooth part of a function contains redundant information, which can be described very well by a few parameters, and only non-smooth parts of a function require more parameters to describe them. In other words, by properly choosing regularization, DNNs can lead to adaptive sparse representations of functions having certain singularities. With this understanding, we construct an adaptive representation of a function, especially for a function having certain singularity by adopting a sparse regularization model. Our idea for the adaptive representation is to impose different sparsity penalties for different layers. Specifically, we propose a multiscale-like sparse regularization using the \\(l_1\\) -norm of the weight matrix for each layer with a different parameter for a different layer. The regularization with multiple parameters allows us to represent a function in a multiscale-like neural network which is determined by sparse weight matrices having different sparsity at different layers. Such a regularization added to the loss function will enable us to robustly extract critical information of the solution of the PDE. \u5b9e\u9645\u4e0a, \u6211\u4eec\u5c06\u5728\u4e0b\u4e00\u8282\u8bf4\u660e\u7684\u662f\u4e00\u4e2a\u5149\u6ed1\u51fd\u6570\u6216\u51fd\u6570\u7684\u5149\u6ed1\u90e8\u5206\u53ef\u4ee5\u7531\u5177\u6709\u7a00\u758f\u6743\u91cd\u77e9\u9635\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6765\u8868\u793a. \u8fd9\u662f\u56e0\u4e3a\u51fd\u6570\u7684\u5149\u6ed1\u90e8\u5206\u5305\u542b\u4e86\u5197\u4f59\u4fe1\u606f, \u80fd\u591f\u88ab\u5c11\u6570\u53c2\u6570\u63cf\u8ff0\u5f97\u5f88\u597d, \u5e76\u4e14\u53ea\u6709\u51fd\u6570\u5f97\u975e\u5149\u6ed1\u90e8\u5206\u9700\u8981\u66f4\u591a\u7684\u53c2\u6570\u53bb\u63cf\u8ff0\u5b83\u4eec. \u6362\u53e5\u8bdd\u8bf4, \u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684\u6b63\u5219\u5316, \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u5f97\u5230\u5e26\u6709\u5947\u6027\u7684\u51fd\u6570\u7684\u81ea\u9002\u5e94\u7a00\u758f\u8868\u793a. \u6839\u636e\u8fd9\u4e00\u8ba4\u77e5, \u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u7a00\u758f\u6b63\u5219\u5316\u6a21\u578b\u6765\u6784\u9020\u51fd\u6570, \u7279\u522b\u662f\u5e26\u6709\u67d0\u4e9b\u5947\u6027\u7684\u51fd\u6570\u7684\u81ea\u9002\u5e94\u8868\u793a. \u5173\u4e8e\u81ea\u9002\u5e94\u8868\u793a\u7684\u601d\u8def\u662f\u7ed9\u4e0d\u540c\u7684\u5c42\u4e16\u5bb6\u4e0d\u540c\u7684\u7a00\u758f\u5ea6\u60e9\u7f5a. \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7c7b\u591a\u5c3a\u5ea6\u7a00\u758f\u6b63\u5219\u5316, \u5bf9\u6bcf\u4e00\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684 \\(l_1\\) \u8303\u6570\u8d4b\u4e88\u4e0d\u540c\u7684\u53c2\u6570 \u591a\u53c2\u6570\u6b63\u5219\u5316\u4f7f\u5f97\u6211\u4eec\u80fd\u591f\u5728\u7c7b\u591a\u5c3a\u5ea6\u7684\u795e\u7ecf\u7f51\u7edc\u4e2d\u8868\u793a\u4e00\u4e2a\u51fd\u6570, \u8be5\u7f51\u7edc\u7531\u4e0d\u540c\u5c42\u4e0a\u5177\u6709\u4e0d\u540c\u7a00\u758f\u5ea6\u7684\u7a00\u758f\u6743\u91cd\u77e9\u9635\u51b3\u5b9a. \u5728\u635f\u5931\u51fd\u6570\u4e2d\u52a0\u5165\u8fd9\u79cd\u6b63\u5219\u5316, \u53ef\u4ee5\u6709\u6548\u5730\u63d0\u53d6\u504f\u5fae\u5206\u65b9\u7a0b\u89e3\u7684\u5173\u952e\u4fe1\u606f. We now describe the proposed regularization. For layer \\(i\\) , we denote by \\(W^{k,j}_i\\) the \\((k, j)\\) -th entry of matrix \\(W_i\\) , the entry in the \\(k\\) -th row and the \\(j\\) -th column. For this reason, we adopt the \\(l_1\\) -norm of matrix \\(W_i\\) defined by following formula as our sparse regularization. \u6211\u4eec\u73b0\u5728\u63cf\u8ff0\u6240\u63d0\u51fa\u7684\u6b63\u5219\u5316. \u5bf9\u4e8e\u5c42 \\(i\\) , \u6211\u4eec\u7528 \\(W^{k,j}_i\\) \u8868\u793a\u77e9\u9635 \\(W_i\\) \u7684\u7b2c \\(k\\) \u884c\u7b2c \\(j\\) \u5217\u9879. \u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5c06\u77e9\u9635 \\(W_i\\) \u7684 \\(l_1\\) \u8303\u6570\u4f5c\u4e3a\u6211\u4eec\u7684\u7a00\u758f\u6b63\u5219\u5316. \\[ \\|W_i\\|_1:=\\sum_{k=1}^{d_i}\\sum_{j=1}^{d_{i-1}}|W^{k,j}_i| \\] Considering that different layers of the neural network play different roles in approximation of a function, we introduce here a multi-parameter regularization model \u8003\u8651\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u540c\u5c42\u5728\u903c\u8fd1\u4e00\u4e2a\u51fd\u6570\u65f6\u626e\u6f14\u4e0d\u540c\u7684\u89d2\u8272, \u6211\u4eec\u5f15\u5165\u5982\u4e0b\u591a\u53c2\u6570\u6b63\u5219\u5316\u6a21\u578b \\[ \\text{Regularization} :=\\sum_{i=1}^D \\alpha_i\\|W_i\\|_1 \\tag{6} \\] where \\(\\alpha_i\\) are nonnegative regularization parameters. \u5176\u4e2d \\(\\alpha_i\\) \u662f\u975e\u8d1f\u7684\u6b63\u5219\u5316\u53c2\u6570. The use of different parameters for weight matrices of different layers in the regularization term (6) allows us to penalize the weight matrices at different layers of the neural network differently in order to extract the multiscale representation of the solution to be learned. That is, for a fixed \\(i\\) , parameter \\(\\alpha_i\\) determines the sparsity of weight matrix \\(W_i\\) . The larger the parameter \\(\\alpha_i\\) , the more sparse the weight matrix \\(W_i\\) is. The regularized loss function takes the form \u5728\u6b63\u5219\u5316\u9879 (6) \u4e2d\u5bf9\u4e8e\u4e0d\u540c\u5c42\u7684\u6743\u91cd\u77e9\u9635\u4f7f\u7528\u4e0d\u540c\u53c2\u6570\u4f7f\u5f97\u6211\u4eec\u5bf9\u7f51\u7edc\u4e0d\u540c\u5c42\u7684\u6743\u91cd\u77e9\u9635\u8fdb\u884c\u60e9\u7f5a\u4ee5\u63d0\u53d6\u6240\u9700\u5b66\u4e60\u7684\u89e3\u7684\u591a\u5c3a\u5ea6\u8868\u793a. \u5373\u5bf9\u4e8e\u4e00\u4e2a\u56fa\u5b9a\u7684 \\(i\\) , \u53c2\u6570 \\(\\alpha_i\\) \u51b3\u5b9a\u4e86\u6743\u91cd\u77e9\u9635 \\(W_i\\) \u7684\u7a00\u758f\u6027. \\(\\alpha_i\\) \u8d8a\u5927, \u6743\u91cd\u77e9\u9635 \\(W_i\\) \u8d8a\u7a00\u758f. \u6b63\u5219\u5316\u635f\u5931\u51fd\u6570\u5f62\u5f0f\u5982\u4e0b: \\[ Loss := Loss_{PDE}+ \\beta(Loss_0+ Loss_b) +\\sum_{i=1}^D \\alpha_i\\|W_i\\|_1. \\tag{7} \\] The parameters \\(\\Theta:= \\{W_i, b_i\\}^D_{i=1}\\) of the neural network \\(\\mathcal{N}_\\Theta(t,x)\\) are learned by minimizing the loss function \u795e\u7ecf\u7f51\u7edc \\(\\mathcal{N}_\\Theta(t,x)\\) \u7684\u53c2\u6570 \\(\\Theta\\) \u901a\u8fc7\u6700\u5c0f\u5316\u5982\u4e0b\u635f\u5931\u51fd\u6570\u8fdb\u884c\u5b66\u4e60. \\[ \\min_\\Theta \\bigg\\{\\frac{1}{N_f} \\sum_{i=1}^{N_f} |\\mathcal{F}(\\mathcal{N}_\\Theta(t^i_f, x^i_f))|^2 + \\beta(Loss_0+Loss_b) + \\sum_{i=1}^D \\alpha_i\\|W_i\\|_1\\bigg\\}.\\tag{8} \\] Truncating the weights of the layers close to the input layer has an impact on all subsequent layers. In practice, we usually set smaller regularization parameters in layers close to the input and larger regularization parameters in layers close to the output. The resulting neural network will exhibit denser weight matrices near the input layer and sparser weight matrices near the output layer. This network structure reflects the multi-scale nature of neural networks and is automatically learned by sparse regularization. \u622a\u65ad\u9760\u8fd1\u8f93\u5165\u5c42\u7684\u9690\u85cf\u5c42\u7684\u6743\u91cd\u4f1a\u5bf9\u540e\u7eed\u5c42\u4ea7\u751f\u5f71\u54cd. \u5b9e\u8df5\u4e2d\u6211\u4eec\u901a\u5e38\u5bf9\u9760\u8fd1\u8f93\u5165\u5c42\u7684\u9690\u85cf\u5c42\u8bbe\u7f6e\u66f4\u5c0f\u7684\u6b63\u5219\u5316\u53c2\u6570, \u5bf9\u9760\u8fd1\u8f93\u51fa\u5c42\u7684\u9690\u85cf\u5c42\u8bbe\u7f6e\u66f4\u5927\u7684\u6b63\u5219\u5316\u53c2\u6570. \u5f97\u5230\u7684\u795e\u7ecf\u7f51\u7edc\u4f1a\u8868\u73b0\u51fa\u9760\u8fd1\u8f93\u5165\u5c42\u7684\u6743\u91cd\u77e9\u9635\u66f4\u5bc6\u96c6, \u9760\u8fd1\u8f93\u51fa\u5c42\u7684\u6743\u91cd\u77e9\u9635\u66f4\u7a00\u758f. \u8fd9\u6837\u7684\u7f51\u7edc\u7ed3\u6784\u53cd\u6620\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u591a\u5c3a\u5ea6\u672c\u8d28\u4e14\u81ea\u52a8\u5730\u7531\u7a00\u758f\u6b63\u5219\u5316\u5b66\u4e60\u5230. Appropriate choices of the regularization parameters are key to achieve good prediction results. We need to balance sparsity and prediction accuracy. Since there are multiple regularization parameters, the regularization parameters are chosen by grid search layer by layer in this paper. In practice, we first choose the regularization parameters close to the output layer, and then gradually choose the regularization coefficients close to the input layer. \u6b63\u5219\u5316\u53c2\u6570\u7684\u9002\u5f53\u9009\u62e9\u662f\u83b7\u5f97\u826f\u597d\u9884\u6d4b\u7ed3\u679c\u7684\u5173\u952e. \u6211\u4eec\u9700\u8981\u5e73\u8861\u7a00\u758f\u6027\u548c\u9884\u6d4b\u7cbe\u5ea6. \u56e0\u4e3a\u6709\u591a\u4e2a\u6b63\u5219\u5316\u53c2\u6570, \u6240\u4ee5\u672c\u6587\u7684\u6b63\u5219\u5316\u53c2\u6570\u901a\u8fc7\u9010\u5c42\u7f51\u683c\u641c\u7d22\u5f97\u5230. \u5b9e\u8df5\u4e2d\u6211\u4eec\u9996\u5148\u9009\u62e9\u9760\u8fd1\u8f93\u51fa\u5c42\u7684\u6b63\u5219\u5316\u53c2\u6570, \u7136\u540e\u9010\u6e10\u9009\u62e9\u9760\u8fd1\u8f93\u5165\u5c42\u7684\u6b63\u5219\u5316\u7cfb\u6570. We refer equation (8) as to the sparse DNN ( SDNN ) model for the partial differential equation. Upon solving the minimization problem (8) , we obtain an approximate solution \\(u(t,x) := \\mathcal{N}_\\Theta(t, x)\\) with sparse weight matrices. When the regularization parameters \\(\\alpha_i\\) are all set to \\(0\\) , the SDNN model (8) reduces to the PINN model introduced in 35 . We will compare numerical performance of the proposed SDNN model with that of PINN model, for both the Burgers equation and the Schr\u00f6dinger equation. \u6211\u4eec\u5f15\u7528\u65b9\u7a0b (8) \u4f5c\u4e3a\u7528\u4e8e\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc ( SDNN ) \u6a21\u578b. \u57fa\u4e8e\u6c42\u89e3\u6700\u5c0f\u5316\u95ee\u9898 (8) , \u6211\u4eec\u83b7\u5f97\u4e86\u4e00\u4e2a\u5e26\u6709i\u5b66\u672f\u6743\u91cd\u77e9\u9635\u7684\u8fd1\u4f3c\u89e3 \\(u(t,x) := \\mathcal{N}_\\Theta(t, x)\\) . \u5f53\u6b63\u5219\u5316\u53c2\u6570 \\(\\alpha_i\\) \u5168\u90e8\u8bbe\u7f6e\u4e3a \\(0\\) , \u90a3\u4e48 SDNN \u6a21\u578b\u5c06\u9000\u5316\u4e3a PINN \u6a21\u578b. \u6211\u4eec\u5c06\u5728 Burgers \u65b9\u7a0b\u548c Schr\u00f6dinger \u65b9\u7a0b\u4e0a\u6bd4\u8f83\u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u548c PINN \u7684\u6570\u503c\u8868\u73b0. Function Adaptive Approximation by the SDNN Model We explore in this section the capacity of the proposed multi-parameter regularization in adaptive representing functions that have certain singularities. We will first reveal that a DNN indeed has an intrinsic multiscale-like structure which is desirable for representing non-smooth functions. We demonstrate in our numerical studies that the proposed SDNN model can reconstruct neural networks which approximate functions in the same accuracy order with nearly the same order of network complexity, regardless the smoothness of the functions. We include in this section a numerical study of reconstruction of black holes by the proposed SDNN model. In this section, we use the rectified linear unit (ReLU) function as an activation function. \u672c\u8282\u6211\u4eec\u63a2\u7a76\u6240\u63d0\u51fa\u7684\u591a\u53c2\u6570\u6b63\u5219\u5316\u5728\u81ea\u9002\u5e94\u8868\u793a\u5e26\u6709\u67d0\u4e9b\u5947\u6027\u7684\u51fd\u6570\u7684\u80fd\u529b. \u6211\u4eec\u9996\u5148\u63ed\u793a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u786e\u5b9e\u5177\u6709\u5185\u5728\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784, \u8fd9\u79cd\u7ed3\u6784\u5bf9\u4e8e\u8868\u793a\u975e\u5149\u6ed1\u51fd\u6570\u662f\u53ef\u53d6\u7684. \u6570\u503c\u7814\u7a76\u8868\u660e, \u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u65e0\u8bba\u51fd\u6570\u7684\u5149\u6ed1\u5ea6\u5982\u4f55, \u90fd\u80fd\u4ee5\u8fd1\u4f3c\u76f8\u540c\u7684\u7f51\u7edc\u590d\u6742\u5ea6\u9636\u6570\u91cd\u6784\u51fa\u7cbe\u5ea6\u76f8\u540c\u7684\u51fd\u6570. \u5728\u8fd9\u4e00\u8282\u4e2d\u8fd8\u5305\u62ec\u4e00\u4e2a\u4f7f\u7528\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u91cd\u5efa\u9ed1\u6d1e\u7684\u6570\u503c\u7814\u7a76. \u5728\u672c\u8282\u4e2d, \u6211\u4eec\u4f7f\u7528\u6574\u6d41\u7ebf\u6027\u5355\u4f4d\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570. \\[ \\text{ReLU}(x) := \\max\\{0, x\\}, x \\in \\mathbb{R} \\] We first describe the data fitting problem. Given training points \\((x_i, y_i), i =1, 2, \\cdots, N\\) , a non-regularized neural network is determined by minimizing the regression error, that is, \u6211\u4eec\u9996\u5148\u63cf\u8ff0\u6570\u636e\u62df\u5408\u95ee\u9898. \u7ed9\u5b9a\u8bad\u7ec3\u6837\u672c\u70b9 \\((x_i, y_i), i =1, 2, \\cdots, N\\) , \u975e\u6b63\u5219\u5316\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u6700\u5c0f\u5316\u5982\u4e0b\u56de\u5f52\u635f\u5931\u786e\u5b9a \\[ \\min_\\Theta\\frac{1}{N} \\sum_{i=1}^N |\\mathcal{N}_\\Theta(x_i) - y_i|^2.\\tag{9} \\] The multi-parameter sparse regularization DNN model for the data fitting problem reads \u7528\u4e8e\u6570\u636e\u62df\u5408\u95ee\u9898\u7684\u591a\u53c2\u6570\u7a00\u758f\u6b63\u5219\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5219\u6700\u5c0f\u5316\u5982\u4e0b\u635f\u5931 \\[ \\min_\\Theta\\bigg\\{\\frac{1}{N}\\sum_{i=1}^N|\\mathcal{N}_\\Theta(x_i) - y_i|^2+\\sum_{i=1}^D \\alpha_i \\|W_i\\|_1\\bigg\\},\\tag{10} \\] where \\(\\alpha_i\\) are nonnegative regularization parameters and \\(W_i\\) are weight matrices. \u5176\u4e2d \\(\\alpha_i\\) \u662f\u975e\u8d1f\u6b63\u5219\u5316\u53c2\u6570, \\(W_i\\) \u662f\u6743\u91cd\u77e9\u9635. In examples to be presented in this section and the section that follows, the network structure is described by the number of neurons in each layer. Specifically, we use the notation \\([d_0, d_1,\\cdots,d_D]\\) to describe networks that have one input layer, \\(D - 1\\) hidden layers and one output layer, with \\(d_0, d_1,\\cdots, d_D\\) number of neurons, respectively. The regularization parameters, which will be presented as a vector \\(\\alpha:= [\\alpha_1, \\alpha_2,\\cdots, \\alpha_D]\\) , are chosen so that best results are obtained. We will use the relative \\(L_2\\) error to measure approximation accuracy. Suppose that \\(y_i\\) is the exact value of function \\(f\\) to be approximated at \\(x_i\\) , that is, \\(y_i= f(x_i)\\) , and suppose that \\(\\hat{y}_i:= \\mathcal{N}_\\Theta (x_i)\\) is the output of the neural network approximation of \\(f\\) . We let \\(y := [y_1, y_2,\\cdots, y_N]\\) and \\(\\hat{y} := [\\hat{y}_1, \\hat{y}_2,\\cdots, \\hat{y}_N]\\) , and define the error by \\(\\dfrac{\\|y-\\hat{y}\\|_2}{\\|y\\|_2}\\) . Sparsity of the weight matrices is measured by the percentage of zero entries in the weight matrices \\(W_i\\) . In our computation, we set a weight matrix entry \u5728\u672c\u8282\u548c\u4e4b\u540e\u7ae0\u8282\u5c55\u793a\u7684\u4f8b\u5b50\u4e2d, \u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u6784\u901a\u8fc7\u6bcf\u5c42\u795e\u7ecf\u5143\u7684\u6570\u91cf\u8fdb\u884c\u63cf\u8ff0. \u5177\u4f53\u7684, \u6211\u4eec\u4f7f\u7528 \\([d_0, d_1,\\cdots,d_D]\\) \u6765\u63cf\u8ff0\u5177\u6709\u4e00\u5c42\u8f93\u5165\u5c42, \\(D-1\\) \u5c42\u9690\u85cf\u5c42\u548c\u4e00\u5c42\u8f93\u51fa\u5c42, \u5bf9\u5e94\u795e\u7ecf\u5143\u6570\u91cf\u4e3a \\(d_0, d_1,\\cdots, d_D\\) \u7684\u795e\u7ecf\u7f51\u7edc. \u6b63\u5219\u5316\u53c2\u6570\u5c06\u8868\u793a\u4e3a\u4e00\u4e2a\u5411\u91cf \\(\\alpha:= [\\alpha_1, \\alpha_2,\\cdots, \\alpha_D]\\) , \u5c06\u9009\u62e9\u51fa\u80fd\u83b7\u5f97\u6700\u4f73\u7ed3\u679c\u7684\u53c2\u6570. \u6211\u4eec\u5c06\u4f7f\u7528\u76f8\u5bf9 \\(L_2\\) \u8bef\u5dee\u6765\u5ea6\u91cf\u903c\u8fd1\u7cbe\u5ea6. \u5047\u8bbe \\(y_i\\) \u662f\u88ab\u903c\u8fd1\u51fd\u6570 \\(f\\) \u5728 \\(x_i\\) \u7684\u7cbe\u786e\u503c, \u5373 \\(y_i=f(x_i)\\) , \u8bbe \\(\\hat{y}_i:= \\mathcal{N}_\\Theta (x_i)\\) \u662f\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c \\(f\\) \u7684\u8f93\u51fa. \u6211\u4eec\u8ba1\u7b97 \\(N\\) \u4e2a\u70b9\u4e0a\u7684 \\(y := [y_1, y_2,\\cdots, y_N]\\) \u4ee5\u53ca \\(\\hat{y} := [\\hat{y}_1, \\hat{y}_2,\\cdots, \\hat{y}_N]\\) , \u4ece\u800c\u5b9a\u4e49\u8bef\u5dee\u4e3a \\(\\dfrac{\\|y-\\hat{y}\\|_2}{\\|y\\|_2}\\) . \u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u5219\u901a\u8fc7\u6743\u91cd\u77e9\u9635 \\(W_i\\) \u91cc\u96f6\u5143\u7d20\u7684\u767e\u5206\u6bd4\u8fdb\u884c\u5ea6\u91cf. \u5728\u6211\u4eec\u7684\u8ba1\u7b97\u4e2d, \u6211\u4eec\u8bbe\u7f6e\u4e00\u4e2a\u6743\u91cd\u77e9\u9635\u5143\u7d20\u4e3a \\[ W^{k,j}_i= 0,\\quad \\text{if}\\ |W^{k,j}_i| < \\epsilon, \\] where \\(\\epsilon\\) is small positive number. In our numerical examples, we set \\(\\epsilon := 0.001\\) by default. For all numerical examples, the non-smooth, non-convex optimization problem (10) is solved by the Adam algorithm, which is an improved version of the stochastic gradient descent algorithm proposed in 27 for training deep learning models. \u5176\u4e2d \\(\\epsilon\\) \u662f\u4e00\u4e2a\u8f83\u5c0f\u7684\u6574\u6570. \u5728\u6211\u4eec\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u9ed8\u8ba4\u8bbe\u7f6e \\(\\epsilon=0.001\\) . \u5bf9\u4e8e\u6240\u6709\u7684\u6570\u503c\u4f8b\u5b50, \u975e\u5149\u6ed1\u975e\u51f8\u4f18\u5316\u95ee\u9898 (10) \u901a\u8fc7\u7528\u4e8e\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7684\u6539\u8fdb\u7248 Adam \u7b97\u6cd5\u8fdb\u884c\u6c42\u89e3. Intrinsic Adaptivity of the SDNN Model We first investigate whether the SDNN model (10) can generate a network that has an intrinsic adaptive representation of a function. That is, a function generated by the model has a multiscale-like structure so that the reconstructed neural networks approximate functions in the same accuracy order with nearly the same order of network complexity, regardless the smoothness of the functions. In particular, when a function is singular a sparse network with higher layers is generated to capture the higher resolution information of the function. The complexity of the network is nearly proportional to the reciprocal of the approximation error regardless whether the function is smooth or not. In this experiment, we consider two examples: (1) one-dimensional functions and (2) two-dimensional functions. \u6211\u4eec\u9996\u5148\u7814\u7a76 SDNN \u6a21\u578b (10) \u662f\u5426\u80fd\u591f\u751f\u6210\u5177\u6709\u51fd\u6570\u5185\u5728\u81ea\u9002\u5e94\u8868\u793a\u7684\u7f51\u7edc. \u7531\u8be5\u6a21\u578b\u751f\u6210\u7684\u51fd\u6570\u5177\u6709\u591a\u5c3a\u5ea6\u7ed3\u6784, \u4f7f\u5f97\u91cd\u6784\u540e\u7684\u795e\u7ecf\u7f51\u7edc\u65e0\u8bba\u51fd\u6570\u7684\u5149\u6ed1\u5ea6\u5982\u4f55, \u90fd\u80fd\u4ee5\u76f8\u540c\u7684\u7cbe\u5ea6\u9636\u903c\u8fd1\u51fd\u6570, \u800c\u7f51\u7edc\u7684\u590d\u6742\u5ea6\u9636\u51e0\u4e4e\u76f8\u540c. \u7279\u522b\u5730, \u5f53\u4e00\u4e2a\u51fd\u6570\u662f\u5947\u5f02\u7684, \u4f1a\u4ea7\u751f\u4e00\u4e2a\u5177\u6709\u66f4\u591a\u5c42\u7684\u7a00\u758f\u7f51\u7edc\u6765\u6355\u83b7\u8be5\u51fd\u6570\u7684\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u4fe1\u606f. \u4e0d\u7ba1\u51fd\u6570\u662f\u5426\u5149\u6ed1, \u7f51\u7edc\u590d\u6742\u5ea6\u51e0\u4e4e\u4e0e\u903c\u8fd1\u8bef\u5dee\u7684\u5012\u6570\u6210\u6b63\u6bd4. \u5728\u8fd9\u4e2a\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u8003\u8651\u4e24\u4e2a\u4f8b\u5b50: (1) \u4e00\u7ef4\u51fd\u6570 (2) \u4e8c\u7ef4\u51fd\u6570. In our first example, we consider approximation of the quadratic function \u5728\u7b2c\u4e00\u4e2a\u4f8b\u5b50\u4e2d, \u6211\u4eec\u4f7f\u7528 SDNN \u5bf9\u4e8c\u6b21\u51fd\u6570\u548c\u4e8c\u6b21\u5206\u6bb5\u51fd\u6570\u8fdb\u884c\u8fd1\u4f3c. \\[ f(x) := x^2, \\tag{11} \\] and the piecewise quadratic function by SDNN . \\[ f(x) = \\begin{cases} x^2 + 1, &x \\geq 0,\\\\ x^2, &x < 0, \\end{cases}\\tag{12} \\] Note that the function defined by (11) is smooth and the function by (12) has a jump discontinuity at the point \\(0\\) . We applied the sparse regularized network having the architecture \\([1, 10, 10, 10, 10, 1]\\) to learn these functions. We divide the interval \\([-2, 2]\\) by the nodes \\(x_j:= -2 + jh\\) , for \\(j := 0, 1,\\cdots, 200\\) , with \\(h := 1/50\\) , and sample the functions \\(f\\) at \\(x_j\\) . The test set is \\(\\{(x_k, f(x_k))\\}\\) , where \\(x_k:= -2 + kh, h := 1/30, k = 0, 1,\\cdots, 120\\) . The network is trained by the Adam algorithm with epochs \\(20,000\\) and initial learning rate \\(0.001\\) . For function (11) , regularization parameters are set to be \\([0, 10^{-4}, 10^{-4}, 10^{-3},10^{-3}]\\) . We obtain the prediction error \\(5.94\\times10^{-3}\\) for the test set. Sparsity of the resulting weight matrices is \\([0.0\\%, 87.0\\%, 95.0\\%, 98.0\\%, 90.0\\%]\\) and the number of nonzero weight matrix entries is \\(31\\) . Left of Figure 1 shows the reconstructed SDNN for the function defined by (11) . \u6ce8\u610f\u7531 (11) \u5b9a\u4e49\u7684\u51fd\u6570\u662f\u5149\u6ed1\u7684, (12) \u5b9a\u4e49\u7684\u51fd\u6570\u5728\u70b9 \\(0\\) \u5904\u662f\u8df3\u8dc3\u95f4\u65ad\u7684. \u6211\u4eec\u4f7f\u7528\u7f51\u7edc\u67b6\u6784\u4e3a \\([1, 10, 10, 10, 10, 1]\\) \u7684\u7a00\u758f\u6b63\u5219\u5316\u7f51\u7edc\u6765\u5b66\u4e60\u8fd9\u4e9b\u51fd\u6570. \u6211\u4eec\u5c06\u533a\u95f4 \\([-2,2]\\) \u8fdb\u884c\u4e24\u767e\u7b49\u5206, \u5e76\u5728\u8fd9\u4e9b\u70b9\u4e0a\u91c7\u6837 \\(f\\) . \u6d4b\u8bd5\u96c6\u5219\u662f\u5c06\u533a\u95f4\u4e00\u767e\u4e8c\u5341\u7b49\u5206. \u7f51\u7edc\u901a\u8fc7 Adam \u7b97\u6cd5\u8bad\u7ec3 \\(20,000\\) \u4e2a epochs, \u521d\u59cb\u5b66\u4e60\u7387\u4e3a \\(0.001\\) . \u5bf9\u4e8e\u51fd\u6570 (11) , \u6b63\u5219\u5316\u53c2\u6570\u8bbe\u7f6e\u4e3a \\([0, 10^{-4}, 10^{-4}, 10^{-3},10^{-3}]\\) . \u6211\u4eec\u5728\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u4e86 \\(5.94\\times10^{-3}\\) \u7684\u9884\u6d4b\u8bef\u5dee. \u5f97\u5230\u7684\u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u4e3a \\([0.0\\%, 87.0\\%, 95.0\\%, 98.0\\%, 90.0\\%]\\) , \u6743\u91cd\u77e9\u9635\u5143\u7d20\u975e\u96f6\u6570\u91cf\u4e3a \\(31\\) . \u56fe 1 \u7684\u5de6\u56fe\u5c55\u793a\u4e86\u903c\u8fd1\u51fd\u6570 (11) \u7684 SDNN \u7684\u91cd\u6784\u7ed3\u679c. For function (12) the regularization parameters are chosen as \\([10^{-5}, 10^{-4}, 10^{-4},10^{-4}, 10^{-3}]\\) . We obtain the prediction error \\(5.42\\times10^{-3}\\) for the test set. Sparsity of the resulting weight matrices is \\([50\\%, 93.0\\%, 88.0\\%, 93.0\\%, 90.0\\%]\\) and the number of nonzero weight matrix entries is 32. The reconstructed function is shown in Right of Figure 1 . \u5bf9\u4e8e\u51fd\u6570 (12) , \u6b63\u5219\u5316\u53c2\u6570\u8bbe\u7f6e\u4e3a \\([10^{-5}, 10^{-4}, 10^{-4},10^{-4}, 10^{-3}]\\) . \u6211\u4eec\u5728\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u4e86 \\(5.42\\times10^{-3}\\) \u7684\u9884\u6d4b\u8bef\u5dee. \u5f97\u5230\u7684\u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u4e3a \\([0.0\\%, 87.0\\%, 95.0\\%, 98.0\\%, 90.0\\%]\\) , \u6743\u91cd\u77e9\u9635\u5143\u7d20\u975e\u96f6\u6570\u91cf\u4e3a \\(32\\) . \u56fe 1 \u7684\u53f3\u56fe\u5c55\u793a\u4e86\u903c\u8fd1\u51fd\u6570 (12) \u7684 SDNN \u7684\u91cd\u6784\u7ed3\u679c. Figure 1 : Numerical results of SDNN : for function (11) (Left); for function (12) (Right) Numerical results for both functions (11) and (12) are summarized in Table 1 . These results demonstrate that even though the function (12) has a jump discontinuity at the point \\(0\\) , the proposed SDNN model can generate a network with nearly the same number of nonzero weight matrix entries and with the same accuracy as those for the smooth function (11) . This shows that the proposed SDNN model has a good adaptive approximation property. \u8868\u683c 1 \u603b\u7ed3\u4e86 (11) \u548c (12) \u7684\u6570\u503c\u7ed3\u679c. \u8fd9\u4e9b\u7ed3\u679c\u8bf4\u660e\u4e86\u5c3d\u7ba1\u51fd\u6570 (12) \u5728 \\(0\\) \u5904\u8df3\u8dc3\u95f4\u65ad, \u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u80fd\u591f\u751f\u6210\u548c\u903c\u8fd1\u5149\u6ed1\u51fd\u6570 (11) \u65f6\u975e\u96f6\u6743\u91cd\u77e9\u9635\u5143\u7d20\u6570\u91cf\u51e0\u4e4e\u76f8\u540c\u4e14\u7cbe\u5ea6\u76f8\u540c\u7684\u7f51\u7edc. \u8fd9\u8bf4\u660e\u4e86\u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u6709\u826f\u597d\u7684\u81ea\u9002\u5e94\u903c\u8fd1\u6027\u8d28. Results for function (11) (12) Regularization parameters \\([0, 10^{-4}, 10^{-4}, 10^{-3}, 10^{-3}]\\) \\([10^{-5}, 10^{-4}, 10^{-4}, 10^{-4}, 10^{-3}]\\) Relative \\(L_2\\) error \\(5.94\\times10^{-3}\\) \\(5.42\\times10^{-3}\\) Sparsity of weight matrices \\([0.0\\%, 87.0\\%, 95.0\\%, 98.0\\%, 90.0\\%]\\) \\([50\\%, 93.0\\%, 88.0\\%, 93.0\\%, 90.0\\%]\\) No. of nonzero entries 31 32 Table 1 : Numerical result for quadratic function (11) and piecewise quadratic function (12) with network structure \\([1, 10, 10, 10, 10, 1]\\) . In our second example, we consider approximation of two-dimensional functions, once again one smooth function and one discontinuous function. We study smooth function \u5728\u7b2c\u4e8c\u4e2a\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u8003\u8651\u4e8c\u7ef4\u51fd\u6570, \u540c\u6837\u662f\u4e00\u4e2a\u5149\u6ed1\u51fd\u6570, \u4e00\u4e2a\u662f\u4e0d\u8fde\u7eed\u51fd\u6570. \u5149\u6ed1\u51fd\u6570\u7684\u56fe\u50cf\u5728\u56fe 2 \u5de6\u56fe\u5c55\u793a, \u5206\u6bb5\u51fd\u6570\u7684\u56fe\u50cf\u5728\u56fe 3 \u5de6\u56fe\u5c55\u793a. \\[ g(x, y) := e^{2x+y^2},\\tag{13} \\] whose image is illustrated in Figure 2 (Left), and piecewise function \\[ g_d(x, y) = \\begin{cases} e^{2x+y^2} + 1, &x \\geq 0,\\\\ e^{2x+y^2}, &x < 0,\\\\ \\end{cases}\\tag{14} \\] whose image is illustrated in Figure 3 (Left). Note that function (13) is smooth and function (14) has a jump discontinuity along \\(x = 0\\) . \u6ce8\u610f\u51fd\u6570 (13) \u662f\u5149\u6ed1\u7684, \u51fd\u6570 (14) \u6cbf\u7740 \\(x=0\\) \u6709\u8df3\u8dc3\u95f4\u65ad. For these two functions, the training data set is composed of grid points \\([-1, 1]\\times[-1, 1]\\) uniformly discretized with step size \\(1/200\\) on \\(x\\) and \\(y\\) direction, and the test set is composed of grid points \\([-1, 1]\\times[-1, 1]\\) uniformly discretized with step size \\(1/300\\) on the \\(x\\) and \\(y\\) directions. The network has \\(2\\) inputs, \\(4\\) hidden layers, and \\(1\\) output, with the architecture \\([2, 20, 20, 20, 20, 1]\\) . For each hidden layer, there are \\(20\\) neurons. The initial learning rate for Adam is set to \\(0.001\\) . The batch size is equal to \\(1024\\) . \u5bf9\u4e8e\u8fd9\u4e24\u4e2a\u51fd\u6570, \u8bad\u7ec3\u6570\u636e\u7531\u5bf9\u5b9a\u4e49\u57df \\([-1, 1]\\times[-1, 1]\\) \u5728 \\(x\\) \\(y\\) \u4e24\u4e2a\u65b9\u5411\u6b65\u957f\u4e3a \\(1/200\\) \u5f97\u5230\u7684\u7f51\u683c\u70b9\u7ec4\u6210, \u6d4b\u8bd5\u96c6\u5219\u8fdb\u884c\u4e09\u767e\u7b49\u5206. \u7f51\u7edc\u6709\u4e24\u4e2a\u8f93\u5165, \u56db\u5c42\u9690\u85cf\u5c42\u548c\u4e00\u4e2a\u8f93\u51fa, \u7f51\u7edc\u7ed3\u6784\u4e3a \\([2, 20, 20, 20, 20, 1]\\) . \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u9690\u85cf\u5c42, \u90fd\u6709\u4e8c\u5341\u4e2a\u795e\u7ecf\u5143. Adam \u521d\u59cb\u5b66\u4e60\u7387\u4e3a 0.001. \u6279\u91cf\u5927\u5c0f\u53d6 1024. For function (13) we set the sparse regularization parameters as \\([0, 10^{-6}, 10^{-4},10^{-4}, 10^{-4}]\\) . After \\(10,000\\) epochs training, the sparsity of weight matrices is \\([0.0\\%, 68.5\\%, 95.75\\%, 97.75\\%, 80.0\\%]\\) and the number of nonzero weight matrix entries is \\(178\\) . The prediction error for the test set is \\(4.38\\times10^{-3}\\) . For function (14) , the regularization parameters are set to be \\([10^{-4}, 10^{-5}, 10^{-5}, 10^{-4}, 10^{-4}]\\) . The sparsity of weight matrices after regularization are \\([60.0\\%, 71.75\\%, 81.75\\%, 97.5\\%, 90.0\\%]\\) and the number of nonzero weight matrix entries is \\(206\\) . The prediction error for sparse regularized deep neural network is \\(4.27\\times10^{-3}\\) , which is even slightly better than that for function (13) . The images of the reconstructed functions are shown respectively in Figures 2 , Figure 3 (Right). Numerical results for this example are reported in Table 2 . \u5bf9\u4e8e\u51fd\u6570 (13) , \u6211\u4eec\u8bbe\u7f6e\u7a00\u758f\u6b63\u5219\u5316\u53c2\u6570\u4e3a \\([0, 10^{-6}, 10^{-4},10^{-4}, 10^{-4}]\\) . \u5728 \\(10,000\\) \u4e2a epochs \u8bad\u7ec3\u540e, \u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u4e3a \\([0.0\\%, 68.5\\%, 95.75\\%, 97.75\\%, 80.0\\%]\\) \u4e14\u975e\u96f6\u6743\u91cd\u77e9\u9635\u5143\u7d20\u6570\u91cf\u4e3a \\(178\\) . \u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u9884\u6d4b\u8bef\u5dee\u4e3a \\(4.38\\times10^{-3}\\) . \u5bf9\u4e8e\u51fd\u6570 (14) , \u6211\u4eec\u8bbe\u7f6e\u7a00\u758f\u6b63\u5219\u5316\u53c2\u6570\u4e3a \\([10^{-4}, 10^{-5}, 10^{-5}, 10^{-4}, 10^{-4}]\\) . \u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u4e3a \\([60.0\\%, 71.75\\%, 81.75\\%, 97.5\\%, 90.0\\%]\\) \u4e14\u975e\u96f6\u6743\u91cd\u77e9\u9635\u5143\u7d20\u6570\u91cf\u4e3a \\(206\\) . \u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u9884\u6d4b\u8bef\u5dee\u4e3a \\(4.27\\times10^{-3}\\) , \u751a\u81f3\u6bd4\u51fd\u6570 (13) \u7684\u7ed3\u679c\u8fd8\u7a0d\u5fae\u597d\u4e00\u70b9. \u91cd\u6784\u7684\u51fd\u6570\u56fe\u50cf\u5206\u522b\u5728\u56fe 2 \u53f3\u56fe\u548c\u56fe 3 \u53f3\u56fe\u5c55\u793a. \u8868\u683c 2 \u62a5\u544a\u4e86\u8fd9\u4e2a\u4f8b\u5b50\u7684\u6570\u503c\u7ed3\u679c. Figure 2 : Left: image of function \\(e^{2x+y^2}\\) . Right: predicted by fully connected neural network. Figure 3 : image of piecewise discontinuous function (14) . Right: predicted by sparse regularized neural network. Results for function (13) (14) Regularization parameters \\([0, 10^{-6}, 10^{-4}, 10^{-4}, 10^{-4}]\\) \\([10^{-4}, 10^{-5}, 10^{-5}, 10^{-4}, 10^{-4}]\\) Relative \\(L_2\\) error \\(4.38\\times10^{-3}\\) \\(4.27\\times10^{-3}\\) Sparsity of weight matrices \\([0.0\\%, 68.5\\%, 95.75\\%, 97.75\\%, 80.0\\%]\\) \\([60.0\\%, 71.75\\%, 81.75\\%, 97.5\\%, 90.0\\%]\\) No. of nonzero entries 178 206 Table 2 : Numerical result for two dimensional function (13) and (14) with network structure \\([2,20, 20, 20, 20, 1]\\) . The numerical results presented in this subsection indicate that indeed the proposed SDNN model has an excellent adaptivity property in the sense that it generates networks with nearly the same number of nonzero weight matrix entries and the same order of approximation accuracy for functions regardless their smoothness. \u8fd9\u4e00\u5c0f\u8282\u5c55\u793a\u7684\u6570\u503c\u7ed3\u679c\u6307\u51fa\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u786e\u5b9e\u6709\u4f18\u79c0\u7684\u81ea\u9002\u5e94\u6027\u8d28, \u5373\u4e0d\u7ba1\u51fd\u6570\u5149\u6ed1\u6027\u5982\u4f55, \u6a21\u578b\u751f\u6210\u7684\u7f51\u7edc\u5177\u6709\u51e0\u4e4e\u76f8\u540c\u7684\u6743\u91cd\u533a\u95f4\u975e\u96f6\u8881\u672f\u6570\u91cf\u548c\u76f8\u540c\u8fd1\u4f3c\u7cbe\u5ea6\u7684\u9636\u6570. An Example of Adaptive Function Approximation by the SDNN Model The second experiment is designed to test the sparsity of the network learned from the SDNN model (10) and the model\u2019s generalization ability. Specifically, in this example, we demonstrate that the sparse model (10) leads to a sparse DNN with higher accuracy in comparison to the standard DNN model (9) . We consider the absolute value function \u7b2c\u4e8c\u4e2a\u5b9e\u9a8c\u65e8\u5728\u6d4b\u8bd5\u4ece SDNN \u6a21\u578b\u5b66\u4e60\u5230\u7684\u7f51\u7edc\u7684\u7a00\u758f\u6027\u548c\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b. \u5177\u4f53\u5730, \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\u6211\u4eec\u8bc1\u660e\u4e86\u7a00\u758f\u6a21\u578b\u80fd\u591f\u5bfc\u51fa\u76f8\u6bd4\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u66f4\u9ad8\u7cbe\u5ea6\u7684\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc. \u6211\u4eec\u8003\u8651\u7edd\u5bf9\u503c\u51fd\u6570 \\[ y = f(x) := |x|, \\text{for} x \\in \\mathbb{R}. \\] Note that function \\(f\\) is not differentiable at \\(x = 0\\) . \u6ce8\u610f\u51fd\u6570 \\(f\\) \u5728 \\(x=0\\) \u5904\u4e0d\u53ef\u5fae. We adopt the same network architecture, that is, \\(1\\) input layer, \\(2\\) hidden layers, \\(1\\) output layer and each hidden layer containing \\(5\\) neurons, for both the standard DNN model (9) and the SDNN model (10) . The training set is composed of equal-distance grid points laying in \\([-2, 2]\\) with step size \\(0.01\\) . The test set is composed of equal-distance grid points in \\([-5, 5]\\) with step size \\(0.1\\) . For the sparse regularized network, the regularization parameters are set as \\([10^{-4}, 10^{-3}, 10^{-3}]\\) . For both the standard DNN model and the SDNN model, the number of epoch equals \\(10,000\\) . The initial learning rate is set to \\(0.001\\) . \u6211\u4eec\u5bf9\u4e8e\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b (9) \u548c SDNN \u6a21\u578b (10) \u91c7\u7528\u76f8\u540c\u7684\u7f51\u7edc\u67b6\u6784, \u5373\u4e00\u5c42\u8f93\u5165\u5c42, \\(2\\) \u5c42\u9690\u85cf\u5c42, \\(1\\) \u5c42\u8f93\u51fa\u5c42, \u6bcf\u5c42\u9690\u85cf\u5c42\u6709 \\(5\\) \u4e2a\u795e\u7ecf\u5143. \u8bad\u7ec3\u96c6\u7531\u5728\u533a\u95f4 \\([-2,2]\\) \u4e0a\u8bbe\u7f6e\u6b65\u957f\u4e3a \\(0.01\\) \u7684\u7b49\u8ddd\u683c\u70b9\u7ec4\u6210. \u6d4b\u8bd5\u96c6\u7531\u5728\u533a\u95f4 \\([-5,5]\\) \u4e0a\u8bbe\u7f6e\u6b65\u957f\u4e3a \\(0.1\\) \u7684\u7b49\u8ddd\u683c\u70b9\u7ec4\u6210. \u5bf9\u4e8e\u7a00\u758f\u6b63\u5219\u5316\u7f51\u7edc, \u6b63\u5219\u5316\u53c2\u6570\u8bbe\u7f6e\u4e3a \\([10^{-4}, 10^{-3}, 10^{-3}]\\) . \u5bf9\u4e8e\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u548c SDNN \u6a21\u578b, epochs \u7684\u6570\u91cf\u5747\u4e3a \\(10,000\\) . \u521d\u59cb\u5b66\u4e60\u7387\u8bbe\u7f6e\u4e3a \\(0.001\\) . We present numerical results of this experiment in Table 3 , where we compare errors and sparsity of the functions learned from the two models. Clearly, the network learned from the standard DNN model is non-sparse: all entries of its weight matrices are nonzero. While the network learned from the SDNN model has a good sparsity property: There are only \\(1\\) non-zero entries in \\(W_3\\) and \\(2\\) non-zero entries in \\(W_2\\) in the network learned from the SDNN model. Note that the absolution value function is the linear composition of two ReLU functions, that is \\(|x| = \\text{ReLU}(x)+\\text{ReLU}(-x)\\) . The SDNN model is able to find a linear combination of the two functions to represent the function \\(f(x) := |x|\\) but the standard DNN model fails to do so. \u6211\u4eec\u5728\u8868\u683c 3 \u4e2d\u5c55\u793a\u4e86\u8fd9\u4e00\u5b9e\u9a8c\u7684\u6570\u503c\u7ed3\u679c, \u5176\u4e2d\u6211\u4eec\u5bf9\u6bd4\u4e86\u4ece\u4e24\u4e2a\u6a21\u578b\u5b66\u4e60\u5230\u7684\u51fd\u6570\u7684\u8bef\u5dee\u548c\u7a00\u758f\u5ea6. \u5f88\u660e\u663e, \u4ece\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\u5b66\u4e60\u5f97\u7f51\u7edc\u662f\u975e\u7a00\u758f\u7684: \u6743\u91cd\u77e9\u9635\u7684\u6240\u6709\u5143\u7d20\u90fd\u4e0d\u4e3a\u96f6. \u800c\u4ece SDNN \u6a21\u578b\u4e2d\u5b66\u4e60\u7684\u7f51\u7edc\u7531\u826f\u597d\u7684\u7a00\u758f\u6027\u8d28: \\(W_3\\) \u4e2d\u53ea\u6709\u4e00\u4e2a\u975e\u96f6\u5143\u7d20, \\(W_2\\) \u4e2d\u53ea\u6709\u4e24\u4e2a\u975e\u96f6\u5143\u7d20. \u6ce8\u610f\u7edd\u5bf9\u503c\u51fd\u6570\u662f\u4e24\u4e2a\u6574\u6d41\u7ebf\u6027\u5355\u5143\u7684\u7ebf\u6027\u7ec4\u5408, \u5373 \\(|x| = \\text{ReLU}(x)+\\text{ReLU}(-x)\\) . SDNN \u6a21\u578b\u80fd\u591f\u627e\u5230\u4e24\u4e2a\u51fd\u6570\u7684\u7ebf\u6027\u7ec4\u5408\u6765\u8868\u793a\u7edd\u5bf9\u503c\u51fd\u6570, \u800c\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5219\u4e0d\u884c. Model Relative \\(L_2\\) error Sparsity of weight matrices \\([W_1, W_2, W_3]\\) Standard DNN model \\(5.58\\times10^{-2}\\) \\([0\\%, 0\\%, 0\\%]\\) SDNN model \\(1.87\\times10^{-3}\\) \\([20\\%, 92\\%, 80\\%]\\) Table 3 : Approximation of the absolute value function by a SDNN with regularization parameters \\([10^{-4}, 10^{-3}, 10^{-3}]\\) . We plot the graphs of the reconstructed functions by the standard DNN model (9) and the SDNN model (10) in Figure 4 and Figure 5 , respectively. It can be seen from Figure 4 that the function reconstructed by the standard DNN model (9) has large errors in the interval \\([3, 5]\\) . Figure 5 shows that the function reconstructed by the SDNN model (10) almost coincides with the original function. This example indicates that the SDNN model (10) has better generalization ability than the standard DNN model (9) . \u6211\u4eec\u5206\u522b\u5728\u56fe 4 \u548c\u56fe 5 \u4e2d\u7ed8\u5236\u4e86\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u548c SDNN \u6a21\u578b\u7684\u91cd\u6784\u51fd\u6570. \u53ef\u4ee5\u4ece\u56fe 4 \u4e2d\u770b\u5230\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u91cd\u6784\u51fd\u6570\u5728\u533a\u95f4 \\([3,5]\\) \u4e0a\u6709\u8f83\u5927\u8bef\u5dee. \u56fe 5 \u5219\u5c55\u793a\u4e86 SDNN \u6a21\u578b\u548c\u539f\u59cb\u51fd\u6570\u57fa\u672c\u76f8\u7b26. \u8fd9\u4e00\u4f8b\u5b50\u8bf4\u660e\u4e86 SDNN \u6a21\u578b\u6bd4\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b. Figure 4 (Left) : Reconstruction of function \\(f(x) := |x|\\) by the standard DNN model (9) . Figure 5 (Right) : Reconstruction of function \\(f(x) := |x|\\) by the SDNN model (10) . Reconstruction of A Black Hole In this example, we consider reconstruction of the image of a black hole by the SDNN model. Specifically, we compare numerical results and reconstructed image quality of the SDNN model with those of the standard DNN model. We choose a color image of the black hole shown in Figure 6 (Left), which is turned into a gray image shown in Figure 6 (Right). The image has the size \\(128 \\times 128\\) and can be represented as a two-dimensional discrete function. The value of the gray image at the point \\((x_1, x_2)\\) is defined as a \\(f_{image}(x_1, x_2), x_1, x_2= 1, 2,\\cdots, 128\\) . The function clearly has singularities. \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d, \u6211\u4eec\u8003\u8651\u901a\u8fc7 SDNN \u6a21\u578b \u91cd\u6784\u9ed1\u6d1e\u56fe\u50cf. \u5177\u4f53\u5730, \u6211\u4eec\u5c06\u6bd4\u8f83 SDNN \u6a21\u578b\u548c\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u6570\u503c\u7ed3\u679c\u548c\u91cd\u6784\u56fe\u50cf\u8d28\u91cf. \u6211\u4eec\u9009\u62e9\u56fe 6 \u5de6\u56fe\u6240\u793a\u7684\u9ed1\u6d1e\u7684\u5f69\u8272\u56fe\u7247, \u8f6c\u5316\u4e3a\u56fe 6 \u53f3\u56fe\u6240\u793a\u7684\u7070\u5ea6\u56fe\u7247. \u56fe\u7247\u5927\u5c0f\u4e3a \\(128\\times 128\\) \u80fd\u591f\u8868\u793a\u4e3a\u4e8c\u7ef4\u79bb\u6563\u51fd\u6570. \u7070\u5ea6\u56fe\u5728 \\((x_1,x_2)\\) \u7684\u503c\u5b9a\u4e49\u4e3a \\(f_{image}(x_1, x_2)\\) , \u8fd9\u4e2a\u51fd\u6570\u663e\u7136\u5177\u6709\u5947\u6027. Figure 6 : Left: color image of the black hole. Right: gray image of the black hole. The network architecture that we used for the construction is \\([2, 100, 100, 100, 100, 100, 100, 1]\\) . We randomly choose \\(5,000\\) points \\((x^i_1, x^i_2, f_{image}(x^i_1, x^i_2))\\) by uniform sampling, \\(i =1, 2,\\cdots, 5,000\\) , from the image of the black hole to train both the standard neural network and the sparse regularized network. The optimizer is chosen as the Adam algorithm with batch size \\(1,024\\) . The number of epoch is \\(40,000\\) . The patience parameter of early stopping is \\(200\\) . Prediction results by the standard DNN model and by the SDNN model are shown respectively on Figure 7 (Left) and (Right). \u6211\u4eec\u4f7f\u7528\u7684\u7f51\u7edc\u67b6\u6784\u4e3a \\([2, 100, 100, 100, 100, 100, 100, 1]\\) . \u6211\u4eec\u4ece\u9ed1\u6d1e\u56fe\u50cf\u4e2d\u5747\u5300\u91c7\u6837 \\(5,000\\) \u4e2a\u6837\u672c\u70b9 \\((x^i_1, x^i_2, f_{image}(x^i_1, x^i_2))\\) \u7528\u4e8e\u8bad\u7ec3\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u7a00\u758f\u6b63\u5219\u5316\u7f51\u7edc. \u9009\u62e9 Adam \u7b97\u6cd5\u4f5c\u4e3a\u4f18\u5316\u5668, \u6279\u91cf\u5927\u5c0f\u4e3a \\(1024\\) . epoch \u7684\u6570\u91cf\u4e3a \\(40,000\\) . \u65e9\u505c\u6cd5\u7684\u8010\u5fc3\u53c2\u6570\u8bbe\u7f6e\u4e3a \\(200\\) , \u5373\u5141\u8bb8 \\(200\\) \u4e2a epochs \u5185\u6a21\u578b\u6027\u80fd\u6ca1\u6709\u63d0\u5347. \u56fe 7 \u7684\u5de6\u56fe\u548c\u53f3\u56fe\u5206\u522b\u5c55\u793a\u4e86\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c SDNN \u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c. Figure 7 : Images of the black hole reconstructed: by the standard DNN model (Left) and by the SDNN model (Right). Error images of the two models are presented in Figure 8 , from which it can be seen that the sparse network has a smaller reconstruction error. The prediction error of the fully connected network is \\(9.66\\times10^{-3}\\) . For the sparse regularized neural network, the regularized parameters are set to be \\([10^{-9}, 10^{-9}, 10^{-9}, 10^{-9}, 10^{-8}, 10^{-8}, 10^{-8}]\\) . The prediction error of the sparse regularized network is \\(9.28\\times10^{-3}\\) . The sparsity of the weight matrices are \\([44.0\\%, 78.3\\%, 78.4\\%, 80.3\\%, 96.5\\%, 98.2\\%, 84.0\\%]\\) . It shows that the sparse regularized network uses fewer neurons and has smaller prediction error. This indicates that by using the proposed multi-parameter sparse regularization, the deep neural network has the ability of multi-scale and adaptive learning. \u4e24\u4e2a\u6a21\u578b\u7684\u8bef\u5dee\u56fe\u50cf\u5728\u56fe 8 \u4e2d\u5c55\u793a, \u80fd\u591f\u4ece\u4e2d\u770b\u5230\u7a00\u758f\u7f51\u7edc\u5177\u6709\u66f4\u5c0f\u7684\u91cd\u6784\u8bef\u5dee. \u5168\u8fde\u63a5\u7f51\u7edc\u7684\u9884\u6d4b\u8bef\u5dee\u4e3a \\(9.66\\times10^{-3}\\) . \u5bf9\u4e8e\u7a00\u758f\u6b63\u5219\u5316\u795e\u7ecf\u7f51\u7edc, \u6b63\u5219\u5316\u53c2\u6570\u8bbe\u7f6e\u4e3a \\([10^{-9}, 10^{-9}, 10^{-9}, 10^{-9}, 10^{-8}, 10^{-8}, 10^{-8}]\\) . \u7a00\u758f\u6b63\u5219\u5316\u7f51\u7edc\u7684\u9884\u6d4b\u8bef\u5dee\u4e3a \\(9.28\\times10^{-3}\\) . \u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u4e3a \\([44.0\\%, 78.3\\%, 78.4\\%, 80.3\\%, 96.5\\%, 98.2\\%, 84.0\\%]\\) . \u8fd9\u5c55\u793a\u4e86\u7a00\u758f\u6b63\u5219\u5316\u7f51\u7edc\u4f7f\u7528\u4e86\u66f4\u5c11\u7684\u795e\u7ecf\u5143\u4e14\u83b7\u5f97\u4e86\u66f4\u5c0f\u7684\u9884\u6d4b\u8bef\u5dee. \u8fd9\u8bf4\u660e\u4e86\u901a\u8fc7\u4f7f\u7528\u6211\u4eec\u63d0\u51fa\u7684\u591a\u53c2\u6570\u7a00\u758f\u6b63\u5219\u5316, \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6709\u591a\u5c3a\u5ea6\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u80fd\u529b. Figure 8 : Reconstruction errors of the black hole: by the standard DNN model (Left) and by the SDNN model (Right). Model Relative \\(L_2\\) error Sparsity of weight matrices Standard DNN model \\(9.66\\times10^{-3}\\) \\([0\\%, 0\\%, 0\\%, 0\\%, 0\\%, 0\\%, 0\\%]\\) SDNN model \\(9.28\\times10^{-3}\\) \\([44.0\\%, 78.3\\%, 78.4\\%, 80.3\\%, 96.5\\%, 98.2\\%, 84.0\\%]\\) Table 4 : Numerical results of the black hole reconstructed by the standard DNN model vs. the SDNN model. Numerical Solutions of Partial Differential Equations We study in this section numerical performance of the proposed SDNN model for solving partial differential equations. We consider two equations: the Burgers equation and the Schr\u00f6dinger equation. For both of these two equations, we choose the hyperbolic tangent (tanh) function defined by \u672c\u8282\u6211\u4eec\u7814\u7a76\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u7528\u4e8e\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u6570\u503c\u6027\u80fd. \u6211\u4eec\u8003\u8651\u4e24\u4e2a\u65b9\u7a0b: Burgers \u65b9\u7a0b\u548c Schr\u00f6dinger \u65b9\u7a0b. \\[ \\tanh(x) := \\frac{e^x-e^{-x}}{e^x+ e^{-x}}, x \\in \\mathbb{R} \\] as the activation function to build networks for our approximate solutions due to its differentiability which is required by the differential equations. \u5bf9\u8fd9\u4e24\u4e2a\u65b9\u7a0b, \u6211\u4eec\u90fd\u4f7f\u7528\u53cc\u66f2\u6b63\u5207\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\u6765\u5efa\u7acb\u7f51\u7edc\u4ee5\u8fd1\u4f3c\u89e3\u51fd\u6570. \u8fd9\u662f\u56e0\u4e3a\u5b83\u7684\u53ef\u5fae\u5206\u6027\u6b63\u662f\u5fae\u5206\u65b9\u7a0b\u6240\u8981\u6c42\u7684. The Burgers Equation The Burgers equation has attracted much attention since it is often used as simplified model for turbulence and shock waves 31 . It is well-known that the solution of this equation presents a jump discontinuity (a shock wave), even though the initial function is smooth. Burgers \u65b9\u7a0b\u7531\u4e8e\u7ecf\u5e38\u4f5c\u4e3a\u6e4d\u6d41\u548c\u6fc0\u6ce2\u7684\u7b80\u5316\u6a21\u578b\u800c\u53d7\u5230\u5f88\u591a\u5173\u6ce8. \u4f17\u6240\u5468\u77e5\u5373\u4f7f\u521d\u59cb\u51fd\u6570\u662f\u5149\u6ed1\u7684, \u8fd9\u4e2a\u65b9\u7a0b\u7684\u89e3\u4e5f\u5b58\u5728\u8df3\u8dc3\u4e0d\u8fde\u7eed\u6027 (\u6fc0\u6ce2). In this example, we consider the following one dimensional Burgers equation \u5728\u8fd9\u4e00\u4f8b\u5b50\u4e2d, \u6211\u4eec\u8003\u8651\u5982\u4e0b\u4e00\u7ef4 Burgers \u65b9\u7a0b. \\[ \\begin{align} &u_t(t, x) + u(t, x)u_x(t, x) - \\frac{0.01}{\\pi} u_{xx}(t, x) = 0, &t \\in (0, 1], x \\in (-1, 1), \\tag{15}\\\\ &u(0, x) = - \\sin(\\pi x),\\tag{16}\\\\ &u(t,-1) = u(t, 1) = 0.\\tag{17} \\end{align} \\] The analytic solution of this equation, known in 2 , will be used as our exact solution for comparison. Indeed, the analytic solution has the form \u8fd9\u4e00\u65b9\u7a0b\u7684\u89e3\u6790\u89e3\u5c06\u4f5c\u4e3a\u6211\u4eec\u7684\u7cbe\u786e\u89e3\u4ee5\u8fdb\u884c\u5bf9\u6bd4. \u89e3\u6790\u89e3\u7684\u5f62\u5f0f\u5982\u4e0b \\[ u(t, x) := -\\frac{\\int_{-\\infty}^\\infty \\sin\\pi(x-\\eta)h(x-\\eta)\\exp(-\\eta^2/4vt)\\text{d}\\eta}{\\int_{-\\infty}^\\infty h(x-\\eta)\\exp(-\\eta^2/4vt)\\text{d}\\eta}t \\in [0, 1], x \\in [-1, 1], \\] where \\(\u03bd := \\dfrac{0.01}{\\pi}\\) and \\(h(y) := \\exp(-\\cos \\pi y/2\\pi \u03bd)\\) . A neural network solution of equation (15) - (17) was obtained recently from the standard DNN model in 35 . \u4e0a\u8ff0\u65b9\u7a0b\u7684\u795e\u7ecf\u7f51\u7edc\u89e3\u662f\u4ece PINN \u8bba\u6587\u7684\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u83b7\u5f97\u7684. We apply the setting (1) - (3) with \u6211\u4eec\u5c06\u504f\u5fae\u5206\u65b9\u7a0b\u4e00\u822c\u5f62\u5f0f\u4ee3\u5165\u53ef\u77e5 \\[ \\mathcal{F}(u(t, x)) := u_t(t, x) + u(t, x)u_x(t, x) - \\frac{0.01}{\\pi} u_{xx}(t, x), t \\in (0, 1], x \\in (-1, 1). \\] Let \\(\\{x^i_0, u^i_0\\}^{N_0}_{i=1}\\) denote the training data of \\(u\\) satisfying initial condition (16) , that is, \\(u^i_0= - \\sin(\\pi x^i_0)\\) . Let \\(\\{t^i_{b_1}\\}_{i=1}^{N_{b_1}}\\) and \\(\\{t^i_{b_2}\\}_{i=1}^{N_{b_2}}\\) be the collocation points related to boundary condition (17) for \\(x = -1\\) and \\(x = 1\\) respectively. We denote by \\(\\{t^i_f, x^i_f\\}_{i=1}^{N_f}\\) the collocation points for \\(\\mathcal{F}(u(t, x))\\) in \\([0, 1]\\times(-1, 1)\\) . The sparse deep neural network \\(\\mathcal{N}_\\Theta (t, x)\\) are learned by model (8) with \u4ee4 \\(\\{x^i_0, u^i_0\\}^{N_0}_{i=1}\\) \u8868\u793a \\(u\\) \u6ee1\u8db3\u521d\u59cb\u6761\u4ef6\u7684\u8bad\u7ec3\u6570\u636e, \u5373 \\(u^i_0= - \\sin(\\pi x^i_0)\\) . \u4ee4 \\(\\{t^i_{b_1}\\}_{i=1}^{N_{b_1}}\\) \u548c \\(\\{t^i_{b_2}\\}_{i=1}^{N_{b_2}}\\) \u5206\u522b\u8868\u793a\u4e0e\u8fb9\u754c\u6761\u4ef6 \\(x=-1\\) \u548c \\(x=1\\) \u76f8\u5173\u7684\u914d\u7f6e\u70b9. \u4ee4 \\(\\{t^i_f, x^i_f\\}_{i=1}^{N_f}\\) \u8868\u793a\u6ee1\u8db3\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u914d\u7f6e\u70b9. \u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc \\(\\mathcal{N}_\\Theta (t, x)\\) \u901a\u8fc7\u6a21\u578b (8) \u53ca\u4ee5\u4e0b\u635f\u5931\u8fdb\u884c\u5b66\u4e60. \\[ \\begin{aligned} Loss_0 &=\\frac{1}{N_0}\\sum_{i=1}^{N_0}|\\mathcal{N}_\\Theta (0, x^i_0) - u^i_0|^2,\\\\ Loss_b &=\\frac{1}{N_b}\\sum_{i=1}^{N_b}|\\mathcal{N}_\\Theta (t^i_{b_1}, -1)| +\\frac{1}{N_{b_2}}\\sum_{i=1}^{N_{b_2}}|\\mathcal{N}_\\Theta (t^i_{b_2}, 1)|. \\end{aligned} \\] In this experiment, \\(100\\) data points are randomly selected from boundary and initial data points, among which \\(N_{b_1}= 25\\) points are located on the boundary \\(x = -1\\) , \\(N_{b_2}= 23\\) points on the boundary \\(x = 1\\) , and \\(N_0= 52\\) points on the initial line \\(t = 0\\) . The distribution of random collocation points is shown in the top of Figure 9 . The number of collocation points of the partial differential equation is \\(N_f= 10,000\\) by employing the Latin hypercube sampling method. The test set is composed of grid points \\([0, 1] \\times [-1, 1]\\) uniformly discretized with step size \\(1/100\\) on the \\(t\\) direction and step size \\(2/255\\) on the \\(x\\) direction. \u5728\u8fd9\u4e2a\u5b9e\u9a8c\u4e2d, \u4ece\u8fb9\u754c\u548c\u521d\u59cb\u6570\u636e\u70b9\u968f\u673a\u9009\u62e9 \\(100\\) \u4e2a\u6570\u636e\u70b9: \\(N_{b_1}= 25\\) \u4e2a\u5728\u8fb9\u754c \\(x=-1\\) \u4e0a, \\(N_{b_2}=23\\) \u4e2a\u5728\u8fb9\u754c \\(x=1\\) \u4e0a, \\(N_0=52\\) \u4e2a\u5728\u521d\u59cb\u7ebf \\(t=0\\) \u4e0a. \u968f\u673a\u914d\u7f6e\u70b9\u7684\u5206\u5e03\u5c55\u793a\u5728\u56fe 9 \u7684\u9876\u90e8. \u504f\u5fae\u5206\u65b9\u7a0b\u7684\u914d\u7f6e\u70b9\u6570\u91cf\u4e3a \\(N_f=10,000\\) \u901a\u8fc7 LHS \u91c7\u6837\u65b9\u6cd5\u83b7\u5f97. \u6d4b\u8bd5\u96c6\u5219\u7531\u5b9a\u4e49\u57df \\([0, 1] \\times [-1, 1]\\) \u4e0a\u65f6\u95f4 \\(t\\) \u65b9\u5411\u6b65\u957f\u4e3a \\(0.01\\) , \\(x\\) \u65b9\u5411\u4e0a\u6b65\u957f\u4e3a \\(2/225\\) \u7684\u5747\u5300\u7f51\u683c\u70b9\u7ec4\u6210. Figure 9 : Burgers equation: Top: The training data and predicted solution \\(u(t, x)\\) for sparse deep neural network with \\([2, 50, 50, 50, 1]\\) , regularization parameter \\(\\alpha = [10^{-6}, 10^{-6}, 10^{-6}, 10^{-4}]\\) , \\(\\beta = 20\\) . Bottom: Predicted solution \\(u(t, x)\\) at time \\(t = 0.3\\) , \\(t = 0.6\\) , and \\(t = 0.8\\) . We use two different network architectures \\([2, 50, 50, 50, 1]\\) and \\([2, 50, 50, 50,50, 50, 50, 50, 1]\\) for DNNs. We choose Adam as the optimizer for both neural networks. The number of epoch is \\(30,000\\) . The initial learning rate is set to \\(0.001\\) . Numerical results of these two networks presented respectively in Table 5 and Table 6 show that the proposed SDNN model outperforms the PINN model in both weight matrix sparsity and approximation accuracy. \u6211\u4eec\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784 \\([2, 50, 50, 50, 1]\\) \u548c \\([2, 50, 50, 50,50, 50, 50, 50, 1]\\) . \u6211\u4eec\u9009\u62e9 Adam \u4f5c\u4e3a\u4f18\u5316\u5668. epochs \u7684\u6570\u91cf\u4e3a \\(30,000\\) . \u521d\u59cb\u5b66\u4e60\u7387\u8bbe\u7f6e\u4e3a \\(0.001\\) . \u8fd9\u4e24\u4e2a\u7f51\u7edc\u7684\u6570\u503c\u7ed3\u679c\u5206\u522b\u5c55\u793a\u5728\u8868\u683c 5 \u548c\u8868\u683c 6 \u91cc. \u8868\u683c\u8bf4\u660e\u4e86\u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u5728\u6743\u91cd\u77e9\u9635\u7a00\u758f\u5ea6\u548c\u8fd1\u4f3c\u8fdb\u5ea6\u4e0a\u90fd\u6bd4 PINN \u6a21\u578b\u4f18\u8d8a. Algorithms Parameters \\(\\alpha\\) & sparsity of weight matrices Relative L2error PINN No regularization \\([0.0\\%, 0.2\\%, 0.5\\%, 0.0\\%]\\) \\(2.45\\times10^{-2}\\) SDNN ( \\(\\beta=20\\) ) \\([10^{-6}, 10^{-6}, 10^{-6}, 10^{-4}]\\) \\([13.0\\%, 61.9\\%, 71.2\\%, 62.0\\%]\\) \\(1.68\\times10^{-3}\\) Table 5 : The Burgers equation: A neural network of \\(4\\) layers, with network architecture \\([2, 50,50, 50, 1]\\) . Algorithms Parameters \\(\\alpha\\) & sparsity of weight matrices Relative L2error PINN No regularization \\([0.0\\%, 0.8\\%, 0.6\\%, 0.6\\%, 0.8\\%, 0.6\\%, 0.4\\%, 0.0\\%]\\) \\(3.39\\times10^{-3}\\) SDNN ( \\(\\beta=10\\) ) \\([0, 0, 0, 0, 10^{-7}, 10^{-1}0, 10^{-6}, 10^{-5}]\\) \\([0.0\\%, 0.7\\%, 0.8\\%, 0.7\\%, 15.6\\%, 0.5\\%, 93.8\\%, 94.0\\%]\\) \\(1.45\\times10^{-4}\\) SDNN ( \\(\\beta=10\\) ) \\([10^{-6}, 10^{-6}, 10^{-6}, 10^{-6}, 10^{-6}, 10^{-6}, 10^{-5}, 10^{-5}]\\) \\([25.0\\%, 78.6\\%, 85.3\\%, 82.8\\%, 79.5\\%, 84.0\\%, 98.6\\%, 94.0\\%]\\) \\(4.83\\times10^{-4}\\) Table 6 : The Burgers equation: Neural networks of \\(8\\) layers with network architecture \\([2, 50, 50,50, 50, 50, 50, 50, 1]\\) . The Schr\u00f6dinger Equation The Schr\u00f6dinger equation is the most essential equation of non-relativistic quantum mechanics. It plays an important role in studying nonlinear optics, Bose-Einstein condensates, protein folding and bending. It is also a model equation for studying waves propagation and soliton 36 . In this subsection, we consider a one-dimensional Schr\u00f6dinger equation with periodic boundary conditions Schr\u00f6dinger \u65b9\u7a0b\u662f\u975e\u76f8\u5bf9\u8bba\u91cf\u5b50\u529b\u5b66\u7684\u6700\u57fa\u672c\u7684\u65b9\u7a0b. \u5b83\u5728\u7814\u7a76\u975e\u7ebf\u6027\u5149\u5b66, Bose-Einstein \u51dd\u805a, \u86cb\u767d\u8d28\u6298\u53e0\u548c\u5f2f\u66f2\u65b9\u9762\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528. \u5b83\u4e5f\u662f\u7814\u7a76\u6ce2\u4f20\u64ad\u548c\u5b64\u5b50\u7684\u4e00\u4e2a\u6a21\u578b\u65b9\u7a0b. \u5728\u8fd9\u4e00\u5c0f\u8282\u4e2d, \u6211\u4eec\u8003\u8651\u5e26\u6709\u5468\u671f\u8fb9\u503c\u6761\u4ef6\u7684\u4e00\u7ef4 Schr\u00f6dinger \u65b9\u7a0b. \\[ \\begin{aligned} &iu_t(t, x) + 0.5u_{xx}(t, x) + |u(t, x)|^2u(t, x) = 0, t \\in (0, \\pi/2], x \\in (-5, 5), \\\\ &u(0, x) = 2 \\text{sech}(x),\\\\ &u(t, -5) = u(t, 5), \\\\ &u_x(t, -5) = u_x(t, 5).\\tag{18} \\end{aligned} \\] Note that the solution \\(u\\) of problem (18) is a complex-valued function. The goal of this study is to test the effectiveness of the proposed SDNN model in solving complex-valued nonlinear differential equations with periodic boundary conditions, with a comparison to the standard DNN model recently developed in 35 . \u6ce8\u610f\u5230\u8fd9\u4e00\u95ee\u9898\u7684\u89e3 \\(u\\) \u662f\u4e00\u4e2a\u590d\u503c\u51fd\u6570. \u8fd9\u4e00\u7814\u7a76\u7684\u76ee\u7684\u662f\u6d4b\u8bd5\u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u5728\u6c42\u89e3\u5e26\u6709\u5468\u671f\u8fb9\u503c\u6761\u4ef6\u7684\u590d\u503c\u975e\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u5f97\u5230\u6709\u6548\u6027, \u5e76\u4e0e\u57fa\u4e8e PINN \u7684\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83. Problem (18) falls into the setting (1) - (3) with \u4e0a\u8ff0\u95ee\u9898\u4ee3\u5165\u4e00\u822c\u504f\u5fae\u5206\u65b9\u7a0b\u8bbe\u7f6e\u53ef\u77e5 \\[ \\mathcal{F}(u(t, x)) := iu_t(t, x) + 0.5u_{xx}(t, x) + |u(t, x)|^2u(t, x), t \\in (0, \\pi/2], x \\in (-5, 5). \\] Let \\(\\psi\\) and \\(\\phi\\) be respectively the real part and imaginary part of the solution \\(u\\) of problem (18) . We intend to approximate the solution \\(u(t, x)\\) by a neural network \\(\\mathcal{N}_\\Theta (t, x)\\) with two inputs \\((t, x)\\) and two outputs which approximate \\(\\psi(t, x)\\) and \\(\\phi(t, x)\\) , respectively. Let \\(\\{x^i_0, u^i_0\\}^{N_0}_{i=1}\\) denote the training data to enforce the initial condition at time \\(t = 0\\) , that is, \\(u^i_0= \\text{sech}(x^i_0)\\) , \\(\\{t^i_b\\}^{N_b}_{i=1}\\) the collocation points on the boundary \\(x = -5\\) and \\(x = 5\\) to enforce the periodic boundary conditions, and \\(\\{t^i_f, x^i_f\\}^{N_f}_{i=1}\\) the collocation points in \\((0, \\pi/2]\\times (-5, 5)\\) . These collocation points were generated by the Latin hypercube sampling method. We then learn the neural network \\(\\mathcal{N}_\\Theta (t, x)\\) by model (8) with \u4ee4 \\(\\psi\\) \u548c \\(\\phi\\) \u5206\u522b\u8868\u793a\u89e3 \\(u\\) \u7684\u5b9e\u90e8\u548c\u865a\u90e8. \u6211\u4eec\u8bd5\u56fe\u7528\u5177\u6709\u4e24\u4e2a\u8f93\u5165 \\((t, x)\\) \u548c\u4e24\u4e2a\u8f93\u51fa\u5206\u522b\u8fd1\u4f3c \\(\\psi(t, x)\\) \u548c \\(\\phi(t, x)\\) \u7684\u795e\u7ecf\u7f51\u7edc \\(\\mathcal{N}_\\Theta (t, x)\\) \u6765\u8fd1\u4f3c\u65b9\u7a0b\u7684\u89e3 \\(u(t, x)\\) . \u4ee4 \\(\\{x^i_0, u^i_0\\}^{N_0}_{i=1}\\) \u8868\u793a\u7528\u4e8e\u6ee1\u8db3 \\(t=0\\) \u4e0a\u521d\u503c\u6761\u4ef6\u7684\u8bad\u7ec3\u6570\u636e, \u5373 \\(u^i_0= \\text{sech}(x^i_0)\\) , \\(\\{t^i_b\\}^{N_b}_{i=1}\\) \u8868\u793a\u7528\u4e8e\u6ee1\u8db3 \\(x=-5\\) \u548c \\(x=5\\) \u4e0a\u5468\u671f\u8fb9\u503c\u6761\u4ef6\u7684\u914d\u7f6e\u70b9. \\(\\{t^i_f, x^i_f\\}^{N_f}_{i=1}\\) \u8868\u793a\u5728 \\((0, \\pi/2]\\times (-5, 5)\\) \u7684\u914d\u7f6e\u70b9. \u8fd9\u4e9b\u914d\u7f6e\u70b9\u901a\u8fc7 LHS \u65b9\u6cd5\u751f\u6210. \u7136\u540e\u6211\u4eec\u901a\u8fc7\u5177\u6709\u4ee5\u4e0b\u635f\u5931\u7684\u6a21\u578b (8) \u6765\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc \\(\\mathcal{N}_\\Theta (t, x)\\) . \\[ \\begin{aligned} Loss_0&:= \\frac{1}{N_0}\\sum_{i=1}^{N_0} |\\mathcal{N}_\\Theta(0,x^i_0)-u^i_0|^2\\\\ Loss_b&:= \\frac{1}{N_b}\\sum_{i=1}^{N_b} \\bigg(|\\mathcal{N}_\\Theta(t^i_b, -5)- \\mathcal{N}_\\Theta(t^i_b, 5)|^2+\\Big|\\frac{\\partial \\mathcal{N}_\\Theta}{\\partial x}(t^i_b, -5)-\\frac{\\partial \\mathcal{N}_\\Theta}{\\partial x}(t^i_b, 5)\\Big|^2\\bigg) \\end{aligned} \\] A reference solution of problem (18) is solved by a Fourier spectral method using the Chebfun package 18 . Specifically, we obtain the reference solution by using \\(256\\) Fourier modes for space discretization and an explicit fourth-order Runge-Kutta method (RK4) with time-step \\(\\Delta t := (\\pi/2) \\times 10^{-6}\\) for time discretization. For more details of the discretization of Schr\u00f6dinger equation (18) , the readers are referred to 35 . For both the standard network and the sparse network, we used the network architecture \\([2, 50, 50, 50, 50, 50, 50, 2]\\) . Both the networks were trained by the Adam algorithm with \\(30,000\\) epochs. The initial learning rate is set to \\(0.001\\) . The training set is composed of \\(N_0:= 50\\) data points on \\(u(0, x)\\) , \\(N_b:= 50\\) sample points for enforcing the periodic boundaries, and \\(N_f:= 20,000\\) sample points inside the solution domain of equation (18) . The test set is composed of grid points \\((0, \\pi/2] \\times [-5, 5]\\) uniformly discretized with step size \\(\\pi/400\\) on the \\(t\\) direction and step size \\(10/256\\) on the \\(x\\) direction. \u95ee\u9898 (18) \u7684\u53c2\u7167\u89e3\u662f\u901a\u8fc7 Chebfun \u5305\u7684\u5085\u7acb\u53f6\u8c31\u65b9\u6cd5\u6c42\u89e3\u7684. \u5177\u4f53\u5730, \u6211\u4eec\u901a\u8fc7\u5728\u7a7a\u95f4\u79bb\u6563\u4e0a\u4f7f\u7528 \\(256\\) \u4e2a\u5085\u7acb\u53f6\u6a21\u548c\u65f6\u95f4\u79bb\u6563\u6b65\u957f\u4e3a \\(\\Delta t := (\\pi/2) \\times 10^{-6}\\) \u7684\u663e\u5f0f\u56db\u9636\u9f99\u683c\u5e93\u5854\u65b9\u6cd5. Schr\u00f6dinger \u65b9\u7a0b\u7684\u66f4\u591a\u79bb\u6563\u7ec6\u8282\u53ef\u4ee5\u53c2\u9605 PINN \u8bba\u6587. \u5bf9\u4e8e\u6807\u51c6\u7f51\u7edc\u548c\u7a00\u758f\u7f51\u7edc, \u6211\u4eec\u4f7f\u7528\u7f51\u7edc\u67b6\u6784\u4e3a \\([2, 50, 50, 50, 50, 50, 50, 2]\\) , \u90fd\u4f7f\u7528 Adam \u7b97\u6cd5\u8bad\u7ec3 \\(30,000\\) \u4e2a epochs. \u521d\u59cb\u5b66\u4e60\u7387\u8bbe\u7f6e\u4e3a \\(0.001\\) . \u8bad\u7ec3\u96c6\u7531 \\(u(0,x)\\) \u4e0a\u7684 \\(N_0=50\\) \u4e2a\u6570\u636e\u70b9, \u7528\u4e8e\u6ee1\u8db3\u5468\u671f\u8fb9\u503c\u6761\u4ef6\u7684 \\(N_b=50\\) \u6837\u672c\u70b9\u548c\u65b9\u7a0b\u5b9a\u4e49\u57df\u5185\u7684 \\(N_f=20,000\\) \u4e2a\u6837\u672c\u70b9\u7ec4\u6210. \u6d4b\u8bd5\u96c6\u5219\u7531\u5b9a\u4e49\u57df \\((0, \\pi/2] \\times [-5, 5]\\) \u6cbf \\(t\\) \u65b9\u5411\u4e0a\u5747\u5300\u79bb\u6563\u6b65\u957f\u4e3a \\(\\pi/400\\) \u548c\u6cbf \\(x\\) \u65b9\u5411\u4e0a\u5747\u5300\u79bb\u6563\u6b65\u957f\u4e3a \\(10/256\\) \u7684\u7f51\u683c\u70b9\u7ec4\u6210. Numerical results for this example are listed in Table 7 . As we can see, the sparse network has a smaller prediction error than the standard network. When regularization parameters \\(\\alpha = [0, 0, 0, 0, 5\\times10^{-7}, 10^{-6}, 10^{-5}]\\) , the relative \\(L_2\\) error is smaller than the PINN method. When regularization parameters \\(\\alpha\\) are taken as \\([9\\times10^{-7}, 5\\times10^{-7}, 6\\times10^{-7}, 7\\times10^{-7}, 8\\times10^{-7}, 10^{-6}, 10^{-5}]\\) , the sparsity of weight matrices are \\([22.0\\%,50.5\\%, 51.9\\%, 50.6\\%, 50.0\\%, 64.5\\%, 66.0\\%]\\) . In other words, after removing more than half of the neural network connections, the sparse neural network still has a slightly higher prediction accuracy. The predicted solution of the SDNN is illustrated in Figure 10 . These numerical results clearly confirm that the proposed SDNN model outperforms the standard DNN model. \u8fd9\u4e00\u4f8b\u5b50\u7684\u6570\u503c\u7ed3\u679c\u5217\u4e3e\u5728\u8868\u683c 7 \u4e2d. \u6b63\u5982\u6211\u4eec\u6240\u770b\u5230\u7684, \u7a00\u758f\u7f51\u7edc\u76f8\u6bd4\u6807\u51c6\u7f51\u7edc\u5177\u6709\u66f4\u5c0f\u7684\u9884\u6d4b\u8bef\u5dee. \u5f53\u6b63\u5219\u5316\u53c2\u6570 \\(\\alpha = [0, 0, 0, 0, 5\\times10^{-7}, 10^{-6}, 10^{-5}]\\) , \u76f8\u5bf9 \\(L_2\\) \u8bef\u5dee\u6bd4 PINN \u7684\u8981\u5c0f. \u5f53\u6b63\u5219\u5316\u53c2\u6570\u4e3a \\([9\\times10^{-7}, 5\\times10^{-7}, 6\\times10^{-7}, 7\\times10^{-7}, 8\\times10^{-7}, 10^{-6}, 10^{-5}]\\) , \u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u4e3a \\([22.0\\%,50.5\\%, 51.9\\%, 50.6\\%, 50.0\\%, 64.5\\%, 66.0\\%]\\) . \u6362\u53e5\u8bdd\u8bf4, \u79fb\u9664\u4e86\u795e\u7ecf\u7f51\u7edc\u8d85\u8fc7\u4e00\u534a\u7684\u8fde\u63a5\u540e, \u7a00\u758f\u795e\u7ecf\u7f51\u7edc\u4ecd\u7136\u7531\u7a0d\u5fae\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6. SDNN \u7684\u9884\u6d4b\u89e3\u5982\u56fe 10 \u6240\u793a. \u8fd9\u4e9b\u6570\u503c\u7ed3\u679c\u6e05\u6670\u5730\u8bc1\u5b9e\u4e86\u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u6bd4\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u8d8a. Algorithms Parameters \\(\\alpha\\) & sparsity of weight matrices Relative \\(L_2\\) error PINN No regularization \\([0.0\\%, 0.3\\%, 0.4\\%, 0.6\\%, 0.4\\%, 0.68\\%, 0.0\\%]\\) \\(1.41\\times10^{-3}\\) SDNN ( \\(\\beta = 10\\) ) \\([0, 0, 0, 0, 5\\times10^{-7}, 10^{-6}, 10^{-5}]\\) \\([0.7\\%, 0.3\\%, 0.4\\%, 50.6\\%, 38.0\\%, 74.7\\%, 77.0\\%]\\) \\(8.15\\times10^{-4}\\) SDNN ( \\(\\beta = 10\\) ) \\([9\\times10^{-7}, 5\\times10^{-7}, 6\\times10^{-7}, 7\\times10^{-7}, 8\\times10^{-7}, 10^{-6}, 10^{-5}]\\) \\([22.0\\%, 50.5\\%, 51.9\\%, 50.6\\%, 50.0\\%, 64.5\\%, 66.0\\%]\\) \\(1.38\\times10^{-3}\\) Table 7 : The Schr\u00f6dinger equation: The neural network of \\(7\\) layers with network architecture \\([2,50, 50, 50, 50, 50, 50, 2]\\) . Figure 10 : The Schr\u00f6dinger equation: Top: The training data and predicted solution \\(|u(t, x)|\\) by SDNN with network architecture \\([2, 50, 50, 50, 50, 50, 50, 2]\\) , regularization parameters \\(\\alpha := [9\\times10^{-7}, 5\\times10^{-7}, 6\\times10^{-7}, 7\\times10^{-7}, 8\\times10^{-7}, 10^{-6}, 10^{-5}]\\) , and \\(\\beta := 10\\) . Bottom: Predicted solutions at time \\(t := 0.55\\) , \\(t := 0.79\\) , and \\(t := 1.02\\) . Conclusion A sparse network requires less memory and computing time to operate it and thus it is desirable. We have developed a sparse deep neural network model by employing a sparse regularization with multiple parameters for solving nonlinear partial differential equations. Noticing that neural networks are layer-by-layer composite structures with an intrinsic multi-scale structure, we observe that the network weights of different layers have different weights of importance. Aiming at generating a sparse network structure while maintaining approximation accuracy, we proposed to impose different regularization parameters on different layers of the neural network. We first tested the proposed sparse regularization model in approximation of singular functions, and discovered that the proposed model can not only generate an adaptive approximation of functions having singularities but also have better generalization than the standard network. We then developed a sparse deep neural network model for solving nonlinear partial differential equations whose solutions may have certain singularities. Numerical examples show that the proposed model can remove redundant network connections leading to sparse networks and has better generalization ability. Theoretical investigation will be performed in a follow-up paper. \u7a00\u758f\u7f51\u7edc\u53ea\u9700\u8981\u66f4\u5c11\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u65f6\u95f4\u6765\u64cd\u4f5c\u5b83, \u56e0\u6b64\u5b83\u662f\u53ef\u53d6\u7684. \u6211\u4eec\u91c7\u7528\u591a\u53c2\u6570\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6cd5\u5efa\u7acb\u4e86\u6c42\u89e3\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b. \u6ce8\u610f\u5230\u795e\u7ecf\u7f51\u7edc\u662f\u5177\u6709\u5185\u5728\u591a\u5c3a\u5ea6\u7ed3\u6784\u7684\u5c42\u5c42\u590d\u5408\u7ed3\u6784, \u6211\u4eec\u89c2\u5bdf\u5230\u4e0d\u540c\u5c42\u7684\u7f51\u7edc\u6743\u91cd\u6709\u4e0d\u540c\u7684\u91cd\u8981\u6027\u6743\u91cd. \u4e3a\u4e86\u5728\u4fdd\u6301\u903c\u8fd1\u7cbe\u5ea6\u7684\u540c\u65f6\u751f\u6210\u7a00\u758f\u7f51\u7edc\u7ed3\u6784, \u6211\u4eec\u63d0\u51fa\u5728\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u540c\u5c42\u4e0a\u52a0\u5165\u4e0d\u540c\u7684\u6b63\u5219\u5316\u53c2\u6570. \u9996\u5148\u5bf9\u7a00\u758f\u6b63\u5219\u5316\u6a21\u578b\u5728\u5947\u6027\u51fd\u6570\u903c\u8fd1\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u6d4b\u8bd5, \u53d1\u73b0\u8be5\u6a21\u578b\u4e0d\u4ec5\u80fd\u81ea\u9002\u5e94\u903c\u8fd1\u5177\u6709\u5947\u6027\u7684\u51fd\u6570, \u800c\u4e14\u6bd4\u6807\u51c6\u7f51\u7edc\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b. \u7136\u540e, \u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6765\u89e3\u51b3\u89e3\u6709\u4e00\u5b9a\u7684\u5947\u6027\u7684\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b. \u6570\u503c\u7b97\u4f8b\u8868\u660e, \u8be5\u6a21\u578b\u80fd\u591f\u53bb\u9664\u5197\u4f59\u7f51\u7edc\u8fde\u63a5\u5f97\u5230\u7a00\u758f\u7f51\u7edc, \u5177\u6709\u8f83\u597d\u7684\u6cdb\u5316\u80fd\u529b. \u7406\u8bba\u7814\u7a76\u5c06\u5728\u540e\u7eed\u8bba\u6587\u4e2d\u8fdb\u884c. References \u672a\u88ab\u5b9e\u9645\u5f15\u7528\u7684\u6587\u732e . [3] Optimal Approximation with Sparsely Connected Deep Neural Networks, [2019] [SIAM] [15] Stochastic Subgradient Method Converges On Tame Functions. [2020]. [21] Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. [2011]. The Gap between Theory and Practice in Function Approximation with Deep Neural Networks. [2021] [SIAM]. \u21a9 Spectral and Finite Difference Solutions of the Burgers Equation [1986]. \u21a9 Multiparameter Regularization for Volterra Kernel Identification Via Multiscale Collocation Methods. [2009] Y. Xu . \u21a9 Robust Uncertainty Principles: Exact Signal Reconstruction from Highly Incomplete Frequency Information [2006]. \u21a9 An Introduction to Compressive Sampling. [2008]. \u21a9 Multi-Parameter Tikhonov Regularization for Linear Ill-Posed Operator Equations [2008] Y. Xu . \u21a9 Multiscale Methods for Fredholm Integral Equations. [2015] Y. Xu . \u21a9 Deep Learning Networks for Stock Market Analysis and Prediction: Methodology, Data Representations, and Case Studies. [2017]. \u21a9 Approximation by Superpositions of A Sigmoidal Function. [1989]. \u21a9 \u21a9 Robust Training and Initialization of Deep Neural Networks: An Adaptive Basis Viewpoint. [2020]. \u21a9 \u21a9 Context-Dependent Pretrained Deep Neural Networks for Large-Vocabulary Speech Recognition. [2012]. \u21a9 (\u5c0f\u6ce2\u5341\u8bb2) Ten Lectures on Wavelets, [1992]. \u21a9 Nonlinear Approximation and (Deep) ReLU Networks. [2021]. \u21a9 (BERT) BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding . [2018] [arXiv:1810.04805v1] \u21a9 Neural Network Approximation. [2021]. \u21a9 Deeply Learning Deep Inelastic Scattering Kinematics. [2021] Y. Xu , [arxiv:2108.11638v1] \u21a9 Compressive Sensing. [2006]. \u21a9 Chebfun Guide. [2014]. \u21a9 Hierarchical Models in the Brain, [2008]. \u21a9 Deep Learning . [2016] [MIT Press]. \u21a9 (DeepBSDE) Solving High-Dimensional Partial Differential Equations Using Deep Learning . [2018] [PNAS]. \u21a9 ReLU Deep Neural Networks and Linear Finite Elements. [2020]. \u21a9 Latin Hypercube Sampling and the Propagation of Uncertainty in Analyses of Complex Systems. [2003]. \u21a9 The Future of Deep Learning Will Be Sparse. [2021]. \u21a9 \u21a9 Sparsity in Deep Learning: Pruning and Growth for Efficient Inference and Training in Neural Networks . [2021]. \u21a9 \u21a9 Deep Learned Finite Elements, [2020]. \u21a9 Adam: A Method for Stochastic Optimization . [2015] [ICLR]. \u21a9 Imagenet Classification with Deep Convolutional Neural Networks. [2012]. \u21a9 Artificial Neural Networks for Solving Ordinary and Partial Differential Equations. [1998]. \u21a9 Neural-Network Methods for Boundary Value Problems with Irregular Boundaries. [2000]. \u21a9 Nonlinear Stability of An Undercompressive Shock for Complex Burgers Equation. [1995] \u21a9 Multi-Parameter Regularization Methods for High-Resolution Image Reconstruction with Displacement Errors. [2007] Y. Xu . \u21a9 Using the Matrix Refinement Equation for the Construction of Wavelets on Invariant Sets. [1994] Y. Xu . \u21a9 (DHM) Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations . [2018]. \u21a9 (PINN) Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations . [2019] [JCP]. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Novel Soliton Solutions of the Nonlinear Schr\u00f6dinger Equation Model. [2000]. \u21a9 A Representer Theorem for Deep Neural Networks. [2019]. \u21a9 Sparse Regularization with the \\(l_0\\) -norm . [2021] Y. Xu , arXiv:2111.08244. \u21a9 Generalized Mercer Kernels and Reproducing Kernel Banach Spaces. [2019] Y. Xu . \u21a9 Convergence of Deep ReLU Networks. [2021] Y. Xu , arXiv:2107.12530. \u21a9 \u21a9 Convergence of Deep Convolutional Neural Networks. [2021] Y. Xu , arXiv:2109.13542. \u21a9 \u21a9 Reproducing Kernel Banach Spaces for Machine Learning. [2009] Y. Xu . \u21a9 \u21a9","title":"Sparse Deep Neural Network for Nonlinear Partial Differential Equations"},{"location":"Scholars/PN-2207.13266/#sparse-deep-neural-network-for-nonlinear-partial-differential-equations","text":"\u4f5c\u8005: \u8bb8\u8dc3\u751f, Zeng Taishan \u94fe\u63a5: arXiv:2207.13266v1 \u65f6\u95f4: 2022-07-27 \u6807\u7b7e: Sparse Approximation, \u6df1\u5ea6\u5b66\u4e60, \u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b, Sparse Regularization, Adaptive Approximation \u76ee\u5f55 [Toc] author{ color: red; }","title":"Sparse Deep Neural Network for Nonlinear Partial Differential Equations"},{"location":"Scholars/PN-2207.13266/#abstarct","text":"More competent learning models are demanded for data processing due to increasingly greater amounts of data available in applications. Data that we encounter often have certain embedded sparsity structures. That is, if they are represented in an appropriate basis, their energies can concentrate on a small number of basis functions. This paper is devoted to a numerical study of adaptive approximation of solutions of nonlinear partial differential equations whose solutions may have singularities, by deep neural networks (DNNs) with a sparse regularization with multiple parameters. Noting that DNNs have an intrinsic multi-scale structure which is favorable for adaptive representation of functions, by employing a penalty with multiple parameters, we develop DNNs with a multi-scale sparse regularization ( SDNN ) for effectively representing functions having certain singularities. We then apply the proposed SDNN to numerical solutions of the Burgers equation and the Schr\u00f6dinger equation. Numerical examples confirm that solutions generated by the proposed SDNN are sparse and accurate. \u7531\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u7528\u7684\u6570\u636e\u8d8a\u6765\u8d8a\u591a, \u56e0\u6b64\u9700\u8981\u66f4\u6709\u80fd\u529b\u7684\u5b66\u4e60\u6a21\u578b\u6765\u8fdb\u884c\u6570\u636e\u5904\u7406. \u6211\u4eec\u9047\u5230\u7684\u6570\u636e\u901a\u5e38\u5177\u6709\u67d0\u79cd\u5d4c\u5165\u5f0f\u7a00\u758f\u7ed3\u6784. \u4e5f\u5c31\u662f\u8bf4, \u5982\u679c\u7528\u9002\u5f53\u7684\u57fa\u51fd\u6570\u8868\u793a\u5b83\u4eec, \u5b83\u4eec\u7684\u80fd\u91cf\u5c31\u53ef\u4ee5\u96c6\u4e2d\u5728\u5c11\u91cf\u7684\u57fa\u51fd\u6570\u4e0a. \u672c\u6587\u5229\u7528\u5177\u6709\u591a\u53c2\u6570\u7a00\u758f\u6b63\u5219\u5316\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc, \u5bf9\u89e3\u5177\u6709\u5947\u5f02\u6027\u7684\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u89e3\u7684\u81ea\u9002\u5e94\u903c\u8fd1\u8fdb\u884c\u4e86\u6570\u503c\u7814\u7a76. \u6ce8\u610f\u5230\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u5185\u5728\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784, \u6709\u5229\u4e8e\u51fd\u6570\u7684\u81ea\u9002\u5e94\u8868\u793a, \u901a\u8fc7\u4f7f\u7528\u591a\u53c2\u6570\u60e9\u7f5a, \u6211\u4eec\u5efa\u7acb\u4e86\u5177\u6709\u591a\u5c3a\u5ea6\u7a00\u758f\u6b63\u5219\u5316\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (SDNN) \u6765\u6709\u6548\u5730\u8868\u793a\u5177\u6709\u67d0\u4e9b\u5947\u5f02\u6027\u7684\u51fd\u6570. \u7136\u540e\u6211\u4eec\u5c06\u6240\u63d0\u51fa\u7684 SDNN \u5e94\u7528\u4e8e\u6c42\u89e3 Burgers \u65b9\u7a0b\u548c Schr\u00f6dinger \u65b9\u7a0b\u7684\u6570\u503c\u89e3. \u6570\u503c\u7b97\u4f8b\u8868\u660e, \u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u751f\u6210\u7684\u89e3\u662f\u7a00\u758f\u4e14\u51c6\u786e\u7684.","title":"Abstarct"},{"location":"Scholars/PN-2207.13266/#introduction","text":"The goal of this paper is to develop a sparse regularization deep neural network model for numerical solutions of nonlinear partial differential equations whose solutions may have singularities. We will mainly focus on designing a sparse regularization model by employing multiple parameters to balance sparsity of different layers and the overall accuracy. The proposed ideas are tested in this paper numerically to confirm our intuition and more in-depth theoretical studies will be followed in a future paper. \u672c\u6587\u7684\u76ee\u7684\u662f\u5efa\u7acb\u4e00\u4e2a\u7a00\u758f\u6b63\u5219\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b, \u7528\u4e8e\u6c42\u89e3\u90a3\u4e9b\u53ef\u80fd\u5177\u6709\u5947\u5f02\u6027\u7684\u89e3\u7684\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u6570\u503c\u89e3. \u6211\u4eec\u5c06\u7740\u91cd\u4e8e\u8bbe\u8ba1\u4e00\u4e2a\u7a00\u758f\u6b63\u5219\u5316\u6a21\u578b, \u91c7\u7528\u591a\u4e2a\u53c2\u6570\u6765\u5e73\u8861\u4e0d\u540c\u5c42\u7684\u7a00\u758f\u6027\u548c\u6574\u4f53\u7cbe\u5ea6. \u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u7684\u76f4\u89c9, \u672c\u6587\u5bf9\u6240\u63d0\u51fa\u7684\u89c2\u70b9\u8fdb\u884c\u4e86\u6570\u503c\u5b9e\u9a8c, \u5e76\u5c06\u5728\u4eca\u540e\u7684\u8bba\u6587\u4e2d\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u7406\u8bba\u7814\u7a76. Artificial intelligence especially deep neural networks (DNN) has received great attention in many research fields. From the approximation theory point of view, a neural network is built by functional composition to approximate a continuous function with arbitrary accuracy. Deep neural networks are proven to have better approximation by practice and theory due to their relatively large number of hidden layers. Deep neural network has achieved state-of-the-art performance in a wide range of applications, including speech recognition 11 , computer vision 28 , natural language processing 14 , and finance 8 . For an overview of deep learning the readers are referred to monograph 20 . Recently, there was great interest in applying deep neural networks to the field of scientific computing, such as discovering the differential equations from observed data 34 , solving the partial differential equation (PDE) 21 29 30 35 , and problem aroused in physics 16 . Mathematical understanding of deep neural networks received much attention in the applied mathematics community. A universal approximation theory of neural network for Borel measurable function on compact domain is established in 9 . Some recent research studies the expressivity of deep neural networks for different function spaces 15 , for example, Sobolev spaces, Barron functions, and H\u00f6lder spaces. There are close connections between deep neural network and traditional approximation methods, such as splines 13 37 , compressed sensing 1 , and finite elements 22 26 . Convergence of deep neural networks and deep convolutional neural networks are studied in 40 and 41 respectively. Some work aims at understanding the training process of DNN. For instance, in paper 10 , the training process of DNN is interpreted as learning adaptive basis from data. \u4eba\u5de5\u667a\u80fd, \u7279\u522b\u662f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5df2\u7ecf\u5728\u8bb8\u591a\u9886\u57df\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u5173\u6ce8. \u4ece\u903c\u8fd1\u7406\u8bba\u7684\u89d2\u5ea6\u6765\u770b, \u795e\u7ecf\u7f51\u7edc\u662f\u901a\u8fc7\u51fd\u6570\u7ec4\u5408\u6765\u903c\u8fd1\u4efb\u610f\u7cbe\u5ea6\u7684\u8fde\u7eed\u51fd\u6570. \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7531\u4e8e\u5176\u76f8\u5bf9\u8f83\u5927\u7684\u9690\u5c42\u6570, \u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u88ab\u8bc1\u660e\u5177\u6709\u8f83\u597d\u7684\u903c\u8fd1\u6027\u80fd. \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bed\u97f3\u8bc6\u522b, \u8ba1\u7b97\u673a\u89c6\u89c9, \u81ea\u7136\u8bed\u8a00\u5904\u7406 (BERT) \u548c\u91d1\u878d\u7b49\u5e7f\u6cdb\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd. \u5bf9\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6982\u8ff0, \u8bfb\u8005\u53ef\u4ee5\u53c2\u8003\u4e13\u8457 Deep Learning . \u6700\u8fd1, \u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u4e8e\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u5f15\u8d77\u4e86\u6781\u5927\u7684\u5174\u8da3, \u4f8b\u5982\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u53d1\u73b0\u5fae\u5206\u65b9\u7a0b, \u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b, \u4ee5\u53ca\u7269\u7406\u5b66\u4e2d\u51fa\u73b0\u7684\u76f8\u5173\u95ee\u9898. \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u5b66\u7406\u89e3\u5728\u5e94\u7528\u6570\u5b66\u754c\u5907\u53d7\u5173\u6ce8. \u6587\u732e 9 \u4e2d\u5efa\u7acb\u4e86\u4e00\u4e2a\u7528\u4e8e\u7d27\u81f4\u57df\u4e0a Borel \u53ef\u6d4b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u901a\u7528\u903c\u8fd1\u7406\u8bba. \u6700\u8fd1\u7684\u4e00\u4e9b\u5de5\u4f5c\u7814\u7a76\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u4e0d\u540c\u51fd\u6570\u7a7a\u95f4\u7684\u8868\u8fbe\u80fd\u529b, \u4f8b\u5982 Sobolev \u7a7a\u95f4, Barron \u51fd\u6570\u548c H\u00f6lder \u7a7a\u95f4. \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0e\u4f20\u7edf\u903c\u8fd1\u65b9\u6cd5\u6709\u7740\u5bc6\u5207\u7684\u8054\u7cfb, \u4f8b\u5982\u6837\u6761\u51fd\u6570, \u538b\u7f29\u611f\u77e5\u548c\u6709\u9650\u5143. \u6587\u732e 40 \u548c 41 \u4e2d\u5206\u522b\u7814\u7a76\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6536\u655b\u6027. \u4e00\u4e9b\u5de5\u4f5c\u8bd5\u56fe\u7406\u89e3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b. \u4f8b\u5982, \u6587\u732e 10 \u4e2d\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u88ab\u89e3\u91ca\u4e3a\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u81ea\u9002\u5e94\u57fa. Traditionally, deep neural networks are dense and over-parameterized. A dense network model requires more memory and other computational resources during training and inference of the model. Increasingly greater amounts of data and related model sizes demand the availability of more competent learning models. Compared to dense models, sparse deep neural networks require less memory, less computing time and have better interpretability. Hence, sparse deep neural network models are desirable. On the other hand, animal brains are found to have hierarchical and sparse structures 19 . The connectivity of an animal brain becomes sparser as the size of the brain grows larger. Therefore, it is not only necessary but also natural to design sparse networks. In fact, it was pointed out in 24 that the future of deep learning relies on sparsity. Further more, over-parameterized and dense models tend to lead to overfitting and weakening the ability to generalize over unseen examples. Sparse models can improve accuracy of approximation. Sparse regularization is a popular way to learn the sparse solutions 5 38 39 42 . The readers are referred to 25 for an overview of sparse deep learning. \u4e00\u822c\u6765\u8bf4, \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bc6\u96c6\u4e14\u8fc7\u53c2\u6570\u5316. \u5728\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d, \u5bc6\u96c6\u7f51\u7edc\u6a21\u578b\u9700\u8981\u66f4\u591a\u7684\u5185\u5b58\u548c\u5176\u4ed6\u8ba1\u7b97\u8d44\u6e90. \u8d8a\u6765\u8d8a\u591a\u7684\u6570\u636e\u548c\u76f8\u5173\u6a21\u578b\u7684\u5927\u5c0f\u8981\u6c42\u63d0\u4f9b\u66f4\u6709\u80fd\u529b\u7684\u5b66\u4e60\u6a21\u578b. \u4e0e\u5bc6\u96c6\u6a21\u578b\u76f8\u6bd4, \u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u5185\u5b58\u66f4\u5c11, \u8ba1\u7b97\u65f6\u95f4\u66f4\u77ed, \u53ef\u89e3\u91ca\u6027\u66f4\u597d\u7b49\u4f18\u70b9. \u56e0\u6b64, \u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u662f\u53ef\u53d6\u7684. \u53e6\u4e00\u65b9\u9762, \u52a8\u7269\u7684\u5927\u8111\u88ab\u53d1\u73b0\u5177\u6709\u5c42\u6b21\u548c\u7a00\u758f\u7ed3\u6784. \u52a8\u7269\u5927\u8111\u7684\u8fde\u63a5\u6027\u968f\u7740\u5927\u8111\u5c3a\u5bf8\u7684\u589e\u5927\u800c\u53d8\u5f97\u7a00\u758f. \u56e0\u6b64\u8bbe\u8ba1\u7a00\u758f\u7f51\u7edc\u4e0d\u4ec5\u662f\u5fc5\u8981\u7684, \u800c\u4e14\u662f\u81ea\u7136\u7684. \u4e8b\u5b9e\u4e0a, \u5728\u6587\u732e 24 \u4e2d\u6307\u51fa, \u6df1\u5ea6\u5b66\u4e60\u7684\u672a\u6765\u4f9d\u8d56\u4e8e\u7a00\u758f\u6027. \u6b64\u5916, \u8fc7\u53c2\u6570\u5316\u548c\u5bc6\u96c6\u6a21\u578b\u5f80\u5f80\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u524a\u5f31\u5728\u672a\u77e5\u6837\u672c\u4e0a\u6cdb\u5316\u7684\u80fd\u529b. \u7a00\u758f\u6a21\u578b\u53ef\u4ee5\u63d0\u9ad8\u903c\u8fd1\u7cbe\u5ea6. \u7a00\u758f\u6b63\u5219\u5316\u662f\u5b66\u4e60\u7a00\u758f\u89e3\u7684\u4e00\u79cd\u6d41\u884c\u65b9\u6cd5. \u8bfb\u8005\u53ef\u53c2\u8003 25 \u4ee5\u4e86\u89e3\u7a00\u758f\u6df1\u5ea6\u5b66\u4e60. Although much progress has been made in theoretical research of deep learning, it remains a challenging issue to construct an effective neural network approximation for general function spaces using as few neuron connections or neurons as possible. Most of existing network structures are specific for a particular class of functions. In this paper, we aim to propose a multi-scale sparse regularized neural network to approximate the function effectively. A neural network with multiple hidden layers can be viewed as a multi-scale transformation from simple features to complex features. The layer-by-layer composite of functions can be seen as a generalization of wavelet transforms 7 12 33 . For neurons in different layers, corresponding to different transformation scales, the corresponding features have different levels of importance. Imposing different regularization parameters for different scales was proved to be an effective way to deal with multi-scale regularization problems 3 6 32 . Inspired by multi-scale analysis, we propose a sparse regularization network model by applying different sparse regularization penalties to the neuron connections in different layers. During the training process, the neural network adaptively learns matrix weights from given data. By sparse optimization, many weight connections are automatically zero. The remaining neural networks composed of non-zero weights form the sparse deep neural network that we desire. \u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u7684\u7406\u8bba\u7814\u7a76\u5df2\u7ecf\u53d6\u5f97\u4e86\u5f88\u5927\u7684\u8fdb\u5c55, \u4f46\u662f\u5982\u4f55\u5229\u7528\u5c3d\u53ef\u80fd\u5c11\u7684\u795e\u7ecf\u5143\u8fde\u63a5\u6216\u795e\u7ecf\u5143\u6765\u6784\u9020\u4e00\u4e2a\u6709\u6548\u7684\u7528\u4e8e\u4e00\u822c\u51fd\u6570\u7a7a\u95f4\u7684\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898. \u5927\u591a\u6570\u73b0\u6709\u7684\u7f51\u7edc\u7ed3\u6784\u90fd\u662f\u7279\u5b9a\u4e8e\u67d0\u4e00\u7c7b\u51fd\u6570\u7684. \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u591a\u5c3a\u5ea6\u7a00\u758f\u6b63\u5219\u5316\u795e\u7ecf\u7f51\u7edc\u6765\u6709\u6548\u5730\u903c\u8fd1\u51fd\u6570. \u5177\u6709\u591a\u4e2a\u9690\u85cf\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u770b\u4f5c\u662f\u7531\u7b80\u5355\u7279\u5f81\u5411\u590d\u6742\u7279\u5f81\u7684\u591a\u5c3a\u5ea6\u53d8\u6362. \u51fd\u6570\u7684\u9010\u5c42\u590d\u5408\u53ef\u4ee5\u770b\u4f5c\u662f\u5c0f\u6ce2\u53d8\u6362\u7684\u4e00\u79cd\u63a8\u5e7f. \u5bf9\u4e8e\u4e0d\u540c\u5c42\u7684\u795e\u7ecf\u5143, \u5bf9\u5e94\u4e8e\u4e0d\u540c\u7684\u53d8\u6362\u5c3a\u5ea6, \u76f8\u5e94\u7684\u7279\u5f81\u5177\u6709\u4e0d\u540c\u7ea7\u522b\u7684\u91cd\u8981\u6027. \u9488\u5bf9\u4e0d\u540c\u5c3a\u5ea6\u8bbe\u7f6e\u4e0d\u540c\u7684\u6b63\u5219\u5316\u53c2\u6570\u662f\u5904\u7406\u591a\u5c3a\u5ea6\u6b63\u5219\u5316\u95ee\u9898\u7684\u4e00\u79cd\u6709\u6548\u65b9\u6cd5. \u53d7\u591a\u5c3a\u5ea6\u5206\u6790\u7684\u542f\u53d1, \u6211\u4eec\u901a\u8fc7\u5bf9\u4e0d\u540c\u5c42\u7684\u795e\u7ecf\u5143\u8fde\u63a5\u65bd\u52a0\u4e0d\u540c\u7684\u7a00\u758f\u6b63\u5219\u5316\u60e9\u7f5a, \u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u6b63\u5219\u5316\u7f51\u7edc\u6a21\u578b. \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u795e\u7ecf\u7f51\u7edc\u4ece\u7ed9\u5b9a\u7684\u6570\u636e\u4e2d\u81ea\u9002\u5e94\u5730\u5b66\u4e60\u77e9\u9635\u6743\u91cd. \u901a\u8fc7\u7a00\u758f\u4f18\u5316, \u8bb8\u591a\u6743\u91cd\u8fde\u63a5\u81ea\u52a8\u4e3a\u96f6. \u5176\u4f59\u7531\u975e\u96f6\u6743\u91cd\u7ec4\u6210\u7684\u795e\u7ecf\u7f51\u7edc\u6784\u6210\u6211\u4eec\u6240\u9700\u8981\u7684\u7a00\u758f\u6df1\u5c42\u795e\u7ecf\u7f51\u7edc. This paper is organized in five sections. In Section 2, we describe a multi-parameter regularization model for solving partial differential equations by using deep neural networks. We study in Section 3 the capacity of the proposed multi-parameter regularization in adaptive representing functions having certain singularities. In Section 4, we investigate numerical solutions of nonlinear partial differential equations by using the proposed SDNN model. Specifically, we consider two equations: the Burgers equation and the Schr\u00f6dinger equation since solutions of these two equations exhibit certain types of singularities. Finally, a conclusion is drawn in Section 5. \u672c\u6587\u5206\u4e3a\u4e94\u4e2a\u90e8\u5206. \u5728\u7b2c\u4e8c\u8282\u4e2d, \u6211\u4eec\u63cf\u8ff0\u4e86\u4e00\u4e2a\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u591a\u53c2\u6570\u6b63\u5219\u5316\u6a21\u578b. \u5728\u7b2c\u4e09\u8282\u4e2d, \u6211\u4eec\u7814\u7a76\u4e86\u6240\u63d0\u51fa\u7684\u591a\u53c2\u6570\u6b63\u5219\u5316\u6a21\u578b\u5728\u81ea\u9002\u5e94\u8868\u793a\u5177\u6709\u4e00\u5b9a\u5947\u6027\u7684\u51fd\u6570\u80fd\u529b. \u5728\u7b2c\u56db\u8282\u4e2d, \u6211\u4eec\u4f7f\u7528\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u6c42\u89e3\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u6570\u503c\u89e3. \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u8003\u8651\u4e24\u4e2a\u65b9\u7a0b: Burgers \u65b9\u7a0b\u548c Schr\u00f6dinger \u65b9\u7a0b, \u56e0\u4e3a\u8fd9\u4e24\u4e2a\u65b9\u7a0b\u7684\u89e3\u663e\u793a\u51fa\u67d0\u79cd\u7c7b\u578b\u7684\u5947\u6027. \u5728\u7b2c\u4e94\u8282\u4e2d, \u5f97\u51fa\u6700\u540e\u7684\u7ed3\u8bba.","title":"Introduction"},{"location":"Scholars/PN-2207.13266/#a-sparse-dnn-model-for-solving-partial-differential-equations","text":"In this section, we propose a sparse DNN model for solving nonlinear partial differential equations (PDEs). \u5728\u672c\u8282\u4e2d, \u6211\u4eec\u63d0\u51fa\u4e00\u4e2a\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7528\u4e8e\u6c42\u89e3\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b. We begin with describing the PDE and its boundary, initial conditions to be considered in this paper. Suppose that \\(\\Omega\\) is an open domain in \\(\\mathbb{R}^d\\) . By \\(\\Gamma\\) we denote the boundary of the domain \\(\\Omega\\) . Let \\(\\mathcal{F}\\) denote a nonlinear differential operator, \\(\\mathcal{I}\\) the initial condition operator, and \\(\\mathcal{B}\\) the boundary operator. We consider the following boundary/initial value problem of the nonlinear partial differential equation: \u9996\u5148\u4ece\u63cf\u8ff0\u672c\u6587\u8003\u8651\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u53ca\u5176\u521d\u8fb9\u503c\u6761\u4ef6\u5f00\u59cb. \u8bbe \\(\\Omega\\) \u662f \\(\\mathbb{R}^d\\) \u4e0a\u7684\u4e00\u4e2a\u5f00\u96c6. \u7528 \\(\\Gamma\\) \u8868\u793a\u5b9a\u4e49\u57df \\(\\Omega\\) \u7684\u8fb9\u754c. \u7528 \\(\\mathcal{F}\\) \u8868\u793a\u975e\u7ebf\u6027\u5fae\u5206\u7b97\u5b50, \\(\\mathcal{I}\\) \u8868\u793a\u521d\u59cb\u6761\u4ef6\u7b97\u5b50, \\(\\mathcal{B}\\) \u8868\u793a\u8fb9\u754c\u6761\u4ef6\u7b97\u5b50. \u6211\u4eec\u8003\u8651\u5982\u4e0b\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u8fb9\u503c\u95ee\u9898: $$ \\begin{align} \\mathcal{F}(u(t,x)) &= 0, &x\\in\\Omega,\\ &t\\in [0,T],\\tag{1}\\ \\mathcal{I}(u(t,x)) &= 0, &x\\in\\Omega,\\ &t=0,\\tag{2}\\ \\mathcal{B}(u(t,x)) &= 0, &x\\in\\Gamma,\\ &t\\in [0,T],\\tag{3}\\ \\end{align} $$ where \\(T > 0\\) , the data \\(u\\) on \\(\\Gamma\\) and \\(t = 0\\) are given and \\(u\\) in \\(\\Omega\\) is the solution to be learned. The formulation (1) - (3) covers a broad range of problems including conservation laws, reaction-diffusion equations, and Navier-Stokes equations. \u5176\u4e2d \\(T>0\\) , \u5728 \\(\\Gamma\\) \u548c \\(t=0\\) \u5904\u7684\u6570\u636e \\(u\\) \u7ed9\u5b9a, \u5728 \\(\\Omega\\) \u4e0a\u7684 \\(u\\) \u662f\u9700\u8981\u5b66\u4e60\u7684\u89e3. \u516c\u5f0f 1-3 \u6db5\u76d6\u4e86\u5f88\u5927\u8303\u56f4\u7684\u95ee\u9898\u5982\u5b88\u6052\u5f8b, \u53cd\u5e94\u6269\u6563\u65b9\u7a0b, Navier-Stokes \u65b9\u7a0b\u7b49. For example, the one dimensional Burgers equation can be recognized as \u4f8b\u5982, \u4e00\u7ef4 Burgers \u65b9\u7a0b\u53ef\u4ee5\u8868\u793a\u4e3a \\[ \\mathcal{F}(u) := \\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\frac{\\partial^2 u}{\\partial x^2}. \\] The goal of this paper is to develop a sparse DNN model for solving problem(1). We will conduct numerical study of the proposed model by applying it to two equations, the Burgers equation and the Schr\u00f6dinger equation, of practical importance. \u672c\u6587\u7684\u76ee\u6807\u662f\u5efa\u7acb\u4e00\u4e2a\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6c42\u89e3\u95ee\u9898 (1) . \u6211\u4eec\u5c06\u901a\u8fc7\u628a\u6a21\u578b\u5e94\u7528\u5230\u4e24\u4e2a\u5177\u6709\u5b9e\u9645\u91cd\u8981\u6027\u7684\u65b9\u7a0b, Burgers \u65b9\u7a0b\u548c Schr\u00f6dinger \u65b9\u7a0b\u6765\u5bf9\u8fd9\u4e2a\u6a21\u578b\u8fdb\u884c\u6570\u503c\u7814\u7a76. Now, we present the sparse DNN model with multi-parameter regularization. We first recall the the feed forward neural network (FNN). A neural network can be viewed as a composition of functions. A FNN of depth \\(D\\) is defined to be a neural network with an input layer, \\(D - 1\\) hidden layers, and an output layer. A neural network with more than two hidden layers is usually called a deep neural network (DNN). Suppose that there are \\(d_i\\) neurons in the \\(i\\) -th hidden layer. Let \\(W_i\\in \\mathbb{R}^{d_i\\times d_{i-1}}\\) and \\(b_i\\in\\mathbb{R}^{d_i}\\) denote, respectively, the weight matrix and bias vector of the \\(i\\) -th layer. By \\(x_0:= x \\in \\mathbb{R}^{d_0}\\) we denote the input vector and by \\(x_{i-1}\\in \\mathbb{R}^{d_{i-1}}\\) we denote the output vector of the ( \\(i - 1\\) )-th layer. For the \\(i\\) -th hidden layer, we define the affine transform \\(L_i: \\mathbb{R}^{d_{i-1}}\\mapsto \\mathbb{R}^{d_i}\\) by \u73b0\u5728\u6211\u4eec\u4ecb\u7ecd\u5e26\u6709\u591a\u53d8\u91cf\u6b63\u5219\u5316\u7684\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b. \u6211\u4eec\u9996\u5148\u56de\u5fc6\u524d\u9988\u795e\u7ecf\u7f51\u7edc FNN. \u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u89c6\u4e3a\u51fd\u6570\u7684\u590d\u5408. \u4e00\u4e2a \\(D\\) \u5c42\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u662f\u6307\u5177\u6709\u4e00\u5c42\u8f93\u5165\u5c42, \\(D-1\\) \u5c42\u9690\u85cf\u5c42\u548c\u4e00\u5c42\u8f93\u51fa\u5c42\u7684\u795e\u7ecf\u7f51\u7edc. \u5177\u6709\u8d85\u8fc7\u4e24\u5c42\u9690\u85cf\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u79f0\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc DNN. \u5047\u8bbe\u7b2c \\(i\\) \u5c42\u9690\u85cf\u5c42\u4e2d\u6709 \\(d_i\\) \u4e2a\u795e\u7ecf\u7f51\u7edc. \u7528 \\(W_i,b_i\\) \u5206\u522b\u8868\u793a\u7b2c \\(i\\) \u5c42\u7684\u6743\u91cd\u77e9\u9635\u548c\u504f\u5dee\u5411\u91cf. \\(x_0\\) \u8868\u793a\u8f93\u5165\u5411\u91cf, \\(x_{i-1}\\) \u8868\u793a\u7b2c \\(i-1\\) \u5c42\u7684\u8f93\u51fa\u5411\u91cf. \u5bf9\u4e8e\u7b2c \\(i\\) \u5c42\u9690\u85cf\u5c42, \u6211\u4eec\u5b9a\u4e49\u4eff\u5c04\u53d8\u6362 \\(L_i\\) \u5982\u4e0b: \\[ L_i(x_{i-1}) := W_i x_{i-1}+ b_i, i = 1,2,\\cdots,D. \\] For an activation function \\(\\sigma_i\\) , the output vector of the \\(i\\) -th hidden layer is defined as \u5bf9\u4e8e\u6fc0\u6d3b\u51fd\u6570 \\(\\sigma_i\\) , \u7b2c \\(i\\) \u5c42\u9690\u85cf\u5c42\u7684\u8f93\u51fa\u5411\u91cf\u5b9a\u4e49\u4e3a \\[ x_i:= \\sigma_i(L_i(x_{i-1})). \\] Given nonlinear activation functions \\(\\sigma_i, i = 1, 2,\\cdots, D-1\\) , the feed forward neural network \\(N_\\Theta(x)\\) of depth \\(D\\) is defined as \u7ed9\u5b9a \\(D-1\\) \u4e2a\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570 \\(\\sigma_i\\) , \u6df1\u5ea6\u4e3a \\(D\\) \u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc \\(N_\\Theta(x)\\) \u5b9a\u4e49\u4e3a \\[ N_\\Theta(x) := L_D\\circ \\sigma_{D-1} \\circ L_{D-1}\\circ \\cdots\\circ \\sigma_1\\circ L_1(x),\\tag{4} \\] where \\(\\circ\\) denotes the composition operator and \\(\\Theta :=\\{W_i, b_i\\}^D_{i=1}\\) is the set of trainable parameters in the network. \u5176\u4e2d \\(\\circ\\) \u8868\u793a\u590d\u5408\u7b97\u5b50, \\(\\Theta\\) \u662f\u7f51\u7edc\u53ef\u8bad\u7ec3\u53c2\u6570\u96c6\u5408. We first describe the physics-informed neural network ( PINN ) model introduced in 35 for solving the partial differential equation (1) . We denote by \\(Loss_{PDE}\\) the loss of training data on the partial differential equation (1) . We choose \\(N_f\\) collocation points \\((t^i_f, x^i_f)\\) by randomly sampling in domain \\(\\Omega\\) using a sampling method such as Latin hypercube sampling 23 . We then evaluate \\(\\mathcal{F}(\\mathcal{N}_\\Theta(t^i_f, x^i_f))\\) for \\(i = 1, 2,\\cdots N_f\\) and define \u6211\u4eec\u9996\u5148\u4ecb\u7ecd\u7528\u4e8e\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b (1) \u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc PINN. \u6211\u4eec\u4f7f\u7528\u67d0\u79cd\u91c7\u6837\u65b9\u6cd5\u5982\u62c9\u4e01\u8d85\u7acb\u65b9\u91c7\u6837 (LHS) \u4ece\u5b9a\u4e49\u57df \\(\\Omega\\) \u4e2d\u968f\u673a\u91c7\u6837 \\(N_f\\) \u4e2a\u914d\u7f6e\u70b9, \u7136\u540e\u5728\u8fd9\u4e9b\u70b9\u4e0a\u8ba1\u7b97 \\(\\mathcal{F}(\\mathcal{N}_\\Theta(t^i_f, x^i_f))\\) \u7684\u503c\u5e76\u5b9a\u4e49 \\[ Loss_{PDE}:= \\frac{1}{N_f} \\sum_{i=1}^{N_f} |\\mathcal{F}(\\mathcal{N}_\\Theta(t^i_f, x^i_f))|^2, \\] where \\(\\mathcal{F}\\) is the operator for the partial differential equation (1) . \u5176\u4e2d \\(\\mathcal{F}\\) \u662f\u504f\u5fae\u5206\u65b9\u7a0b (1) \u7684\u7b97\u5b50. We next describe the loss function for the boundary/initial condition. We randomly sample \\(N_0\\) points \\(x^i_0\\) for the initial condition (2) , \\(N_b\\) points \\(\\{t^i_b, x^i_b\\}\\) for the boundary condition (3) . The loss function \\(Loss_0\\) related to the initial value condition is given by \u7136\u540e\u6211\u4eec\u4ecb\u7ecd\u8fb9\u754c\u6761\u4ef6/\u521d\u59cb\u6761\u4ef6\u7684\u635f\u5931\u51fd\u6570. \u6211\u4eec\u4e3a\u521d\u59cb\u6761\u4ef6\u968f\u673a\u91c7\u6837 \\(N_0\\) \u4e2a\u70b9, \u4e3a\u8fb9\u754c\u6761\u4ef6\u968f\u673a\u91c7\u6837 \\(N_b\\) \u4e2a\u70b9. \u7136\u540e\u4e0e\u521d\u503c\u6761\u4ef6\u76f8\u5173\u7684\u635f\u5931\u51fd\u6570 \\(Loss_0\\) \u5b9a\u4e49\u5982\u4e0b \\[ Loss_0 := \\frac{1}{N_0} \\sum_{i=1}^{N_0} |\\mathcal{I}(\\mathcal{N}_\\Theta(0, x^i_0))|. \\] The loss function \\(Loss_b\\) pertaining to the boundary value is given as \u4e0e\u8fb9\u503c\u6761\u4ef6\u76f8\u5173\u7684\u635f\u5931\u51fd\u6570 \\(Loss_b\\) \u5b9a\u4e49\u5982\u4e0b \\[ Loss_b := \\frac{1}{N_b} \\sum_{i=1}^{N_b}|\\mathcal{B}(\\mathcal{N}_\\Theta(t^i_b, x^i_b))|, x^i_b\\in \\Gamma. \\] Adding the three loss functions \\(Loss_{PDE}\\) , \\(Loss_0\\) , and \\(Loss_b\\) together gives rise to the PINN model \u5c06\u4e09\u4e2a\u635f\u5931\u51fd\u6570 \\(Loss_{PDE}\\) , \\(Loss_0\\) , \\(Loss_b\\) \u76f8\u52a0\u5c31\u5f97\u5230\u4e86 PINN \u6a21\u578b \\[ \\min_\\Theta\\bigg\\{\\frac{1}{N_f} \\sum_{i=1}^{N_f} |\\mathcal{F}(\\mathcal{N}_\\Theta(t^i_f, x^i_f))|^2 + \\frac{1}{N_0} \\sum_{i=1}^{N_0} |\\mathcal{I}(\\mathcal{N}_\\Theta(0, x^i_0))| + \\frac{1}{N_b} \\sum_{i=1}^{N_b}|\\mathcal{B}(\\mathcal{N}_\\Theta(t^i_b, x^i_b))|\\bigg\\}\\tag{5} \\] where \\(\\Theta:=\\{W_i, b_i\\}^D_{i=1}\\) . \u5176\u4e2d \\(\\Theta:=\\{W_i, b_i\\}^D_{i=1}\\) . The neural network learned from (5) is often dense and may be over-parameterized. Moreover, training data are often contaminated with noise. When noise presents, over-parameterized models may overfit training data samples and result in bad generalization to the unseen samples. The problem of overfitting is often overcome by adding a regularization term: \u4ece\u4e0a\u8ff0\u635f\u5931\u51fd\u6570\u5b66\u4e60\u5230\u7684\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u662f\u5bc6\u96c6\u7684\u4e14\u53ef\u80fd\u8fc7\u53c2\u6570\u5316. \u6b64\u5916, \u8bad\u7ec3\u6570\u636e\u7ecf\u5e38\u88ab\u566a\u58f0\u6c61\u67d3. \u5f53\u566a\u58f0\u51fa\u73b0, \u8fc7\u53c2\u6570\u5316\u7684\u6a21\u578b\u53ef\u80fd\u5bf9\u8bad\u7ec3\u6570\u636e\u8fc7\u62df\u5408\u4ece\u800c\u5728\u672a\u77e5\u6837\u672c\u4e0a\u6cdb\u5316\u5f97\u5f88\u5dee. \u8fc7\u62df\u5408\u95ee\u9898\u901a\u5e38\u901a\u8fc7\u6dfb\u52a0\u4e00\u4e2a\u6b63\u5219\u5316\u9879\u6765\u514b\u670d. \\[ Loss := Loss_{PDE}+ \\beta (Loss_0 + Loss_b) + \\text{Regularization}. \\] The \\(l_1\\) - and \\(l_2\\) -norms are popular choices for regularization. Design of the regularization often makes use of prior information of the solution to be learned. It is known 4 17 42 that the \\(l_1\\) -norm can promote sparsity. Hence, the \\(l_1\\) -norm regularization not only has many advantages over the \\(l_2\\) -norm regularization, but also leads to sparse models which can be more easily interpreted. Therefore, we choose to use the \\(l_1\\) -norm as the regularizer in this study. Furthermore, we observe that DNNs have an intrinsic multiscale structure whose different layers represent different scales of information, which will be validated later by numerical studies. \\(l_1\\) \u8303\u6570\u548c \\(l_2\\) \u8303\u6570\u662f\u6b63\u5219\u5316\u7684\u5e38\u7528\u9009\u62e9. \u6b63\u5219\u5316\u7684\u8bbe\u8ba1\u901a\u5e38\u4f7f\u7528\u9700\u8981\u5b66\u4e60\u7684\u89e3\u7684\u5148\u9a8c\u4fe1\u606f. \u4ece\u5148\u524d\u7684\u5de5\u4f5c\u4e2d\u76f4\u5230 \\(l_1\\) \u8303\u6570\u53ef\u4ee5\u4fc3\u8fdb\u7a00\u758f\u6027. \u56e0\u6b64 \\(l_1\\) \u8303\u6570\u6b63\u5219\u5316\u4e0d\u4ec5\u6bd4 \\(l_2\\) \u8303\u6570\u6b63\u5219\u5316\u6709\u8bf8\u591a\u4f18\u70b9, \u8fd8\u80fd\u591f\u5bfc\u51fa\u66f4\u6613\u4e8e\u89e3\u91ca\u7684\u7a00\u758f\u6a21\u578b. \u56e0\u6b64\u6211\u4eec\u5728\u672c\u9879\u5de5\u4f5c\u4e2d\u9009\u62e9 \\(l_1\\) \u8303\u6570\u4f5c\u4e3a\u6b63\u5219\u5316\u9879. \u6b64\u5916, \u6211\u4eec\u89c2\u5bdf\u5230\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6709\u4e00\u4e2a\u5185\u5728\u591a\u5c3a\u5ea6\u7ed3\u6784, \u4e0d\u540c\u5c42\u8868\u793a\u4fe1\u606f\u7684\u4e0d\u540c\u5c3a\u5ea6, \u8fd9\u5c06\u5728\u4e4b\u540e\u7684\u6570\u503c\u7b97\u4f8b\u4e2d\u5f97\u5230\u9a8c\u8bc1. In fact, we will demonstrate in the next section that a smooth function or smooth parts of a function can be represented by a DNN with sparse weight matrices. This is because a smooth part of a function contains redundant information, which can be described very well by a few parameters, and only non-smooth parts of a function require more parameters to describe them. In other words, by properly choosing regularization, DNNs can lead to adaptive sparse representations of functions having certain singularities. With this understanding, we construct an adaptive representation of a function, especially for a function having certain singularity by adopting a sparse regularization model. Our idea for the adaptive representation is to impose different sparsity penalties for different layers. Specifically, we propose a multiscale-like sparse regularization using the \\(l_1\\) -norm of the weight matrix for each layer with a different parameter for a different layer. The regularization with multiple parameters allows us to represent a function in a multiscale-like neural network which is determined by sparse weight matrices having different sparsity at different layers. Such a regularization added to the loss function will enable us to robustly extract critical information of the solution of the PDE. \u5b9e\u9645\u4e0a, \u6211\u4eec\u5c06\u5728\u4e0b\u4e00\u8282\u8bf4\u660e\u7684\u662f\u4e00\u4e2a\u5149\u6ed1\u51fd\u6570\u6216\u51fd\u6570\u7684\u5149\u6ed1\u90e8\u5206\u53ef\u4ee5\u7531\u5177\u6709\u7a00\u758f\u6743\u91cd\u77e9\u9635\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6765\u8868\u793a. \u8fd9\u662f\u56e0\u4e3a\u51fd\u6570\u7684\u5149\u6ed1\u90e8\u5206\u5305\u542b\u4e86\u5197\u4f59\u4fe1\u606f, \u80fd\u591f\u88ab\u5c11\u6570\u53c2\u6570\u63cf\u8ff0\u5f97\u5f88\u597d, \u5e76\u4e14\u53ea\u6709\u51fd\u6570\u5f97\u975e\u5149\u6ed1\u90e8\u5206\u9700\u8981\u66f4\u591a\u7684\u53c2\u6570\u53bb\u63cf\u8ff0\u5b83\u4eec. \u6362\u53e5\u8bdd\u8bf4, \u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684\u6b63\u5219\u5316, \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u5f97\u5230\u5e26\u6709\u5947\u6027\u7684\u51fd\u6570\u7684\u81ea\u9002\u5e94\u7a00\u758f\u8868\u793a. \u6839\u636e\u8fd9\u4e00\u8ba4\u77e5, \u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u7a00\u758f\u6b63\u5219\u5316\u6a21\u578b\u6765\u6784\u9020\u51fd\u6570, \u7279\u522b\u662f\u5e26\u6709\u67d0\u4e9b\u5947\u6027\u7684\u51fd\u6570\u7684\u81ea\u9002\u5e94\u8868\u793a. \u5173\u4e8e\u81ea\u9002\u5e94\u8868\u793a\u7684\u601d\u8def\u662f\u7ed9\u4e0d\u540c\u7684\u5c42\u4e16\u5bb6\u4e0d\u540c\u7684\u7a00\u758f\u5ea6\u60e9\u7f5a. \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7c7b\u591a\u5c3a\u5ea6\u7a00\u758f\u6b63\u5219\u5316, \u5bf9\u6bcf\u4e00\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684 \\(l_1\\) \u8303\u6570\u8d4b\u4e88\u4e0d\u540c\u7684\u53c2\u6570 \u591a\u53c2\u6570\u6b63\u5219\u5316\u4f7f\u5f97\u6211\u4eec\u80fd\u591f\u5728\u7c7b\u591a\u5c3a\u5ea6\u7684\u795e\u7ecf\u7f51\u7edc\u4e2d\u8868\u793a\u4e00\u4e2a\u51fd\u6570, \u8be5\u7f51\u7edc\u7531\u4e0d\u540c\u5c42\u4e0a\u5177\u6709\u4e0d\u540c\u7a00\u758f\u5ea6\u7684\u7a00\u758f\u6743\u91cd\u77e9\u9635\u51b3\u5b9a. \u5728\u635f\u5931\u51fd\u6570\u4e2d\u52a0\u5165\u8fd9\u79cd\u6b63\u5219\u5316, \u53ef\u4ee5\u6709\u6548\u5730\u63d0\u53d6\u504f\u5fae\u5206\u65b9\u7a0b\u89e3\u7684\u5173\u952e\u4fe1\u606f. We now describe the proposed regularization. For layer \\(i\\) , we denote by \\(W^{k,j}_i\\) the \\((k, j)\\) -th entry of matrix \\(W_i\\) , the entry in the \\(k\\) -th row and the \\(j\\) -th column. For this reason, we adopt the \\(l_1\\) -norm of matrix \\(W_i\\) defined by following formula as our sparse regularization. \u6211\u4eec\u73b0\u5728\u63cf\u8ff0\u6240\u63d0\u51fa\u7684\u6b63\u5219\u5316. \u5bf9\u4e8e\u5c42 \\(i\\) , \u6211\u4eec\u7528 \\(W^{k,j}_i\\) \u8868\u793a\u77e9\u9635 \\(W_i\\) \u7684\u7b2c \\(k\\) \u884c\u7b2c \\(j\\) \u5217\u9879. \u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5c06\u77e9\u9635 \\(W_i\\) \u7684 \\(l_1\\) \u8303\u6570\u4f5c\u4e3a\u6211\u4eec\u7684\u7a00\u758f\u6b63\u5219\u5316. \\[ \\|W_i\\|_1:=\\sum_{k=1}^{d_i}\\sum_{j=1}^{d_{i-1}}|W^{k,j}_i| \\] Considering that different layers of the neural network play different roles in approximation of a function, we introduce here a multi-parameter regularization model \u8003\u8651\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u540c\u5c42\u5728\u903c\u8fd1\u4e00\u4e2a\u51fd\u6570\u65f6\u626e\u6f14\u4e0d\u540c\u7684\u89d2\u8272, \u6211\u4eec\u5f15\u5165\u5982\u4e0b\u591a\u53c2\u6570\u6b63\u5219\u5316\u6a21\u578b \\[ \\text{Regularization} :=\\sum_{i=1}^D \\alpha_i\\|W_i\\|_1 \\tag{6} \\] where \\(\\alpha_i\\) are nonnegative regularization parameters. \u5176\u4e2d \\(\\alpha_i\\) \u662f\u975e\u8d1f\u7684\u6b63\u5219\u5316\u53c2\u6570. The use of different parameters for weight matrices of different layers in the regularization term (6) allows us to penalize the weight matrices at different layers of the neural network differently in order to extract the multiscale representation of the solution to be learned. That is, for a fixed \\(i\\) , parameter \\(\\alpha_i\\) determines the sparsity of weight matrix \\(W_i\\) . The larger the parameter \\(\\alpha_i\\) , the more sparse the weight matrix \\(W_i\\) is. The regularized loss function takes the form \u5728\u6b63\u5219\u5316\u9879 (6) \u4e2d\u5bf9\u4e8e\u4e0d\u540c\u5c42\u7684\u6743\u91cd\u77e9\u9635\u4f7f\u7528\u4e0d\u540c\u53c2\u6570\u4f7f\u5f97\u6211\u4eec\u5bf9\u7f51\u7edc\u4e0d\u540c\u5c42\u7684\u6743\u91cd\u77e9\u9635\u8fdb\u884c\u60e9\u7f5a\u4ee5\u63d0\u53d6\u6240\u9700\u5b66\u4e60\u7684\u89e3\u7684\u591a\u5c3a\u5ea6\u8868\u793a. \u5373\u5bf9\u4e8e\u4e00\u4e2a\u56fa\u5b9a\u7684 \\(i\\) , \u53c2\u6570 \\(\\alpha_i\\) \u51b3\u5b9a\u4e86\u6743\u91cd\u77e9\u9635 \\(W_i\\) \u7684\u7a00\u758f\u6027. \\(\\alpha_i\\) \u8d8a\u5927, \u6743\u91cd\u77e9\u9635 \\(W_i\\) \u8d8a\u7a00\u758f. \u6b63\u5219\u5316\u635f\u5931\u51fd\u6570\u5f62\u5f0f\u5982\u4e0b: \\[ Loss := Loss_{PDE}+ \\beta(Loss_0+ Loss_b) +\\sum_{i=1}^D \\alpha_i\\|W_i\\|_1. \\tag{7} \\] The parameters \\(\\Theta:= \\{W_i, b_i\\}^D_{i=1}\\) of the neural network \\(\\mathcal{N}_\\Theta(t,x)\\) are learned by minimizing the loss function \u795e\u7ecf\u7f51\u7edc \\(\\mathcal{N}_\\Theta(t,x)\\) \u7684\u53c2\u6570 \\(\\Theta\\) \u901a\u8fc7\u6700\u5c0f\u5316\u5982\u4e0b\u635f\u5931\u51fd\u6570\u8fdb\u884c\u5b66\u4e60. \\[ \\min_\\Theta \\bigg\\{\\frac{1}{N_f} \\sum_{i=1}^{N_f} |\\mathcal{F}(\\mathcal{N}_\\Theta(t^i_f, x^i_f))|^2 + \\beta(Loss_0+Loss_b) + \\sum_{i=1}^D \\alpha_i\\|W_i\\|_1\\bigg\\}.\\tag{8} \\] Truncating the weights of the layers close to the input layer has an impact on all subsequent layers. In practice, we usually set smaller regularization parameters in layers close to the input and larger regularization parameters in layers close to the output. The resulting neural network will exhibit denser weight matrices near the input layer and sparser weight matrices near the output layer. This network structure reflects the multi-scale nature of neural networks and is automatically learned by sparse regularization. \u622a\u65ad\u9760\u8fd1\u8f93\u5165\u5c42\u7684\u9690\u85cf\u5c42\u7684\u6743\u91cd\u4f1a\u5bf9\u540e\u7eed\u5c42\u4ea7\u751f\u5f71\u54cd. \u5b9e\u8df5\u4e2d\u6211\u4eec\u901a\u5e38\u5bf9\u9760\u8fd1\u8f93\u5165\u5c42\u7684\u9690\u85cf\u5c42\u8bbe\u7f6e\u66f4\u5c0f\u7684\u6b63\u5219\u5316\u53c2\u6570, \u5bf9\u9760\u8fd1\u8f93\u51fa\u5c42\u7684\u9690\u85cf\u5c42\u8bbe\u7f6e\u66f4\u5927\u7684\u6b63\u5219\u5316\u53c2\u6570. \u5f97\u5230\u7684\u795e\u7ecf\u7f51\u7edc\u4f1a\u8868\u73b0\u51fa\u9760\u8fd1\u8f93\u5165\u5c42\u7684\u6743\u91cd\u77e9\u9635\u66f4\u5bc6\u96c6, \u9760\u8fd1\u8f93\u51fa\u5c42\u7684\u6743\u91cd\u77e9\u9635\u66f4\u7a00\u758f. \u8fd9\u6837\u7684\u7f51\u7edc\u7ed3\u6784\u53cd\u6620\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u591a\u5c3a\u5ea6\u672c\u8d28\u4e14\u81ea\u52a8\u5730\u7531\u7a00\u758f\u6b63\u5219\u5316\u5b66\u4e60\u5230. Appropriate choices of the regularization parameters are key to achieve good prediction results. We need to balance sparsity and prediction accuracy. Since there are multiple regularization parameters, the regularization parameters are chosen by grid search layer by layer in this paper. In practice, we first choose the regularization parameters close to the output layer, and then gradually choose the regularization coefficients close to the input layer. \u6b63\u5219\u5316\u53c2\u6570\u7684\u9002\u5f53\u9009\u62e9\u662f\u83b7\u5f97\u826f\u597d\u9884\u6d4b\u7ed3\u679c\u7684\u5173\u952e. \u6211\u4eec\u9700\u8981\u5e73\u8861\u7a00\u758f\u6027\u548c\u9884\u6d4b\u7cbe\u5ea6. \u56e0\u4e3a\u6709\u591a\u4e2a\u6b63\u5219\u5316\u53c2\u6570, \u6240\u4ee5\u672c\u6587\u7684\u6b63\u5219\u5316\u53c2\u6570\u901a\u8fc7\u9010\u5c42\u7f51\u683c\u641c\u7d22\u5f97\u5230. \u5b9e\u8df5\u4e2d\u6211\u4eec\u9996\u5148\u9009\u62e9\u9760\u8fd1\u8f93\u51fa\u5c42\u7684\u6b63\u5219\u5316\u53c2\u6570, \u7136\u540e\u9010\u6e10\u9009\u62e9\u9760\u8fd1\u8f93\u5165\u5c42\u7684\u6b63\u5219\u5316\u7cfb\u6570. We refer equation (8) as to the sparse DNN ( SDNN ) model for the partial differential equation. Upon solving the minimization problem (8) , we obtain an approximate solution \\(u(t,x) := \\mathcal{N}_\\Theta(t, x)\\) with sparse weight matrices. When the regularization parameters \\(\\alpha_i\\) are all set to \\(0\\) , the SDNN model (8) reduces to the PINN model introduced in 35 . We will compare numerical performance of the proposed SDNN model with that of PINN model, for both the Burgers equation and the Schr\u00f6dinger equation. \u6211\u4eec\u5f15\u7528\u65b9\u7a0b (8) \u4f5c\u4e3a\u7528\u4e8e\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc ( SDNN ) \u6a21\u578b. \u57fa\u4e8e\u6c42\u89e3\u6700\u5c0f\u5316\u95ee\u9898 (8) , \u6211\u4eec\u83b7\u5f97\u4e86\u4e00\u4e2a\u5e26\u6709i\u5b66\u672f\u6743\u91cd\u77e9\u9635\u7684\u8fd1\u4f3c\u89e3 \\(u(t,x) := \\mathcal{N}_\\Theta(t, x)\\) . \u5f53\u6b63\u5219\u5316\u53c2\u6570 \\(\\alpha_i\\) \u5168\u90e8\u8bbe\u7f6e\u4e3a \\(0\\) , \u90a3\u4e48 SDNN \u6a21\u578b\u5c06\u9000\u5316\u4e3a PINN \u6a21\u578b. \u6211\u4eec\u5c06\u5728 Burgers \u65b9\u7a0b\u548c Schr\u00f6dinger \u65b9\u7a0b\u4e0a\u6bd4\u8f83\u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u548c PINN \u7684\u6570\u503c\u8868\u73b0.","title":"A Sparse DNN Model for Solving Partial Differential Equations"},{"location":"Scholars/PN-2207.13266/#function-adaptive-approximation-by-the-sdnn-model","text":"We explore in this section the capacity of the proposed multi-parameter regularization in adaptive representing functions that have certain singularities. We will first reveal that a DNN indeed has an intrinsic multiscale-like structure which is desirable for representing non-smooth functions. We demonstrate in our numerical studies that the proposed SDNN model can reconstruct neural networks which approximate functions in the same accuracy order with nearly the same order of network complexity, regardless the smoothness of the functions. We include in this section a numerical study of reconstruction of black holes by the proposed SDNN model. In this section, we use the rectified linear unit (ReLU) function as an activation function. \u672c\u8282\u6211\u4eec\u63a2\u7a76\u6240\u63d0\u51fa\u7684\u591a\u53c2\u6570\u6b63\u5219\u5316\u5728\u81ea\u9002\u5e94\u8868\u793a\u5e26\u6709\u67d0\u4e9b\u5947\u6027\u7684\u51fd\u6570\u7684\u80fd\u529b. \u6211\u4eec\u9996\u5148\u63ed\u793a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u786e\u5b9e\u5177\u6709\u5185\u5728\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784, \u8fd9\u79cd\u7ed3\u6784\u5bf9\u4e8e\u8868\u793a\u975e\u5149\u6ed1\u51fd\u6570\u662f\u53ef\u53d6\u7684. \u6570\u503c\u7814\u7a76\u8868\u660e, \u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u65e0\u8bba\u51fd\u6570\u7684\u5149\u6ed1\u5ea6\u5982\u4f55, \u90fd\u80fd\u4ee5\u8fd1\u4f3c\u76f8\u540c\u7684\u7f51\u7edc\u590d\u6742\u5ea6\u9636\u6570\u91cd\u6784\u51fa\u7cbe\u5ea6\u76f8\u540c\u7684\u51fd\u6570. \u5728\u8fd9\u4e00\u8282\u4e2d\u8fd8\u5305\u62ec\u4e00\u4e2a\u4f7f\u7528\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u91cd\u5efa\u9ed1\u6d1e\u7684\u6570\u503c\u7814\u7a76. \u5728\u672c\u8282\u4e2d, \u6211\u4eec\u4f7f\u7528\u6574\u6d41\u7ebf\u6027\u5355\u4f4d\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570. \\[ \\text{ReLU}(x) := \\max\\{0, x\\}, x \\in \\mathbb{R} \\] We first describe the data fitting problem. Given training points \\((x_i, y_i), i =1, 2, \\cdots, N\\) , a non-regularized neural network is determined by minimizing the regression error, that is, \u6211\u4eec\u9996\u5148\u63cf\u8ff0\u6570\u636e\u62df\u5408\u95ee\u9898. \u7ed9\u5b9a\u8bad\u7ec3\u6837\u672c\u70b9 \\((x_i, y_i), i =1, 2, \\cdots, N\\) , \u975e\u6b63\u5219\u5316\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u6700\u5c0f\u5316\u5982\u4e0b\u56de\u5f52\u635f\u5931\u786e\u5b9a \\[ \\min_\\Theta\\frac{1}{N} \\sum_{i=1}^N |\\mathcal{N}_\\Theta(x_i) - y_i|^2.\\tag{9} \\] The multi-parameter sparse regularization DNN model for the data fitting problem reads \u7528\u4e8e\u6570\u636e\u62df\u5408\u95ee\u9898\u7684\u591a\u53c2\u6570\u7a00\u758f\u6b63\u5219\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5219\u6700\u5c0f\u5316\u5982\u4e0b\u635f\u5931 \\[ \\min_\\Theta\\bigg\\{\\frac{1}{N}\\sum_{i=1}^N|\\mathcal{N}_\\Theta(x_i) - y_i|^2+\\sum_{i=1}^D \\alpha_i \\|W_i\\|_1\\bigg\\},\\tag{10} \\] where \\(\\alpha_i\\) are nonnegative regularization parameters and \\(W_i\\) are weight matrices. \u5176\u4e2d \\(\\alpha_i\\) \u662f\u975e\u8d1f\u6b63\u5219\u5316\u53c2\u6570, \\(W_i\\) \u662f\u6743\u91cd\u77e9\u9635. In examples to be presented in this section and the section that follows, the network structure is described by the number of neurons in each layer. Specifically, we use the notation \\([d_0, d_1,\\cdots,d_D]\\) to describe networks that have one input layer, \\(D - 1\\) hidden layers and one output layer, with \\(d_0, d_1,\\cdots, d_D\\) number of neurons, respectively. The regularization parameters, which will be presented as a vector \\(\\alpha:= [\\alpha_1, \\alpha_2,\\cdots, \\alpha_D]\\) , are chosen so that best results are obtained. We will use the relative \\(L_2\\) error to measure approximation accuracy. Suppose that \\(y_i\\) is the exact value of function \\(f\\) to be approximated at \\(x_i\\) , that is, \\(y_i= f(x_i)\\) , and suppose that \\(\\hat{y}_i:= \\mathcal{N}_\\Theta (x_i)\\) is the output of the neural network approximation of \\(f\\) . We let \\(y := [y_1, y_2,\\cdots, y_N]\\) and \\(\\hat{y} := [\\hat{y}_1, \\hat{y}_2,\\cdots, \\hat{y}_N]\\) , and define the error by \\(\\dfrac{\\|y-\\hat{y}\\|_2}{\\|y\\|_2}\\) . Sparsity of the weight matrices is measured by the percentage of zero entries in the weight matrices \\(W_i\\) . In our computation, we set a weight matrix entry \u5728\u672c\u8282\u548c\u4e4b\u540e\u7ae0\u8282\u5c55\u793a\u7684\u4f8b\u5b50\u4e2d, \u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u6784\u901a\u8fc7\u6bcf\u5c42\u795e\u7ecf\u5143\u7684\u6570\u91cf\u8fdb\u884c\u63cf\u8ff0. \u5177\u4f53\u7684, \u6211\u4eec\u4f7f\u7528 \\([d_0, d_1,\\cdots,d_D]\\) \u6765\u63cf\u8ff0\u5177\u6709\u4e00\u5c42\u8f93\u5165\u5c42, \\(D-1\\) \u5c42\u9690\u85cf\u5c42\u548c\u4e00\u5c42\u8f93\u51fa\u5c42, \u5bf9\u5e94\u795e\u7ecf\u5143\u6570\u91cf\u4e3a \\(d_0, d_1,\\cdots, d_D\\) \u7684\u795e\u7ecf\u7f51\u7edc. \u6b63\u5219\u5316\u53c2\u6570\u5c06\u8868\u793a\u4e3a\u4e00\u4e2a\u5411\u91cf \\(\\alpha:= [\\alpha_1, \\alpha_2,\\cdots, \\alpha_D]\\) , \u5c06\u9009\u62e9\u51fa\u80fd\u83b7\u5f97\u6700\u4f73\u7ed3\u679c\u7684\u53c2\u6570. \u6211\u4eec\u5c06\u4f7f\u7528\u76f8\u5bf9 \\(L_2\\) \u8bef\u5dee\u6765\u5ea6\u91cf\u903c\u8fd1\u7cbe\u5ea6. \u5047\u8bbe \\(y_i\\) \u662f\u88ab\u903c\u8fd1\u51fd\u6570 \\(f\\) \u5728 \\(x_i\\) \u7684\u7cbe\u786e\u503c, \u5373 \\(y_i=f(x_i)\\) , \u8bbe \\(\\hat{y}_i:= \\mathcal{N}_\\Theta (x_i)\\) \u662f\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c \\(f\\) \u7684\u8f93\u51fa. \u6211\u4eec\u8ba1\u7b97 \\(N\\) \u4e2a\u70b9\u4e0a\u7684 \\(y := [y_1, y_2,\\cdots, y_N]\\) \u4ee5\u53ca \\(\\hat{y} := [\\hat{y}_1, \\hat{y}_2,\\cdots, \\hat{y}_N]\\) , \u4ece\u800c\u5b9a\u4e49\u8bef\u5dee\u4e3a \\(\\dfrac{\\|y-\\hat{y}\\|_2}{\\|y\\|_2}\\) . \u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u5219\u901a\u8fc7\u6743\u91cd\u77e9\u9635 \\(W_i\\) \u91cc\u96f6\u5143\u7d20\u7684\u767e\u5206\u6bd4\u8fdb\u884c\u5ea6\u91cf. \u5728\u6211\u4eec\u7684\u8ba1\u7b97\u4e2d, \u6211\u4eec\u8bbe\u7f6e\u4e00\u4e2a\u6743\u91cd\u77e9\u9635\u5143\u7d20\u4e3a \\[ W^{k,j}_i= 0,\\quad \\text{if}\\ |W^{k,j}_i| < \\epsilon, \\] where \\(\\epsilon\\) is small positive number. In our numerical examples, we set \\(\\epsilon := 0.001\\) by default. For all numerical examples, the non-smooth, non-convex optimization problem (10) is solved by the Adam algorithm, which is an improved version of the stochastic gradient descent algorithm proposed in 27 for training deep learning models. \u5176\u4e2d \\(\\epsilon\\) \u662f\u4e00\u4e2a\u8f83\u5c0f\u7684\u6574\u6570. \u5728\u6211\u4eec\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u9ed8\u8ba4\u8bbe\u7f6e \\(\\epsilon=0.001\\) . \u5bf9\u4e8e\u6240\u6709\u7684\u6570\u503c\u4f8b\u5b50, \u975e\u5149\u6ed1\u975e\u51f8\u4f18\u5316\u95ee\u9898 (10) \u901a\u8fc7\u7528\u4e8e\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7684\u6539\u8fdb\u7248 Adam \u7b97\u6cd5\u8fdb\u884c\u6c42\u89e3.","title":"Function Adaptive Approximation by the SDNN Model"},{"location":"Scholars/PN-2207.13266/#intrinsic-adaptivity-of-the-sdnn-model","text":"We first investigate whether the SDNN model (10) can generate a network that has an intrinsic adaptive representation of a function. That is, a function generated by the model has a multiscale-like structure so that the reconstructed neural networks approximate functions in the same accuracy order with nearly the same order of network complexity, regardless the smoothness of the functions. In particular, when a function is singular a sparse network with higher layers is generated to capture the higher resolution information of the function. The complexity of the network is nearly proportional to the reciprocal of the approximation error regardless whether the function is smooth or not. In this experiment, we consider two examples: (1) one-dimensional functions and (2) two-dimensional functions. \u6211\u4eec\u9996\u5148\u7814\u7a76 SDNN \u6a21\u578b (10) \u662f\u5426\u80fd\u591f\u751f\u6210\u5177\u6709\u51fd\u6570\u5185\u5728\u81ea\u9002\u5e94\u8868\u793a\u7684\u7f51\u7edc. \u7531\u8be5\u6a21\u578b\u751f\u6210\u7684\u51fd\u6570\u5177\u6709\u591a\u5c3a\u5ea6\u7ed3\u6784, \u4f7f\u5f97\u91cd\u6784\u540e\u7684\u795e\u7ecf\u7f51\u7edc\u65e0\u8bba\u51fd\u6570\u7684\u5149\u6ed1\u5ea6\u5982\u4f55, \u90fd\u80fd\u4ee5\u76f8\u540c\u7684\u7cbe\u5ea6\u9636\u903c\u8fd1\u51fd\u6570, \u800c\u7f51\u7edc\u7684\u590d\u6742\u5ea6\u9636\u51e0\u4e4e\u76f8\u540c. \u7279\u522b\u5730, \u5f53\u4e00\u4e2a\u51fd\u6570\u662f\u5947\u5f02\u7684, \u4f1a\u4ea7\u751f\u4e00\u4e2a\u5177\u6709\u66f4\u591a\u5c42\u7684\u7a00\u758f\u7f51\u7edc\u6765\u6355\u83b7\u8be5\u51fd\u6570\u7684\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u4fe1\u606f. \u4e0d\u7ba1\u51fd\u6570\u662f\u5426\u5149\u6ed1, \u7f51\u7edc\u590d\u6742\u5ea6\u51e0\u4e4e\u4e0e\u903c\u8fd1\u8bef\u5dee\u7684\u5012\u6570\u6210\u6b63\u6bd4. \u5728\u8fd9\u4e2a\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u8003\u8651\u4e24\u4e2a\u4f8b\u5b50: (1) \u4e00\u7ef4\u51fd\u6570 (2) \u4e8c\u7ef4\u51fd\u6570. In our first example, we consider approximation of the quadratic function \u5728\u7b2c\u4e00\u4e2a\u4f8b\u5b50\u4e2d, \u6211\u4eec\u4f7f\u7528 SDNN \u5bf9\u4e8c\u6b21\u51fd\u6570\u548c\u4e8c\u6b21\u5206\u6bb5\u51fd\u6570\u8fdb\u884c\u8fd1\u4f3c. \\[ f(x) := x^2, \\tag{11} \\] and the piecewise quadratic function by SDNN . \\[ f(x) = \\begin{cases} x^2 + 1, &x \\geq 0,\\\\ x^2, &x < 0, \\end{cases}\\tag{12} \\] Note that the function defined by (11) is smooth and the function by (12) has a jump discontinuity at the point \\(0\\) . We applied the sparse regularized network having the architecture \\([1, 10, 10, 10, 10, 1]\\) to learn these functions. We divide the interval \\([-2, 2]\\) by the nodes \\(x_j:= -2 + jh\\) , for \\(j := 0, 1,\\cdots, 200\\) , with \\(h := 1/50\\) , and sample the functions \\(f\\) at \\(x_j\\) . The test set is \\(\\{(x_k, f(x_k))\\}\\) , where \\(x_k:= -2 + kh, h := 1/30, k = 0, 1,\\cdots, 120\\) . The network is trained by the Adam algorithm with epochs \\(20,000\\) and initial learning rate \\(0.001\\) . For function (11) , regularization parameters are set to be \\([0, 10^{-4}, 10^{-4}, 10^{-3},10^{-3}]\\) . We obtain the prediction error \\(5.94\\times10^{-3}\\) for the test set. Sparsity of the resulting weight matrices is \\([0.0\\%, 87.0\\%, 95.0\\%, 98.0\\%, 90.0\\%]\\) and the number of nonzero weight matrix entries is \\(31\\) . Left of Figure 1 shows the reconstructed SDNN for the function defined by (11) . \u6ce8\u610f\u7531 (11) \u5b9a\u4e49\u7684\u51fd\u6570\u662f\u5149\u6ed1\u7684, (12) \u5b9a\u4e49\u7684\u51fd\u6570\u5728\u70b9 \\(0\\) \u5904\u662f\u8df3\u8dc3\u95f4\u65ad\u7684. \u6211\u4eec\u4f7f\u7528\u7f51\u7edc\u67b6\u6784\u4e3a \\([1, 10, 10, 10, 10, 1]\\) \u7684\u7a00\u758f\u6b63\u5219\u5316\u7f51\u7edc\u6765\u5b66\u4e60\u8fd9\u4e9b\u51fd\u6570. \u6211\u4eec\u5c06\u533a\u95f4 \\([-2,2]\\) \u8fdb\u884c\u4e24\u767e\u7b49\u5206, \u5e76\u5728\u8fd9\u4e9b\u70b9\u4e0a\u91c7\u6837 \\(f\\) . \u6d4b\u8bd5\u96c6\u5219\u662f\u5c06\u533a\u95f4\u4e00\u767e\u4e8c\u5341\u7b49\u5206. \u7f51\u7edc\u901a\u8fc7 Adam \u7b97\u6cd5\u8bad\u7ec3 \\(20,000\\) \u4e2a epochs, \u521d\u59cb\u5b66\u4e60\u7387\u4e3a \\(0.001\\) . \u5bf9\u4e8e\u51fd\u6570 (11) , \u6b63\u5219\u5316\u53c2\u6570\u8bbe\u7f6e\u4e3a \\([0, 10^{-4}, 10^{-4}, 10^{-3},10^{-3}]\\) . \u6211\u4eec\u5728\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u4e86 \\(5.94\\times10^{-3}\\) \u7684\u9884\u6d4b\u8bef\u5dee. \u5f97\u5230\u7684\u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u4e3a \\([0.0\\%, 87.0\\%, 95.0\\%, 98.0\\%, 90.0\\%]\\) , \u6743\u91cd\u77e9\u9635\u5143\u7d20\u975e\u96f6\u6570\u91cf\u4e3a \\(31\\) . \u56fe 1 \u7684\u5de6\u56fe\u5c55\u793a\u4e86\u903c\u8fd1\u51fd\u6570 (11) \u7684 SDNN \u7684\u91cd\u6784\u7ed3\u679c. For function (12) the regularization parameters are chosen as \\([10^{-5}, 10^{-4}, 10^{-4},10^{-4}, 10^{-3}]\\) . We obtain the prediction error \\(5.42\\times10^{-3}\\) for the test set. Sparsity of the resulting weight matrices is \\([50\\%, 93.0\\%, 88.0\\%, 93.0\\%, 90.0\\%]\\) and the number of nonzero weight matrix entries is 32. The reconstructed function is shown in Right of Figure 1 . \u5bf9\u4e8e\u51fd\u6570 (12) , \u6b63\u5219\u5316\u53c2\u6570\u8bbe\u7f6e\u4e3a \\([10^{-5}, 10^{-4}, 10^{-4},10^{-4}, 10^{-3}]\\) . \u6211\u4eec\u5728\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u4e86 \\(5.42\\times10^{-3}\\) \u7684\u9884\u6d4b\u8bef\u5dee. \u5f97\u5230\u7684\u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u4e3a \\([0.0\\%, 87.0\\%, 95.0\\%, 98.0\\%, 90.0\\%]\\) , \u6743\u91cd\u77e9\u9635\u5143\u7d20\u975e\u96f6\u6570\u91cf\u4e3a \\(32\\) . \u56fe 1 \u7684\u53f3\u56fe\u5c55\u793a\u4e86\u903c\u8fd1\u51fd\u6570 (12) \u7684 SDNN \u7684\u91cd\u6784\u7ed3\u679c. Figure 1 : Numerical results of SDNN : for function (11) (Left); for function (12) (Right) Numerical results for both functions (11) and (12) are summarized in Table 1 . These results demonstrate that even though the function (12) has a jump discontinuity at the point \\(0\\) , the proposed SDNN model can generate a network with nearly the same number of nonzero weight matrix entries and with the same accuracy as those for the smooth function (11) . This shows that the proposed SDNN model has a good adaptive approximation property. \u8868\u683c 1 \u603b\u7ed3\u4e86 (11) \u548c (12) \u7684\u6570\u503c\u7ed3\u679c. \u8fd9\u4e9b\u7ed3\u679c\u8bf4\u660e\u4e86\u5c3d\u7ba1\u51fd\u6570 (12) \u5728 \\(0\\) \u5904\u8df3\u8dc3\u95f4\u65ad, \u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u80fd\u591f\u751f\u6210\u548c\u903c\u8fd1\u5149\u6ed1\u51fd\u6570 (11) \u65f6\u975e\u96f6\u6743\u91cd\u77e9\u9635\u5143\u7d20\u6570\u91cf\u51e0\u4e4e\u76f8\u540c\u4e14\u7cbe\u5ea6\u76f8\u540c\u7684\u7f51\u7edc. \u8fd9\u8bf4\u660e\u4e86\u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u6709\u826f\u597d\u7684\u81ea\u9002\u5e94\u903c\u8fd1\u6027\u8d28. Results for function (11) (12) Regularization parameters \\([0, 10^{-4}, 10^{-4}, 10^{-3}, 10^{-3}]\\) \\([10^{-5}, 10^{-4}, 10^{-4}, 10^{-4}, 10^{-3}]\\) Relative \\(L_2\\) error \\(5.94\\times10^{-3}\\) \\(5.42\\times10^{-3}\\) Sparsity of weight matrices \\([0.0\\%, 87.0\\%, 95.0\\%, 98.0\\%, 90.0\\%]\\) \\([50\\%, 93.0\\%, 88.0\\%, 93.0\\%, 90.0\\%]\\) No. of nonzero entries 31 32 Table 1 : Numerical result for quadratic function (11) and piecewise quadratic function (12) with network structure \\([1, 10, 10, 10, 10, 1]\\) . In our second example, we consider approximation of two-dimensional functions, once again one smooth function and one discontinuous function. We study smooth function \u5728\u7b2c\u4e8c\u4e2a\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u8003\u8651\u4e8c\u7ef4\u51fd\u6570, \u540c\u6837\u662f\u4e00\u4e2a\u5149\u6ed1\u51fd\u6570, \u4e00\u4e2a\u662f\u4e0d\u8fde\u7eed\u51fd\u6570. \u5149\u6ed1\u51fd\u6570\u7684\u56fe\u50cf\u5728\u56fe 2 \u5de6\u56fe\u5c55\u793a, \u5206\u6bb5\u51fd\u6570\u7684\u56fe\u50cf\u5728\u56fe 3 \u5de6\u56fe\u5c55\u793a. \\[ g(x, y) := e^{2x+y^2},\\tag{13} \\] whose image is illustrated in Figure 2 (Left), and piecewise function \\[ g_d(x, y) = \\begin{cases} e^{2x+y^2} + 1, &x \\geq 0,\\\\ e^{2x+y^2}, &x < 0,\\\\ \\end{cases}\\tag{14} \\] whose image is illustrated in Figure 3 (Left). Note that function (13) is smooth and function (14) has a jump discontinuity along \\(x = 0\\) . \u6ce8\u610f\u51fd\u6570 (13) \u662f\u5149\u6ed1\u7684, \u51fd\u6570 (14) \u6cbf\u7740 \\(x=0\\) \u6709\u8df3\u8dc3\u95f4\u65ad. For these two functions, the training data set is composed of grid points \\([-1, 1]\\times[-1, 1]\\) uniformly discretized with step size \\(1/200\\) on \\(x\\) and \\(y\\) direction, and the test set is composed of grid points \\([-1, 1]\\times[-1, 1]\\) uniformly discretized with step size \\(1/300\\) on the \\(x\\) and \\(y\\) directions. The network has \\(2\\) inputs, \\(4\\) hidden layers, and \\(1\\) output, with the architecture \\([2, 20, 20, 20, 20, 1]\\) . For each hidden layer, there are \\(20\\) neurons. The initial learning rate for Adam is set to \\(0.001\\) . The batch size is equal to \\(1024\\) . \u5bf9\u4e8e\u8fd9\u4e24\u4e2a\u51fd\u6570, \u8bad\u7ec3\u6570\u636e\u7531\u5bf9\u5b9a\u4e49\u57df \\([-1, 1]\\times[-1, 1]\\) \u5728 \\(x\\) \\(y\\) \u4e24\u4e2a\u65b9\u5411\u6b65\u957f\u4e3a \\(1/200\\) \u5f97\u5230\u7684\u7f51\u683c\u70b9\u7ec4\u6210, \u6d4b\u8bd5\u96c6\u5219\u8fdb\u884c\u4e09\u767e\u7b49\u5206. \u7f51\u7edc\u6709\u4e24\u4e2a\u8f93\u5165, \u56db\u5c42\u9690\u85cf\u5c42\u548c\u4e00\u4e2a\u8f93\u51fa, \u7f51\u7edc\u7ed3\u6784\u4e3a \\([2, 20, 20, 20, 20, 1]\\) . \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u9690\u85cf\u5c42, \u90fd\u6709\u4e8c\u5341\u4e2a\u795e\u7ecf\u5143. Adam \u521d\u59cb\u5b66\u4e60\u7387\u4e3a 0.001. \u6279\u91cf\u5927\u5c0f\u53d6 1024. For function (13) we set the sparse regularization parameters as \\([0, 10^{-6}, 10^{-4},10^{-4}, 10^{-4}]\\) . After \\(10,000\\) epochs training, the sparsity of weight matrices is \\([0.0\\%, 68.5\\%, 95.75\\%, 97.75\\%, 80.0\\%]\\) and the number of nonzero weight matrix entries is \\(178\\) . The prediction error for the test set is \\(4.38\\times10^{-3}\\) . For function (14) , the regularization parameters are set to be \\([10^{-4}, 10^{-5}, 10^{-5}, 10^{-4}, 10^{-4}]\\) . The sparsity of weight matrices after regularization are \\([60.0\\%, 71.75\\%, 81.75\\%, 97.5\\%, 90.0\\%]\\) and the number of nonzero weight matrix entries is \\(206\\) . The prediction error for sparse regularized deep neural network is \\(4.27\\times10^{-3}\\) , which is even slightly better than that for function (13) . The images of the reconstructed functions are shown respectively in Figures 2 , Figure 3 (Right). Numerical results for this example are reported in Table 2 . \u5bf9\u4e8e\u51fd\u6570 (13) , \u6211\u4eec\u8bbe\u7f6e\u7a00\u758f\u6b63\u5219\u5316\u53c2\u6570\u4e3a \\([0, 10^{-6}, 10^{-4},10^{-4}, 10^{-4}]\\) . \u5728 \\(10,000\\) \u4e2a epochs \u8bad\u7ec3\u540e, \u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u4e3a \\([0.0\\%, 68.5\\%, 95.75\\%, 97.75\\%, 80.0\\%]\\) \u4e14\u975e\u96f6\u6743\u91cd\u77e9\u9635\u5143\u7d20\u6570\u91cf\u4e3a \\(178\\) . \u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u9884\u6d4b\u8bef\u5dee\u4e3a \\(4.38\\times10^{-3}\\) . \u5bf9\u4e8e\u51fd\u6570 (14) , \u6211\u4eec\u8bbe\u7f6e\u7a00\u758f\u6b63\u5219\u5316\u53c2\u6570\u4e3a \\([10^{-4}, 10^{-5}, 10^{-5}, 10^{-4}, 10^{-4}]\\) . \u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u4e3a \\([60.0\\%, 71.75\\%, 81.75\\%, 97.5\\%, 90.0\\%]\\) \u4e14\u975e\u96f6\u6743\u91cd\u77e9\u9635\u5143\u7d20\u6570\u91cf\u4e3a \\(206\\) . \u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u9884\u6d4b\u8bef\u5dee\u4e3a \\(4.27\\times10^{-3}\\) , \u751a\u81f3\u6bd4\u51fd\u6570 (13) \u7684\u7ed3\u679c\u8fd8\u7a0d\u5fae\u597d\u4e00\u70b9. \u91cd\u6784\u7684\u51fd\u6570\u56fe\u50cf\u5206\u522b\u5728\u56fe 2 \u53f3\u56fe\u548c\u56fe 3 \u53f3\u56fe\u5c55\u793a. \u8868\u683c 2 \u62a5\u544a\u4e86\u8fd9\u4e2a\u4f8b\u5b50\u7684\u6570\u503c\u7ed3\u679c. Figure 2 : Left: image of function \\(e^{2x+y^2}\\) . Right: predicted by fully connected neural network. Figure 3 : image of piecewise discontinuous function (14) . Right: predicted by sparse regularized neural network. Results for function (13) (14) Regularization parameters \\([0, 10^{-6}, 10^{-4}, 10^{-4}, 10^{-4}]\\) \\([10^{-4}, 10^{-5}, 10^{-5}, 10^{-4}, 10^{-4}]\\) Relative \\(L_2\\) error \\(4.38\\times10^{-3}\\) \\(4.27\\times10^{-3}\\) Sparsity of weight matrices \\([0.0\\%, 68.5\\%, 95.75\\%, 97.75\\%, 80.0\\%]\\) \\([60.0\\%, 71.75\\%, 81.75\\%, 97.5\\%, 90.0\\%]\\) No. of nonzero entries 178 206 Table 2 : Numerical result for two dimensional function (13) and (14) with network structure \\([2,20, 20, 20, 20, 1]\\) . The numerical results presented in this subsection indicate that indeed the proposed SDNN model has an excellent adaptivity property in the sense that it generates networks with nearly the same number of nonzero weight matrix entries and the same order of approximation accuracy for functions regardless their smoothness. \u8fd9\u4e00\u5c0f\u8282\u5c55\u793a\u7684\u6570\u503c\u7ed3\u679c\u6307\u51fa\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u786e\u5b9e\u6709\u4f18\u79c0\u7684\u81ea\u9002\u5e94\u6027\u8d28, \u5373\u4e0d\u7ba1\u51fd\u6570\u5149\u6ed1\u6027\u5982\u4f55, \u6a21\u578b\u751f\u6210\u7684\u7f51\u7edc\u5177\u6709\u51e0\u4e4e\u76f8\u540c\u7684\u6743\u91cd\u533a\u95f4\u975e\u96f6\u8881\u672f\u6570\u91cf\u548c\u76f8\u540c\u8fd1\u4f3c\u7cbe\u5ea6\u7684\u9636\u6570.","title":"Intrinsic Adaptivity of the SDNN Model"},{"location":"Scholars/PN-2207.13266/#an-example-of-adaptive-function-approximation-by-the-sdnn-model","text":"The second experiment is designed to test the sparsity of the network learned from the SDNN model (10) and the model\u2019s generalization ability. Specifically, in this example, we demonstrate that the sparse model (10) leads to a sparse DNN with higher accuracy in comparison to the standard DNN model (9) . We consider the absolute value function \u7b2c\u4e8c\u4e2a\u5b9e\u9a8c\u65e8\u5728\u6d4b\u8bd5\u4ece SDNN \u6a21\u578b\u5b66\u4e60\u5230\u7684\u7f51\u7edc\u7684\u7a00\u758f\u6027\u548c\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b. \u5177\u4f53\u5730, \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\u6211\u4eec\u8bc1\u660e\u4e86\u7a00\u758f\u6a21\u578b\u80fd\u591f\u5bfc\u51fa\u76f8\u6bd4\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u66f4\u9ad8\u7cbe\u5ea6\u7684\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc. \u6211\u4eec\u8003\u8651\u7edd\u5bf9\u503c\u51fd\u6570 \\[ y = f(x) := |x|, \\text{for} x \\in \\mathbb{R}. \\] Note that function \\(f\\) is not differentiable at \\(x = 0\\) . \u6ce8\u610f\u51fd\u6570 \\(f\\) \u5728 \\(x=0\\) \u5904\u4e0d\u53ef\u5fae. We adopt the same network architecture, that is, \\(1\\) input layer, \\(2\\) hidden layers, \\(1\\) output layer and each hidden layer containing \\(5\\) neurons, for both the standard DNN model (9) and the SDNN model (10) . The training set is composed of equal-distance grid points laying in \\([-2, 2]\\) with step size \\(0.01\\) . The test set is composed of equal-distance grid points in \\([-5, 5]\\) with step size \\(0.1\\) . For the sparse regularized network, the regularization parameters are set as \\([10^{-4}, 10^{-3}, 10^{-3}]\\) . For both the standard DNN model and the SDNN model, the number of epoch equals \\(10,000\\) . The initial learning rate is set to \\(0.001\\) . \u6211\u4eec\u5bf9\u4e8e\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b (9) \u548c SDNN \u6a21\u578b (10) \u91c7\u7528\u76f8\u540c\u7684\u7f51\u7edc\u67b6\u6784, \u5373\u4e00\u5c42\u8f93\u5165\u5c42, \\(2\\) \u5c42\u9690\u85cf\u5c42, \\(1\\) \u5c42\u8f93\u51fa\u5c42, \u6bcf\u5c42\u9690\u85cf\u5c42\u6709 \\(5\\) \u4e2a\u795e\u7ecf\u5143. \u8bad\u7ec3\u96c6\u7531\u5728\u533a\u95f4 \\([-2,2]\\) \u4e0a\u8bbe\u7f6e\u6b65\u957f\u4e3a \\(0.01\\) \u7684\u7b49\u8ddd\u683c\u70b9\u7ec4\u6210. \u6d4b\u8bd5\u96c6\u7531\u5728\u533a\u95f4 \\([-5,5]\\) \u4e0a\u8bbe\u7f6e\u6b65\u957f\u4e3a \\(0.1\\) \u7684\u7b49\u8ddd\u683c\u70b9\u7ec4\u6210. \u5bf9\u4e8e\u7a00\u758f\u6b63\u5219\u5316\u7f51\u7edc, \u6b63\u5219\u5316\u53c2\u6570\u8bbe\u7f6e\u4e3a \\([10^{-4}, 10^{-3}, 10^{-3}]\\) . \u5bf9\u4e8e\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u548c SDNN \u6a21\u578b, epochs \u7684\u6570\u91cf\u5747\u4e3a \\(10,000\\) . \u521d\u59cb\u5b66\u4e60\u7387\u8bbe\u7f6e\u4e3a \\(0.001\\) . We present numerical results of this experiment in Table 3 , where we compare errors and sparsity of the functions learned from the two models. Clearly, the network learned from the standard DNN model is non-sparse: all entries of its weight matrices are nonzero. While the network learned from the SDNN model has a good sparsity property: There are only \\(1\\) non-zero entries in \\(W_3\\) and \\(2\\) non-zero entries in \\(W_2\\) in the network learned from the SDNN model. Note that the absolution value function is the linear composition of two ReLU functions, that is \\(|x| = \\text{ReLU}(x)+\\text{ReLU}(-x)\\) . The SDNN model is able to find a linear combination of the two functions to represent the function \\(f(x) := |x|\\) but the standard DNN model fails to do so. \u6211\u4eec\u5728\u8868\u683c 3 \u4e2d\u5c55\u793a\u4e86\u8fd9\u4e00\u5b9e\u9a8c\u7684\u6570\u503c\u7ed3\u679c, \u5176\u4e2d\u6211\u4eec\u5bf9\u6bd4\u4e86\u4ece\u4e24\u4e2a\u6a21\u578b\u5b66\u4e60\u5230\u7684\u51fd\u6570\u7684\u8bef\u5dee\u548c\u7a00\u758f\u5ea6. \u5f88\u660e\u663e, \u4ece\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\u5b66\u4e60\u5f97\u7f51\u7edc\u662f\u975e\u7a00\u758f\u7684: \u6743\u91cd\u77e9\u9635\u7684\u6240\u6709\u5143\u7d20\u90fd\u4e0d\u4e3a\u96f6. \u800c\u4ece SDNN \u6a21\u578b\u4e2d\u5b66\u4e60\u7684\u7f51\u7edc\u7531\u826f\u597d\u7684\u7a00\u758f\u6027\u8d28: \\(W_3\\) \u4e2d\u53ea\u6709\u4e00\u4e2a\u975e\u96f6\u5143\u7d20, \\(W_2\\) \u4e2d\u53ea\u6709\u4e24\u4e2a\u975e\u96f6\u5143\u7d20. \u6ce8\u610f\u7edd\u5bf9\u503c\u51fd\u6570\u662f\u4e24\u4e2a\u6574\u6d41\u7ebf\u6027\u5355\u5143\u7684\u7ebf\u6027\u7ec4\u5408, \u5373 \\(|x| = \\text{ReLU}(x)+\\text{ReLU}(-x)\\) . SDNN \u6a21\u578b\u80fd\u591f\u627e\u5230\u4e24\u4e2a\u51fd\u6570\u7684\u7ebf\u6027\u7ec4\u5408\u6765\u8868\u793a\u7edd\u5bf9\u503c\u51fd\u6570, \u800c\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5219\u4e0d\u884c. Model Relative \\(L_2\\) error Sparsity of weight matrices \\([W_1, W_2, W_3]\\) Standard DNN model \\(5.58\\times10^{-2}\\) \\([0\\%, 0\\%, 0\\%]\\) SDNN model \\(1.87\\times10^{-3}\\) \\([20\\%, 92\\%, 80\\%]\\) Table 3 : Approximation of the absolute value function by a SDNN with regularization parameters \\([10^{-4}, 10^{-3}, 10^{-3}]\\) . We plot the graphs of the reconstructed functions by the standard DNN model (9) and the SDNN model (10) in Figure 4 and Figure 5 , respectively. It can be seen from Figure 4 that the function reconstructed by the standard DNN model (9) has large errors in the interval \\([3, 5]\\) . Figure 5 shows that the function reconstructed by the SDNN model (10) almost coincides with the original function. This example indicates that the SDNN model (10) has better generalization ability than the standard DNN model (9) . \u6211\u4eec\u5206\u522b\u5728\u56fe 4 \u548c\u56fe 5 \u4e2d\u7ed8\u5236\u4e86\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u548c SDNN \u6a21\u578b\u7684\u91cd\u6784\u51fd\u6570. \u53ef\u4ee5\u4ece\u56fe 4 \u4e2d\u770b\u5230\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u91cd\u6784\u51fd\u6570\u5728\u533a\u95f4 \\([3,5]\\) \u4e0a\u6709\u8f83\u5927\u8bef\u5dee. \u56fe 5 \u5219\u5c55\u793a\u4e86 SDNN \u6a21\u578b\u548c\u539f\u59cb\u51fd\u6570\u57fa\u672c\u76f8\u7b26. \u8fd9\u4e00\u4f8b\u5b50\u8bf4\u660e\u4e86 SDNN \u6a21\u578b\u6bd4\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b. Figure 4 (Left) : Reconstruction of function \\(f(x) := |x|\\) by the standard DNN model (9) . Figure 5 (Right) : Reconstruction of function \\(f(x) := |x|\\) by the SDNN model (10) .","title":"An Example of Adaptive Function Approximation by the SDNN Model"},{"location":"Scholars/PN-2207.13266/#reconstruction-of-a-black-hole","text":"In this example, we consider reconstruction of the image of a black hole by the SDNN model. Specifically, we compare numerical results and reconstructed image quality of the SDNN model with those of the standard DNN model. We choose a color image of the black hole shown in Figure 6 (Left), which is turned into a gray image shown in Figure 6 (Right). The image has the size \\(128 \\times 128\\) and can be represented as a two-dimensional discrete function. The value of the gray image at the point \\((x_1, x_2)\\) is defined as a \\(f_{image}(x_1, x_2), x_1, x_2= 1, 2,\\cdots, 128\\) . The function clearly has singularities. \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d, \u6211\u4eec\u8003\u8651\u901a\u8fc7 SDNN \u6a21\u578b \u91cd\u6784\u9ed1\u6d1e\u56fe\u50cf. \u5177\u4f53\u5730, \u6211\u4eec\u5c06\u6bd4\u8f83 SDNN \u6a21\u578b\u548c\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u6570\u503c\u7ed3\u679c\u548c\u91cd\u6784\u56fe\u50cf\u8d28\u91cf. \u6211\u4eec\u9009\u62e9\u56fe 6 \u5de6\u56fe\u6240\u793a\u7684\u9ed1\u6d1e\u7684\u5f69\u8272\u56fe\u7247, \u8f6c\u5316\u4e3a\u56fe 6 \u53f3\u56fe\u6240\u793a\u7684\u7070\u5ea6\u56fe\u7247. \u56fe\u7247\u5927\u5c0f\u4e3a \\(128\\times 128\\) \u80fd\u591f\u8868\u793a\u4e3a\u4e8c\u7ef4\u79bb\u6563\u51fd\u6570. \u7070\u5ea6\u56fe\u5728 \\((x_1,x_2)\\) \u7684\u503c\u5b9a\u4e49\u4e3a \\(f_{image}(x_1, x_2)\\) , \u8fd9\u4e2a\u51fd\u6570\u663e\u7136\u5177\u6709\u5947\u6027. Figure 6 : Left: color image of the black hole. Right: gray image of the black hole. The network architecture that we used for the construction is \\([2, 100, 100, 100, 100, 100, 100, 1]\\) . We randomly choose \\(5,000\\) points \\((x^i_1, x^i_2, f_{image}(x^i_1, x^i_2))\\) by uniform sampling, \\(i =1, 2,\\cdots, 5,000\\) , from the image of the black hole to train both the standard neural network and the sparse regularized network. The optimizer is chosen as the Adam algorithm with batch size \\(1,024\\) . The number of epoch is \\(40,000\\) . The patience parameter of early stopping is \\(200\\) . Prediction results by the standard DNN model and by the SDNN model are shown respectively on Figure 7 (Left) and (Right). \u6211\u4eec\u4f7f\u7528\u7684\u7f51\u7edc\u67b6\u6784\u4e3a \\([2, 100, 100, 100, 100, 100, 100, 1]\\) . \u6211\u4eec\u4ece\u9ed1\u6d1e\u56fe\u50cf\u4e2d\u5747\u5300\u91c7\u6837 \\(5,000\\) \u4e2a\u6837\u672c\u70b9 \\((x^i_1, x^i_2, f_{image}(x^i_1, x^i_2))\\) \u7528\u4e8e\u8bad\u7ec3\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u7a00\u758f\u6b63\u5219\u5316\u7f51\u7edc. \u9009\u62e9 Adam \u7b97\u6cd5\u4f5c\u4e3a\u4f18\u5316\u5668, \u6279\u91cf\u5927\u5c0f\u4e3a \\(1024\\) . epoch \u7684\u6570\u91cf\u4e3a \\(40,000\\) . \u65e9\u505c\u6cd5\u7684\u8010\u5fc3\u53c2\u6570\u8bbe\u7f6e\u4e3a \\(200\\) , \u5373\u5141\u8bb8 \\(200\\) \u4e2a epochs \u5185\u6a21\u578b\u6027\u80fd\u6ca1\u6709\u63d0\u5347. \u56fe 7 \u7684\u5de6\u56fe\u548c\u53f3\u56fe\u5206\u522b\u5c55\u793a\u4e86\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c SDNN \u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c. Figure 7 : Images of the black hole reconstructed: by the standard DNN model (Left) and by the SDNN model (Right). Error images of the two models are presented in Figure 8 , from which it can be seen that the sparse network has a smaller reconstruction error. The prediction error of the fully connected network is \\(9.66\\times10^{-3}\\) . For the sparse regularized neural network, the regularized parameters are set to be \\([10^{-9}, 10^{-9}, 10^{-9}, 10^{-9}, 10^{-8}, 10^{-8}, 10^{-8}]\\) . The prediction error of the sparse regularized network is \\(9.28\\times10^{-3}\\) . The sparsity of the weight matrices are \\([44.0\\%, 78.3\\%, 78.4\\%, 80.3\\%, 96.5\\%, 98.2\\%, 84.0\\%]\\) . It shows that the sparse regularized network uses fewer neurons and has smaller prediction error. This indicates that by using the proposed multi-parameter sparse regularization, the deep neural network has the ability of multi-scale and adaptive learning. \u4e24\u4e2a\u6a21\u578b\u7684\u8bef\u5dee\u56fe\u50cf\u5728\u56fe 8 \u4e2d\u5c55\u793a, \u80fd\u591f\u4ece\u4e2d\u770b\u5230\u7a00\u758f\u7f51\u7edc\u5177\u6709\u66f4\u5c0f\u7684\u91cd\u6784\u8bef\u5dee. \u5168\u8fde\u63a5\u7f51\u7edc\u7684\u9884\u6d4b\u8bef\u5dee\u4e3a \\(9.66\\times10^{-3}\\) . \u5bf9\u4e8e\u7a00\u758f\u6b63\u5219\u5316\u795e\u7ecf\u7f51\u7edc, \u6b63\u5219\u5316\u53c2\u6570\u8bbe\u7f6e\u4e3a \\([10^{-9}, 10^{-9}, 10^{-9}, 10^{-9}, 10^{-8}, 10^{-8}, 10^{-8}]\\) . \u7a00\u758f\u6b63\u5219\u5316\u7f51\u7edc\u7684\u9884\u6d4b\u8bef\u5dee\u4e3a \\(9.28\\times10^{-3}\\) . \u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u4e3a \\([44.0\\%, 78.3\\%, 78.4\\%, 80.3\\%, 96.5\\%, 98.2\\%, 84.0\\%]\\) . \u8fd9\u5c55\u793a\u4e86\u7a00\u758f\u6b63\u5219\u5316\u7f51\u7edc\u4f7f\u7528\u4e86\u66f4\u5c11\u7684\u795e\u7ecf\u5143\u4e14\u83b7\u5f97\u4e86\u66f4\u5c0f\u7684\u9884\u6d4b\u8bef\u5dee. \u8fd9\u8bf4\u660e\u4e86\u901a\u8fc7\u4f7f\u7528\u6211\u4eec\u63d0\u51fa\u7684\u591a\u53c2\u6570\u7a00\u758f\u6b63\u5219\u5316, \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6709\u591a\u5c3a\u5ea6\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u80fd\u529b. Figure 8 : Reconstruction errors of the black hole: by the standard DNN model (Left) and by the SDNN model (Right). Model Relative \\(L_2\\) error Sparsity of weight matrices Standard DNN model \\(9.66\\times10^{-3}\\) \\([0\\%, 0\\%, 0\\%, 0\\%, 0\\%, 0\\%, 0\\%]\\) SDNN model \\(9.28\\times10^{-3}\\) \\([44.0\\%, 78.3\\%, 78.4\\%, 80.3\\%, 96.5\\%, 98.2\\%, 84.0\\%]\\) Table 4 : Numerical results of the black hole reconstructed by the standard DNN model vs. the SDNN model.","title":"Reconstruction of A Black Hole"},{"location":"Scholars/PN-2207.13266/#numerical-solutions-of-partial-differential-equations","text":"We study in this section numerical performance of the proposed SDNN model for solving partial differential equations. We consider two equations: the Burgers equation and the Schr\u00f6dinger equation. For both of these two equations, we choose the hyperbolic tangent (tanh) function defined by \u672c\u8282\u6211\u4eec\u7814\u7a76\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u7528\u4e8e\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u6570\u503c\u6027\u80fd. \u6211\u4eec\u8003\u8651\u4e24\u4e2a\u65b9\u7a0b: Burgers \u65b9\u7a0b\u548c Schr\u00f6dinger \u65b9\u7a0b. \\[ \\tanh(x) := \\frac{e^x-e^{-x}}{e^x+ e^{-x}}, x \\in \\mathbb{R} \\] as the activation function to build networks for our approximate solutions due to its differentiability which is required by the differential equations. \u5bf9\u8fd9\u4e24\u4e2a\u65b9\u7a0b, \u6211\u4eec\u90fd\u4f7f\u7528\u53cc\u66f2\u6b63\u5207\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\u6765\u5efa\u7acb\u7f51\u7edc\u4ee5\u8fd1\u4f3c\u89e3\u51fd\u6570. \u8fd9\u662f\u56e0\u4e3a\u5b83\u7684\u53ef\u5fae\u5206\u6027\u6b63\u662f\u5fae\u5206\u65b9\u7a0b\u6240\u8981\u6c42\u7684.","title":"Numerical Solutions of Partial Differential Equations"},{"location":"Scholars/PN-2207.13266/#the-burgers-equation","text":"The Burgers equation has attracted much attention since it is often used as simplified model for turbulence and shock waves 31 . It is well-known that the solution of this equation presents a jump discontinuity (a shock wave), even though the initial function is smooth. Burgers \u65b9\u7a0b\u7531\u4e8e\u7ecf\u5e38\u4f5c\u4e3a\u6e4d\u6d41\u548c\u6fc0\u6ce2\u7684\u7b80\u5316\u6a21\u578b\u800c\u53d7\u5230\u5f88\u591a\u5173\u6ce8. \u4f17\u6240\u5468\u77e5\u5373\u4f7f\u521d\u59cb\u51fd\u6570\u662f\u5149\u6ed1\u7684, \u8fd9\u4e2a\u65b9\u7a0b\u7684\u89e3\u4e5f\u5b58\u5728\u8df3\u8dc3\u4e0d\u8fde\u7eed\u6027 (\u6fc0\u6ce2). In this example, we consider the following one dimensional Burgers equation \u5728\u8fd9\u4e00\u4f8b\u5b50\u4e2d, \u6211\u4eec\u8003\u8651\u5982\u4e0b\u4e00\u7ef4 Burgers \u65b9\u7a0b. \\[ \\begin{align} &u_t(t, x) + u(t, x)u_x(t, x) - \\frac{0.01}{\\pi} u_{xx}(t, x) = 0, &t \\in (0, 1], x \\in (-1, 1), \\tag{15}\\\\ &u(0, x) = - \\sin(\\pi x),\\tag{16}\\\\ &u(t,-1) = u(t, 1) = 0.\\tag{17} \\end{align} \\] The analytic solution of this equation, known in 2 , will be used as our exact solution for comparison. Indeed, the analytic solution has the form \u8fd9\u4e00\u65b9\u7a0b\u7684\u89e3\u6790\u89e3\u5c06\u4f5c\u4e3a\u6211\u4eec\u7684\u7cbe\u786e\u89e3\u4ee5\u8fdb\u884c\u5bf9\u6bd4. \u89e3\u6790\u89e3\u7684\u5f62\u5f0f\u5982\u4e0b \\[ u(t, x) := -\\frac{\\int_{-\\infty}^\\infty \\sin\\pi(x-\\eta)h(x-\\eta)\\exp(-\\eta^2/4vt)\\text{d}\\eta}{\\int_{-\\infty}^\\infty h(x-\\eta)\\exp(-\\eta^2/4vt)\\text{d}\\eta}t \\in [0, 1], x \\in [-1, 1], \\] where \\(\u03bd := \\dfrac{0.01}{\\pi}\\) and \\(h(y) := \\exp(-\\cos \\pi y/2\\pi \u03bd)\\) . A neural network solution of equation (15) - (17) was obtained recently from the standard DNN model in 35 . \u4e0a\u8ff0\u65b9\u7a0b\u7684\u795e\u7ecf\u7f51\u7edc\u89e3\u662f\u4ece PINN \u8bba\u6587\u7684\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u83b7\u5f97\u7684. We apply the setting (1) - (3) with \u6211\u4eec\u5c06\u504f\u5fae\u5206\u65b9\u7a0b\u4e00\u822c\u5f62\u5f0f\u4ee3\u5165\u53ef\u77e5 \\[ \\mathcal{F}(u(t, x)) := u_t(t, x) + u(t, x)u_x(t, x) - \\frac{0.01}{\\pi} u_{xx}(t, x), t \\in (0, 1], x \\in (-1, 1). \\] Let \\(\\{x^i_0, u^i_0\\}^{N_0}_{i=1}\\) denote the training data of \\(u\\) satisfying initial condition (16) , that is, \\(u^i_0= - \\sin(\\pi x^i_0)\\) . Let \\(\\{t^i_{b_1}\\}_{i=1}^{N_{b_1}}\\) and \\(\\{t^i_{b_2}\\}_{i=1}^{N_{b_2}}\\) be the collocation points related to boundary condition (17) for \\(x = -1\\) and \\(x = 1\\) respectively. We denote by \\(\\{t^i_f, x^i_f\\}_{i=1}^{N_f}\\) the collocation points for \\(\\mathcal{F}(u(t, x))\\) in \\([0, 1]\\times(-1, 1)\\) . The sparse deep neural network \\(\\mathcal{N}_\\Theta (t, x)\\) are learned by model (8) with \u4ee4 \\(\\{x^i_0, u^i_0\\}^{N_0}_{i=1}\\) \u8868\u793a \\(u\\) \u6ee1\u8db3\u521d\u59cb\u6761\u4ef6\u7684\u8bad\u7ec3\u6570\u636e, \u5373 \\(u^i_0= - \\sin(\\pi x^i_0)\\) . \u4ee4 \\(\\{t^i_{b_1}\\}_{i=1}^{N_{b_1}}\\) \u548c \\(\\{t^i_{b_2}\\}_{i=1}^{N_{b_2}}\\) \u5206\u522b\u8868\u793a\u4e0e\u8fb9\u754c\u6761\u4ef6 \\(x=-1\\) \u548c \\(x=1\\) \u76f8\u5173\u7684\u914d\u7f6e\u70b9. \u4ee4 \\(\\{t^i_f, x^i_f\\}_{i=1}^{N_f}\\) \u8868\u793a\u6ee1\u8db3\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u914d\u7f6e\u70b9. \u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc \\(\\mathcal{N}_\\Theta (t, x)\\) \u901a\u8fc7\u6a21\u578b (8) \u53ca\u4ee5\u4e0b\u635f\u5931\u8fdb\u884c\u5b66\u4e60. \\[ \\begin{aligned} Loss_0 &=\\frac{1}{N_0}\\sum_{i=1}^{N_0}|\\mathcal{N}_\\Theta (0, x^i_0) - u^i_0|^2,\\\\ Loss_b &=\\frac{1}{N_b}\\sum_{i=1}^{N_b}|\\mathcal{N}_\\Theta (t^i_{b_1}, -1)| +\\frac{1}{N_{b_2}}\\sum_{i=1}^{N_{b_2}}|\\mathcal{N}_\\Theta (t^i_{b_2}, 1)|. \\end{aligned} \\] In this experiment, \\(100\\) data points are randomly selected from boundary and initial data points, among which \\(N_{b_1}= 25\\) points are located on the boundary \\(x = -1\\) , \\(N_{b_2}= 23\\) points on the boundary \\(x = 1\\) , and \\(N_0= 52\\) points on the initial line \\(t = 0\\) . The distribution of random collocation points is shown in the top of Figure 9 . The number of collocation points of the partial differential equation is \\(N_f= 10,000\\) by employing the Latin hypercube sampling method. The test set is composed of grid points \\([0, 1] \\times [-1, 1]\\) uniformly discretized with step size \\(1/100\\) on the \\(t\\) direction and step size \\(2/255\\) on the \\(x\\) direction. \u5728\u8fd9\u4e2a\u5b9e\u9a8c\u4e2d, \u4ece\u8fb9\u754c\u548c\u521d\u59cb\u6570\u636e\u70b9\u968f\u673a\u9009\u62e9 \\(100\\) \u4e2a\u6570\u636e\u70b9: \\(N_{b_1}= 25\\) \u4e2a\u5728\u8fb9\u754c \\(x=-1\\) \u4e0a, \\(N_{b_2}=23\\) \u4e2a\u5728\u8fb9\u754c \\(x=1\\) \u4e0a, \\(N_0=52\\) \u4e2a\u5728\u521d\u59cb\u7ebf \\(t=0\\) \u4e0a. \u968f\u673a\u914d\u7f6e\u70b9\u7684\u5206\u5e03\u5c55\u793a\u5728\u56fe 9 \u7684\u9876\u90e8. \u504f\u5fae\u5206\u65b9\u7a0b\u7684\u914d\u7f6e\u70b9\u6570\u91cf\u4e3a \\(N_f=10,000\\) \u901a\u8fc7 LHS \u91c7\u6837\u65b9\u6cd5\u83b7\u5f97. \u6d4b\u8bd5\u96c6\u5219\u7531\u5b9a\u4e49\u57df \\([0, 1] \\times [-1, 1]\\) \u4e0a\u65f6\u95f4 \\(t\\) \u65b9\u5411\u6b65\u957f\u4e3a \\(0.01\\) , \\(x\\) \u65b9\u5411\u4e0a\u6b65\u957f\u4e3a \\(2/225\\) \u7684\u5747\u5300\u7f51\u683c\u70b9\u7ec4\u6210. Figure 9 : Burgers equation: Top: The training data and predicted solution \\(u(t, x)\\) for sparse deep neural network with \\([2, 50, 50, 50, 1]\\) , regularization parameter \\(\\alpha = [10^{-6}, 10^{-6}, 10^{-6}, 10^{-4}]\\) , \\(\\beta = 20\\) . Bottom: Predicted solution \\(u(t, x)\\) at time \\(t = 0.3\\) , \\(t = 0.6\\) , and \\(t = 0.8\\) . We use two different network architectures \\([2, 50, 50, 50, 1]\\) and \\([2, 50, 50, 50,50, 50, 50, 50, 1]\\) for DNNs. We choose Adam as the optimizer for both neural networks. The number of epoch is \\(30,000\\) . The initial learning rate is set to \\(0.001\\) . Numerical results of these two networks presented respectively in Table 5 and Table 6 show that the proposed SDNN model outperforms the PINN model in both weight matrix sparsity and approximation accuracy. \u6211\u4eec\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784 \\([2, 50, 50, 50, 1]\\) \u548c \\([2, 50, 50, 50,50, 50, 50, 50, 1]\\) . \u6211\u4eec\u9009\u62e9 Adam \u4f5c\u4e3a\u4f18\u5316\u5668. epochs \u7684\u6570\u91cf\u4e3a \\(30,000\\) . \u521d\u59cb\u5b66\u4e60\u7387\u8bbe\u7f6e\u4e3a \\(0.001\\) . \u8fd9\u4e24\u4e2a\u7f51\u7edc\u7684\u6570\u503c\u7ed3\u679c\u5206\u522b\u5c55\u793a\u5728\u8868\u683c 5 \u548c\u8868\u683c 6 \u91cc. \u8868\u683c\u8bf4\u660e\u4e86\u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u5728\u6743\u91cd\u77e9\u9635\u7a00\u758f\u5ea6\u548c\u8fd1\u4f3c\u8fdb\u5ea6\u4e0a\u90fd\u6bd4 PINN \u6a21\u578b\u4f18\u8d8a. Algorithms Parameters \\(\\alpha\\) & sparsity of weight matrices Relative L2error PINN No regularization \\([0.0\\%, 0.2\\%, 0.5\\%, 0.0\\%]\\) \\(2.45\\times10^{-2}\\) SDNN ( \\(\\beta=20\\) ) \\([10^{-6}, 10^{-6}, 10^{-6}, 10^{-4}]\\) \\([13.0\\%, 61.9\\%, 71.2\\%, 62.0\\%]\\) \\(1.68\\times10^{-3}\\) Table 5 : The Burgers equation: A neural network of \\(4\\) layers, with network architecture \\([2, 50,50, 50, 1]\\) . Algorithms Parameters \\(\\alpha\\) & sparsity of weight matrices Relative L2error PINN No regularization \\([0.0\\%, 0.8\\%, 0.6\\%, 0.6\\%, 0.8\\%, 0.6\\%, 0.4\\%, 0.0\\%]\\) \\(3.39\\times10^{-3}\\) SDNN ( \\(\\beta=10\\) ) \\([0, 0, 0, 0, 10^{-7}, 10^{-1}0, 10^{-6}, 10^{-5}]\\) \\([0.0\\%, 0.7\\%, 0.8\\%, 0.7\\%, 15.6\\%, 0.5\\%, 93.8\\%, 94.0\\%]\\) \\(1.45\\times10^{-4}\\) SDNN ( \\(\\beta=10\\) ) \\([10^{-6}, 10^{-6}, 10^{-6}, 10^{-6}, 10^{-6}, 10^{-6}, 10^{-5}, 10^{-5}]\\) \\([25.0\\%, 78.6\\%, 85.3\\%, 82.8\\%, 79.5\\%, 84.0\\%, 98.6\\%, 94.0\\%]\\) \\(4.83\\times10^{-4}\\) Table 6 : The Burgers equation: Neural networks of \\(8\\) layers with network architecture \\([2, 50, 50,50, 50, 50, 50, 50, 1]\\) .","title":"The Burgers Equation"},{"location":"Scholars/PN-2207.13266/#the-schrodinger-equation","text":"The Schr\u00f6dinger equation is the most essential equation of non-relativistic quantum mechanics. It plays an important role in studying nonlinear optics, Bose-Einstein condensates, protein folding and bending. It is also a model equation for studying waves propagation and soliton 36 . In this subsection, we consider a one-dimensional Schr\u00f6dinger equation with periodic boundary conditions Schr\u00f6dinger \u65b9\u7a0b\u662f\u975e\u76f8\u5bf9\u8bba\u91cf\u5b50\u529b\u5b66\u7684\u6700\u57fa\u672c\u7684\u65b9\u7a0b. \u5b83\u5728\u7814\u7a76\u975e\u7ebf\u6027\u5149\u5b66, Bose-Einstein \u51dd\u805a, \u86cb\u767d\u8d28\u6298\u53e0\u548c\u5f2f\u66f2\u65b9\u9762\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528. \u5b83\u4e5f\u662f\u7814\u7a76\u6ce2\u4f20\u64ad\u548c\u5b64\u5b50\u7684\u4e00\u4e2a\u6a21\u578b\u65b9\u7a0b. \u5728\u8fd9\u4e00\u5c0f\u8282\u4e2d, \u6211\u4eec\u8003\u8651\u5e26\u6709\u5468\u671f\u8fb9\u503c\u6761\u4ef6\u7684\u4e00\u7ef4 Schr\u00f6dinger \u65b9\u7a0b. \\[ \\begin{aligned} &iu_t(t, x) + 0.5u_{xx}(t, x) + |u(t, x)|^2u(t, x) = 0, t \\in (0, \\pi/2], x \\in (-5, 5), \\\\ &u(0, x) = 2 \\text{sech}(x),\\\\ &u(t, -5) = u(t, 5), \\\\ &u_x(t, -5) = u_x(t, 5).\\tag{18} \\end{aligned} \\] Note that the solution \\(u\\) of problem (18) is a complex-valued function. The goal of this study is to test the effectiveness of the proposed SDNN model in solving complex-valued nonlinear differential equations with periodic boundary conditions, with a comparison to the standard DNN model recently developed in 35 . \u6ce8\u610f\u5230\u8fd9\u4e00\u95ee\u9898\u7684\u89e3 \\(u\\) \u662f\u4e00\u4e2a\u590d\u503c\u51fd\u6570. \u8fd9\u4e00\u7814\u7a76\u7684\u76ee\u7684\u662f\u6d4b\u8bd5\u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u5728\u6c42\u89e3\u5e26\u6709\u5468\u671f\u8fb9\u503c\u6761\u4ef6\u7684\u590d\u503c\u975e\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u5f97\u5230\u6709\u6548\u6027, \u5e76\u4e0e\u57fa\u4e8e PINN \u7684\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83. Problem (18) falls into the setting (1) - (3) with \u4e0a\u8ff0\u95ee\u9898\u4ee3\u5165\u4e00\u822c\u504f\u5fae\u5206\u65b9\u7a0b\u8bbe\u7f6e\u53ef\u77e5 \\[ \\mathcal{F}(u(t, x)) := iu_t(t, x) + 0.5u_{xx}(t, x) + |u(t, x)|^2u(t, x), t \\in (0, \\pi/2], x \\in (-5, 5). \\] Let \\(\\psi\\) and \\(\\phi\\) be respectively the real part and imaginary part of the solution \\(u\\) of problem (18) . We intend to approximate the solution \\(u(t, x)\\) by a neural network \\(\\mathcal{N}_\\Theta (t, x)\\) with two inputs \\((t, x)\\) and two outputs which approximate \\(\\psi(t, x)\\) and \\(\\phi(t, x)\\) , respectively. Let \\(\\{x^i_0, u^i_0\\}^{N_0}_{i=1}\\) denote the training data to enforce the initial condition at time \\(t = 0\\) , that is, \\(u^i_0= \\text{sech}(x^i_0)\\) , \\(\\{t^i_b\\}^{N_b}_{i=1}\\) the collocation points on the boundary \\(x = -5\\) and \\(x = 5\\) to enforce the periodic boundary conditions, and \\(\\{t^i_f, x^i_f\\}^{N_f}_{i=1}\\) the collocation points in \\((0, \\pi/2]\\times (-5, 5)\\) . These collocation points were generated by the Latin hypercube sampling method. We then learn the neural network \\(\\mathcal{N}_\\Theta (t, x)\\) by model (8) with \u4ee4 \\(\\psi\\) \u548c \\(\\phi\\) \u5206\u522b\u8868\u793a\u89e3 \\(u\\) \u7684\u5b9e\u90e8\u548c\u865a\u90e8. \u6211\u4eec\u8bd5\u56fe\u7528\u5177\u6709\u4e24\u4e2a\u8f93\u5165 \\((t, x)\\) \u548c\u4e24\u4e2a\u8f93\u51fa\u5206\u522b\u8fd1\u4f3c \\(\\psi(t, x)\\) \u548c \\(\\phi(t, x)\\) \u7684\u795e\u7ecf\u7f51\u7edc \\(\\mathcal{N}_\\Theta (t, x)\\) \u6765\u8fd1\u4f3c\u65b9\u7a0b\u7684\u89e3 \\(u(t, x)\\) . \u4ee4 \\(\\{x^i_0, u^i_0\\}^{N_0}_{i=1}\\) \u8868\u793a\u7528\u4e8e\u6ee1\u8db3 \\(t=0\\) \u4e0a\u521d\u503c\u6761\u4ef6\u7684\u8bad\u7ec3\u6570\u636e, \u5373 \\(u^i_0= \\text{sech}(x^i_0)\\) , \\(\\{t^i_b\\}^{N_b}_{i=1}\\) \u8868\u793a\u7528\u4e8e\u6ee1\u8db3 \\(x=-5\\) \u548c \\(x=5\\) \u4e0a\u5468\u671f\u8fb9\u503c\u6761\u4ef6\u7684\u914d\u7f6e\u70b9. \\(\\{t^i_f, x^i_f\\}^{N_f}_{i=1}\\) \u8868\u793a\u5728 \\((0, \\pi/2]\\times (-5, 5)\\) \u7684\u914d\u7f6e\u70b9. \u8fd9\u4e9b\u914d\u7f6e\u70b9\u901a\u8fc7 LHS \u65b9\u6cd5\u751f\u6210. \u7136\u540e\u6211\u4eec\u901a\u8fc7\u5177\u6709\u4ee5\u4e0b\u635f\u5931\u7684\u6a21\u578b (8) \u6765\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc \\(\\mathcal{N}_\\Theta (t, x)\\) . \\[ \\begin{aligned} Loss_0&:= \\frac{1}{N_0}\\sum_{i=1}^{N_0} |\\mathcal{N}_\\Theta(0,x^i_0)-u^i_0|^2\\\\ Loss_b&:= \\frac{1}{N_b}\\sum_{i=1}^{N_b} \\bigg(|\\mathcal{N}_\\Theta(t^i_b, -5)- \\mathcal{N}_\\Theta(t^i_b, 5)|^2+\\Big|\\frac{\\partial \\mathcal{N}_\\Theta}{\\partial x}(t^i_b, -5)-\\frac{\\partial \\mathcal{N}_\\Theta}{\\partial x}(t^i_b, 5)\\Big|^2\\bigg) \\end{aligned} \\] A reference solution of problem (18) is solved by a Fourier spectral method using the Chebfun package 18 . Specifically, we obtain the reference solution by using \\(256\\) Fourier modes for space discretization and an explicit fourth-order Runge-Kutta method (RK4) with time-step \\(\\Delta t := (\\pi/2) \\times 10^{-6}\\) for time discretization. For more details of the discretization of Schr\u00f6dinger equation (18) , the readers are referred to 35 . For both the standard network and the sparse network, we used the network architecture \\([2, 50, 50, 50, 50, 50, 50, 2]\\) . Both the networks were trained by the Adam algorithm with \\(30,000\\) epochs. The initial learning rate is set to \\(0.001\\) . The training set is composed of \\(N_0:= 50\\) data points on \\(u(0, x)\\) , \\(N_b:= 50\\) sample points for enforcing the periodic boundaries, and \\(N_f:= 20,000\\) sample points inside the solution domain of equation (18) . The test set is composed of grid points \\((0, \\pi/2] \\times [-5, 5]\\) uniformly discretized with step size \\(\\pi/400\\) on the \\(t\\) direction and step size \\(10/256\\) on the \\(x\\) direction. \u95ee\u9898 (18) \u7684\u53c2\u7167\u89e3\u662f\u901a\u8fc7 Chebfun \u5305\u7684\u5085\u7acb\u53f6\u8c31\u65b9\u6cd5\u6c42\u89e3\u7684. \u5177\u4f53\u5730, \u6211\u4eec\u901a\u8fc7\u5728\u7a7a\u95f4\u79bb\u6563\u4e0a\u4f7f\u7528 \\(256\\) \u4e2a\u5085\u7acb\u53f6\u6a21\u548c\u65f6\u95f4\u79bb\u6563\u6b65\u957f\u4e3a \\(\\Delta t := (\\pi/2) \\times 10^{-6}\\) \u7684\u663e\u5f0f\u56db\u9636\u9f99\u683c\u5e93\u5854\u65b9\u6cd5. Schr\u00f6dinger \u65b9\u7a0b\u7684\u66f4\u591a\u79bb\u6563\u7ec6\u8282\u53ef\u4ee5\u53c2\u9605 PINN \u8bba\u6587. \u5bf9\u4e8e\u6807\u51c6\u7f51\u7edc\u548c\u7a00\u758f\u7f51\u7edc, \u6211\u4eec\u4f7f\u7528\u7f51\u7edc\u67b6\u6784\u4e3a \\([2, 50, 50, 50, 50, 50, 50, 2]\\) , \u90fd\u4f7f\u7528 Adam \u7b97\u6cd5\u8bad\u7ec3 \\(30,000\\) \u4e2a epochs. \u521d\u59cb\u5b66\u4e60\u7387\u8bbe\u7f6e\u4e3a \\(0.001\\) . \u8bad\u7ec3\u96c6\u7531 \\(u(0,x)\\) \u4e0a\u7684 \\(N_0=50\\) \u4e2a\u6570\u636e\u70b9, \u7528\u4e8e\u6ee1\u8db3\u5468\u671f\u8fb9\u503c\u6761\u4ef6\u7684 \\(N_b=50\\) \u6837\u672c\u70b9\u548c\u65b9\u7a0b\u5b9a\u4e49\u57df\u5185\u7684 \\(N_f=20,000\\) \u4e2a\u6837\u672c\u70b9\u7ec4\u6210. \u6d4b\u8bd5\u96c6\u5219\u7531\u5b9a\u4e49\u57df \\((0, \\pi/2] \\times [-5, 5]\\) \u6cbf \\(t\\) \u65b9\u5411\u4e0a\u5747\u5300\u79bb\u6563\u6b65\u957f\u4e3a \\(\\pi/400\\) \u548c\u6cbf \\(x\\) \u65b9\u5411\u4e0a\u5747\u5300\u79bb\u6563\u6b65\u957f\u4e3a \\(10/256\\) \u7684\u7f51\u683c\u70b9\u7ec4\u6210. Numerical results for this example are listed in Table 7 . As we can see, the sparse network has a smaller prediction error than the standard network. When regularization parameters \\(\\alpha = [0, 0, 0, 0, 5\\times10^{-7}, 10^{-6}, 10^{-5}]\\) , the relative \\(L_2\\) error is smaller than the PINN method. When regularization parameters \\(\\alpha\\) are taken as \\([9\\times10^{-7}, 5\\times10^{-7}, 6\\times10^{-7}, 7\\times10^{-7}, 8\\times10^{-7}, 10^{-6}, 10^{-5}]\\) , the sparsity of weight matrices are \\([22.0\\%,50.5\\%, 51.9\\%, 50.6\\%, 50.0\\%, 64.5\\%, 66.0\\%]\\) . In other words, after removing more than half of the neural network connections, the sparse neural network still has a slightly higher prediction accuracy. The predicted solution of the SDNN is illustrated in Figure 10 . These numerical results clearly confirm that the proposed SDNN model outperforms the standard DNN model. \u8fd9\u4e00\u4f8b\u5b50\u7684\u6570\u503c\u7ed3\u679c\u5217\u4e3e\u5728\u8868\u683c 7 \u4e2d. \u6b63\u5982\u6211\u4eec\u6240\u770b\u5230\u7684, \u7a00\u758f\u7f51\u7edc\u76f8\u6bd4\u6807\u51c6\u7f51\u7edc\u5177\u6709\u66f4\u5c0f\u7684\u9884\u6d4b\u8bef\u5dee. \u5f53\u6b63\u5219\u5316\u53c2\u6570 \\(\\alpha = [0, 0, 0, 0, 5\\times10^{-7}, 10^{-6}, 10^{-5}]\\) , \u76f8\u5bf9 \\(L_2\\) \u8bef\u5dee\u6bd4 PINN \u7684\u8981\u5c0f. \u5f53\u6b63\u5219\u5316\u53c2\u6570\u4e3a \\([9\\times10^{-7}, 5\\times10^{-7}, 6\\times10^{-7}, 7\\times10^{-7}, 8\\times10^{-7}, 10^{-6}, 10^{-5}]\\) , \u6743\u91cd\u77e9\u9635\u7684\u7a00\u758f\u6027\u4e3a \\([22.0\\%,50.5\\%, 51.9\\%, 50.6\\%, 50.0\\%, 64.5\\%, 66.0\\%]\\) . \u6362\u53e5\u8bdd\u8bf4, \u79fb\u9664\u4e86\u795e\u7ecf\u7f51\u7edc\u8d85\u8fc7\u4e00\u534a\u7684\u8fde\u63a5\u540e, \u7a00\u758f\u795e\u7ecf\u7f51\u7edc\u4ecd\u7136\u7531\u7a0d\u5fae\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6. SDNN \u7684\u9884\u6d4b\u89e3\u5982\u56fe 10 \u6240\u793a. \u8fd9\u4e9b\u6570\u503c\u7ed3\u679c\u6e05\u6670\u5730\u8bc1\u5b9e\u4e86\u6211\u4eec\u6240\u63d0\u51fa\u7684 SDNN \u6a21\u578b\u6bd4\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u8d8a. Algorithms Parameters \\(\\alpha\\) & sparsity of weight matrices Relative \\(L_2\\) error PINN No regularization \\([0.0\\%, 0.3\\%, 0.4\\%, 0.6\\%, 0.4\\%, 0.68\\%, 0.0\\%]\\) \\(1.41\\times10^{-3}\\) SDNN ( \\(\\beta = 10\\) ) \\([0, 0, 0, 0, 5\\times10^{-7}, 10^{-6}, 10^{-5}]\\) \\([0.7\\%, 0.3\\%, 0.4\\%, 50.6\\%, 38.0\\%, 74.7\\%, 77.0\\%]\\) \\(8.15\\times10^{-4}\\) SDNN ( \\(\\beta = 10\\) ) \\([9\\times10^{-7}, 5\\times10^{-7}, 6\\times10^{-7}, 7\\times10^{-7}, 8\\times10^{-7}, 10^{-6}, 10^{-5}]\\) \\([22.0\\%, 50.5\\%, 51.9\\%, 50.6\\%, 50.0\\%, 64.5\\%, 66.0\\%]\\) \\(1.38\\times10^{-3}\\) Table 7 : The Schr\u00f6dinger equation: The neural network of \\(7\\) layers with network architecture \\([2,50, 50, 50, 50, 50, 50, 2]\\) . Figure 10 : The Schr\u00f6dinger equation: Top: The training data and predicted solution \\(|u(t, x)|\\) by SDNN with network architecture \\([2, 50, 50, 50, 50, 50, 50, 2]\\) , regularization parameters \\(\\alpha := [9\\times10^{-7}, 5\\times10^{-7}, 6\\times10^{-7}, 7\\times10^{-7}, 8\\times10^{-7}, 10^{-6}, 10^{-5}]\\) , and \\(\\beta := 10\\) . Bottom: Predicted solutions at time \\(t := 0.55\\) , \\(t := 0.79\\) , and \\(t := 1.02\\) .","title":"The Schr\u00f6dinger Equation"},{"location":"Scholars/PN-2207.13266/#conclusion","text":"A sparse network requires less memory and computing time to operate it and thus it is desirable. We have developed a sparse deep neural network model by employing a sparse regularization with multiple parameters for solving nonlinear partial differential equations. Noticing that neural networks are layer-by-layer composite structures with an intrinsic multi-scale structure, we observe that the network weights of different layers have different weights of importance. Aiming at generating a sparse network structure while maintaining approximation accuracy, we proposed to impose different regularization parameters on different layers of the neural network. We first tested the proposed sparse regularization model in approximation of singular functions, and discovered that the proposed model can not only generate an adaptive approximation of functions having singularities but also have better generalization than the standard network. We then developed a sparse deep neural network model for solving nonlinear partial differential equations whose solutions may have certain singularities. Numerical examples show that the proposed model can remove redundant network connections leading to sparse networks and has better generalization ability. Theoretical investigation will be performed in a follow-up paper. \u7a00\u758f\u7f51\u7edc\u53ea\u9700\u8981\u66f4\u5c11\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u65f6\u95f4\u6765\u64cd\u4f5c\u5b83, \u56e0\u6b64\u5b83\u662f\u53ef\u53d6\u7684. \u6211\u4eec\u91c7\u7528\u591a\u53c2\u6570\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6cd5\u5efa\u7acb\u4e86\u6c42\u89e3\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b. \u6ce8\u610f\u5230\u795e\u7ecf\u7f51\u7edc\u662f\u5177\u6709\u5185\u5728\u591a\u5c3a\u5ea6\u7ed3\u6784\u7684\u5c42\u5c42\u590d\u5408\u7ed3\u6784, \u6211\u4eec\u89c2\u5bdf\u5230\u4e0d\u540c\u5c42\u7684\u7f51\u7edc\u6743\u91cd\u6709\u4e0d\u540c\u7684\u91cd\u8981\u6027\u6743\u91cd. \u4e3a\u4e86\u5728\u4fdd\u6301\u903c\u8fd1\u7cbe\u5ea6\u7684\u540c\u65f6\u751f\u6210\u7a00\u758f\u7f51\u7edc\u7ed3\u6784, \u6211\u4eec\u63d0\u51fa\u5728\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u540c\u5c42\u4e0a\u52a0\u5165\u4e0d\u540c\u7684\u6b63\u5219\u5316\u53c2\u6570. \u9996\u5148\u5bf9\u7a00\u758f\u6b63\u5219\u5316\u6a21\u578b\u5728\u5947\u6027\u51fd\u6570\u903c\u8fd1\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u6d4b\u8bd5, \u53d1\u73b0\u8be5\u6a21\u578b\u4e0d\u4ec5\u80fd\u81ea\u9002\u5e94\u903c\u8fd1\u5177\u6709\u5947\u6027\u7684\u51fd\u6570, \u800c\u4e14\u6bd4\u6807\u51c6\u7f51\u7edc\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b. \u7136\u540e, \u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u7a00\u758f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6765\u89e3\u51b3\u89e3\u6709\u4e00\u5b9a\u7684\u5947\u6027\u7684\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b. \u6570\u503c\u7b97\u4f8b\u8868\u660e, \u8be5\u6a21\u578b\u80fd\u591f\u53bb\u9664\u5197\u4f59\u7f51\u7edc\u8fde\u63a5\u5f97\u5230\u7a00\u758f\u7f51\u7edc, \u5177\u6709\u8f83\u597d\u7684\u6cdb\u5316\u80fd\u529b. \u7406\u8bba\u7814\u7a76\u5c06\u5728\u540e\u7eed\u8bba\u6587\u4e2d\u8fdb\u884c.","title":"Conclusion"},{"location":"Scholars/PN-2207.13266/#references","text":"\u672a\u88ab\u5b9e\u9645\u5f15\u7528\u7684\u6587\u732e . [3] Optimal Approximation with Sparsely Connected Deep Neural Networks, [2019] [SIAM] [15] Stochastic Subgradient Method Converges On Tame Functions. [2020]. [21] Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. [2011]. The Gap between Theory and Practice in Function Approximation with Deep Neural Networks. [2021] [SIAM]. \u21a9 Spectral and Finite Difference Solutions of the Burgers Equation [1986]. \u21a9 Multiparameter Regularization for Volterra Kernel Identification Via Multiscale Collocation Methods. [2009] Y. Xu . \u21a9 Robust Uncertainty Principles: Exact Signal Reconstruction from Highly Incomplete Frequency Information [2006]. \u21a9 An Introduction to Compressive Sampling. [2008]. \u21a9 Multi-Parameter Tikhonov Regularization for Linear Ill-Posed Operator Equations [2008] Y. Xu . \u21a9 Multiscale Methods for Fredholm Integral Equations. [2015] Y. Xu . \u21a9 Deep Learning Networks for Stock Market Analysis and Prediction: Methodology, Data Representations, and Case Studies. [2017]. \u21a9 Approximation by Superpositions of A Sigmoidal Function. [1989]. \u21a9 \u21a9 Robust Training and Initialization of Deep Neural Networks: An Adaptive Basis Viewpoint. [2020]. \u21a9 \u21a9 Context-Dependent Pretrained Deep Neural Networks for Large-Vocabulary Speech Recognition. [2012]. \u21a9 (\u5c0f\u6ce2\u5341\u8bb2) Ten Lectures on Wavelets, [1992]. \u21a9 Nonlinear Approximation and (Deep) ReLU Networks. [2021]. \u21a9 (BERT) BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding . [2018] [arXiv:1810.04805v1] \u21a9 Neural Network Approximation. [2021]. \u21a9 Deeply Learning Deep Inelastic Scattering Kinematics. [2021] Y. Xu , [arxiv:2108.11638v1] \u21a9 Compressive Sensing. [2006]. \u21a9 Chebfun Guide. [2014]. \u21a9 Hierarchical Models in the Brain, [2008]. \u21a9 Deep Learning . [2016] [MIT Press]. \u21a9 (DeepBSDE) Solving High-Dimensional Partial Differential Equations Using Deep Learning . [2018] [PNAS]. \u21a9 ReLU Deep Neural Networks and Linear Finite Elements. [2020]. \u21a9 Latin Hypercube Sampling and the Propagation of Uncertainty in Analyses of Complex Systems. [2003]. \u21a9 The Future of Deep Learning Will Be Sparse. [2021]. \u21a9 \u21a9 Sparsity in Deep Learning: Pruning and Growth for Efficient Inference and Training in Neural Networks . [2021]. \u21a9 \u21a9 Deep Learned Finite Elements, [2020]. \u21a9 Adam: A Method for Stochastic Optimization . [2015] [ICLR]. \u21a9 Imagenet Classification with Deep Convolutional Neural Networks. [2012]. \u21a9 Artificial Neural Networks for Solving Ordinary and Partial Differential Equations. [1998]. \u21a9 Neural-Network Methods for Boundary Value Problems with Irregular Boundaries. [2000]. \u21a9 Nonlinear Stability of An Undercompressive Shock for Complex Burgers Equation. [1995] \u21a9 Multi-Parameter Regularization Methods for High-Resolution Image Reconstruction with Displacement Errors. [2007] Y. Xu . \u21a9 Using the Matrix Refinement Equation for the Construction of Wavelets on Invariant Sets. [1994] Y. Xu . \u21a9 (DHM) Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations . [2018]. \u21a9 (PINN) Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations . [2019] [JCP]. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Novel Soliton Solutions of the Nonlinear Schr\u00f6dinger Equation Model. [2000]. \u21a9 A Representer Theorem for Deep Neural Networks. [2019]. \u21a9 Sparse Regularization with the \\(l_0\\) -norm . [2021] Y. Xu , arXiv:2111.08244. \u21a9 Generalized Mercer Kernels and Reproducing Kernel Banach Spaces. [2019] Y. Xu . \u21a9 Convergence of Deep ReLU Networks. [2021] Y. Xu , arXiv:2107.12530. \u21a9 \u21a9 Convergence of Deep Convolutional Neural Networks. [2021] Y. Xu , arXiv:2109.13542. \u21a9 \u21a9 Reproducing Kernel Banach Spaces for Machine Learning. [2009] Y. Xu . \u21a9 \u21a9","title":"References"},{"location":"Songs/%E6%B5%AA%E6%BC%AB%E4%B9%8B%E7%BA%A6/","text":"\u30ed\u30de\u30f3\u30b9\u306e\u7d04\u675f/\u6d6a\u6f2b\u4e4b\u7ea6 \u4f5c\u8bcd\uff1a\u5e7e\u7530\u308a\u3089 \u4f5c\u66f2\uff1a\u5e7e\u7530\u308a\u3089 \u6f14\u5531\uff1a\u5e7e\u7530\u308a\u3089 \u3053\u308c\u304b\u3089\u4e8c\u4eba(\u3075\u305f\u308a)\u904e(\u3059)\u3054\u3057\u3066\u3044\u304f\u305f\u3081\u306b \u4e3a\u4e86\u4eca\u540e\u4e24\u4e2a\u4eba\u7684\u751f\u6d3b \u7d04\u675f(\u3084\u304f\u305d\u304f)\u3057\u3066\u307b\u3057\u3044\u3053\u3068\u304c\u3042\u308b\u306e \u60f3\u8981\u548c\u4f60\u6709\u4e2a\u7ea6\u5b9a \u58f0(\u3053\u3048)\u304c\u67af(\u304b)\u308c\u3066\u540d\u524d(\u306a\u307e\u3048)\u304c\u547c(\u3088)\u3079\u306a\u304f\u306a\u308b \u76f4\u5230\u58f0\u97f3\u6c99\u54d1\u5730\u65e0\u6cd5\u547c\u5524\u5f7c\u6b64\u59d3\u540d \u305d\u306e\u65e5(\u3072)\u307e\u3067\u5fd8(\u308f\u3059)\u308c\u306a\u3044\u3067 \u90a3\u5929\u4e4b\u524d\u90fd\u4e0d\u8981\u5fd8\u8bb0 \u5149(\u3072\u304b\u308a)\u3092\u63a2(\u3055\u304c)\u3059\u3088\u3046\u306a\u7720(\u306d\u3080)\u308c\u306a\u3044\u591c(\u3088\u308b)\u306f \u4eff\u4f5b\u5728\u5bfb\u627e\u5149\u660e\u822c\u65e0\u6cd5\u5165\u7720\u7684\u591c\u665a \u671d(\u3042\u3055)\u307e\u3067\u624b(\u3066)\u3092\u63e1(\u306b\u304e)\u3063\u3066\u3044\u3066\u307b\u3057\u3044 \u5e0c\u671b\u4f60\u80fd\u4e00\u76f4\u63e1\u7740\u6211\u7684\u624b\u76f4\u5230\u6e05\u6668 \u6ca2\u5c71(\u305f\u304f\u3055\u3093)\u306e\u611b(\u3042\u3044)\u3067\u6ea2(\u3042\u3075)\u308c\u305f\u306a\u3089 \u5982\u679c\u7231\u591a\u5230\u6ee1\u6ea2\u800c\u51fa \u660e(\u3042)\u3051\u306a\u3044\u591c(\u3088\u308b)\u306e\u5922(\u3086\u3081)\u3092\u898b(\u307f)\u305b\u3066\u307b\u3057\u3044 \u5e0c\u671b\u4f60\u80fd\u8ba9\u6211\u505a\u4e00\u4e2a\u6c38\u4e0d\u7834\u6653\u7684\u7f8e\u68a6 \u5929\u79e4(\u3066\u3093\u3073\u3093)\u306f\u3044\u3064\u3082\u50be(\u304b\u305f\u3080)\u304f\u3051\u3069 \u5929\u5e73\u5c3d\u7ba1\u603b\u662f\u503e\u5411\u4e00\u7aef \u4eca\u591c(\u3053\u3093\u3084)\u3060\u3051\u306f\u540c(\u304a\u306a)\u3058\u3067\u3044\u305f\u3044 \u54ea\u6015\u53ea\u5728\u4eca\u665a\u4e5f\u60f3\u8ba9\u5b83\u8b8a\u5f97\u4e00\u6837 \u4e8c\u4eba(\u3075\u305f\u308a)\u3067\u9032(\u3059\u3059)\u307f\u59cb(\u306f\u3058)\u3081\u305f\u3053\u306e\u5217\u8eca(\u308c\u3063\u3057\u3083)\u306e \u4e24\u4e2a\u4eba\u4e00\u8d77\u5f00\u59cb\u4e58\u5750\u7684\u8fd9\u8d9f\u5217\u8f66 \u5207\u7b26(\u304d\u3063\u3077)\u306f\u6700\u5f8c(\u3055\u3044\u3054)\u307e\u3067\u5931(\u306a)\u304f\u3055\u306a\u3044\u3067\u306d \u76f4\u5230\u6700\u540e\u90fd\u8bf7\u4e0d\u8981\u5c06\u8f66\u7968\u5f04\u4e22\u54e6 \u3082\u3057\u3082\u884c(\u3086)\u304d\u5148(\u3055\u304d)\u3092\u898b(\u307f)\u5931(\u3046\u3057\u306a)\u3063\u305f\u306a\u3089 \u5982\u679c\u8ff7\u5931\u4e86\u76ee\u7684\u5730\u7684\u8bdd \u305d\u306e\u5834\u6240(\u3070\u3057\u3087)\u3067\u307e\u305f\u59cb(\u306f\u3058)\u3081\u3088\u3046 \u5c31\u5728\u90a3\u4e2a\u5730\u65b9\u91cd\u65b0\u5f00\u59cb\u5427 \u982c(\u307b\u307b)\u3092\u6fe1(\u306c)\u3089\u3059\u3088\u3046\u306a\u7720(\u306d\u3080)\u308c\u306a\u3044\u591c(\u3088\u308b)\u306f \u5728\u6cea\u6e7f\u8138\u988a\u7684\u4e0d\u7720\u4e4b\u591c \u5fc3\u5730(\u3053\u3053\u3061)\u3044\u3044\u5de6\u80a9(\u3072\u3060\u308a\u304b\u305f)\u3092\u8cb8(\u304b)\u3057\u3066\u307b\u3057\u3044 \u8bf7\u501f\u7ed9\u6211\u4f60\u90a3\u8212\u9002\u7684\u5de6\u80a9 \u6ca2\u5c71(\u305f\u304f\u3055\u3093)\u306e\u611b(\u3042\u3044)\u3092\u77e5(\u3057)\u308c\u305f\u306e\u306a\u3089 \u5982\u679c\u77e5\u9053\u4e86\u6211\u6eff\u5fc3\u7684\u7231\u610f \u53e3\u7d05(\u304f\u3061\u3079\u306b)\u3092\u6eb6(\u3068)\u304b\u3059\u3088\u3046\u306a\u30ad\u30b9(\u304d\u3059)\u3092\u3057\u3066 \u5c31\u7ed9\u6211\u6765\u4e2a\u80fd\u5c06\u53e3\u7ea2\u90fd\u878d\u5316\u7684\u543b\u5427 \u305d\u306e\u3042\u3068\u306f\u9f3b\u5148(\u306f\u306a\u3055\u304d)\u3067\u304f\u3059\u3063\u3068\u7b11(\u308f\u3089)\u3063\u3066 \u7136\u540e\u8138\u4e0a\u5e26\u7740\u7b11\u610f \u7d42(\u304a)\u308f\u308a\u306f\u306a\u3044\u3068\u8a00(\u3044)\u3063\u3066\u62b1(\u3060)\u304d\u3057\u3081\u3066 \u8bf4\u8fd8\u6ca1\u7ed3\u675f\u518d\u7d27\u62b1\u4f4f\u6211 \u541b(\u304d\u307f)\u306e\u77ed\u6240(\u305f\u3093\u3057\u3087)\u3084\u79c1(\u308f\u305f\u3057)\u306e\u9577\u6240(\u3061\u3087\u3046\u3057\u3087)\u304c\u5909(\u304b)\u308f\u3063\u3066\u3057\u307e\u3063\u3066\u3082 \u5373\u4f7f\u4f60\u7684\u7f3a\u70b9\u548c\u6211\u7684\u4f18\u70b9\u90fd\u6539\u53d8\u4e86 \u4ee3(\u304b)\u308f\u308a\u306f\u5c45(\u3044)\u306a\u3044\u3088 \u304d\u3063\u3068 \u4e5f\u6ca1\u6709\u80fd\u591f\u4ee3\u66ff\u7684\u5b58\u5728 \u601d(\u304a\u3082)\u3044\u51fa(\u3067)\u304c\u793a(\u3057\u3081)\u3059\u3088 \u307e\u305f\u624b(\u3066)\u3092\u53d6(\u3068)\u308d\u3046 \u50cf\u8bb0\u5fc6\u4e2d\u4e00\u6837\u518d\u6b21\u7275\u7740\u624b\u5427 \u661f\u5c51(\u307b\u3057\u304f\u305a)\u306e\u3088\u3046\u306a\u3053\u306e\u4e16\u754c(\u305b\u304b\u3044)\u3067 \u5728\u8fd9\u5982\u661f\u5c18\u822c\u7684\u4e16\u754c\u91cc \u7167(\u3066)\u3089\u3055\u308c\u305f\u5149(\u3072\u304b\u308a)\u306e\u5148(\u3055\u304d)\u306b\u3044\u305f\u3093\u3060 \u5728\u88ab\u7167\u8000\u7684\u5149\u8292\u4e4b\u524d \u541b(\u304d\u307f)\u306e\u307e\u307e\u305d\u306e\u307e\u307e\u304c\u7f8e(\u3046\u3064\u304f)\u3057\u3044\u304b\u3089 \u4f60\u662f\u5982\u6b64\u7684\u7f8e\u4e3d \u305d\u308c\u3067\u3044\u3044 \u305d\u308c\u3060\u3051\u3067\u3044\u3044 \u8fd9\u6837\u5c31\u597d~\u8fd9\u6837\u5c31\u8db3\u5920\u4e86 \u6ca2\u5c71(\u305f\u304f\u3055\u3093)\u306e\u611b(\u3042\u3044)\u3067\u6ea2(\u3042\u3075)\u308c\u305f\u306a\u3089 \u5982\u679c\u7231\u591a\u5230\u6ee1\u6ea2\u800c\u51fa \u660e(\u3042)\u3051\u306a\u3044\u591c(\u3088\u308b)\u306e\u5922(\u3086\u3081)\u3092\u898b(\u307f)\u305b\u3066\u307b\u3057\u3044 \u5e0c\u671b\u4f60\u80fd\u8ba9\u6211\u505a\u4e00\u4e2a\u6c38\u4e0d\u7834\u6653\u7684\u7f8e\u68a6 \u5929\u79e4(\u3066\u3093\u3073\u3093)\u306f\u304d\u3063\u3068\u307e\u305f\u50be(\u304b\u305f\u3080)\u304f\u3051\u3069 \u5c3d\u7ba1\u5929\u5e73\u8fd8\u4f1a\u518d\u5ea6\u503e\u659c \u305a\u3063\u3068\u305a\u3063\u3068\u541b(\u304d\u307f)\u3068\u4e00\u7dd2(\u3044\u3063\u3057\u3087)\u306b\u3044\u305f\u3044~ \u3044\u305f\u3044~ \u8fd8\u662f\u60f3\u8981\u6c38\u8fdc~\u6c38\u8fdc\u548c\u4f60\u5728\u4e00\u8d77","title":"\u30ed\u30de\u30f3\u30b9\u306e\u7d04\u675f/\u6d6a\u6f2b\u4e4b\u7ea6"},{"location":"Songs/%E6%B5%AA%E6%BC%AB%E4%B9%8B%E7%BA%A6/#_1","text":"\u4f5c\u8bcd\uff1a\u5e7e\u7530\u308a\u3089 \u4f5c\u66f2\uff1a\u5e7e\u7530\u308a\u3089 \u6f14\u5531\uff1a\u5e7e\u7530\u308a\u3089 \u3053\u308c\u304b\u3089\u4e8c\u4eba(\u3075\u305f\u308a)\u904e(\u3059)\u3054\u3057\u3066\u3044\u304f\u305f\u3081\u306b \u4e3a\u4e86\u4eca\u540e\u4e24\u4e2a\u4eba\u7684\u751f\u6d3b \u7d04\u675f(\u3084\u304f\u305d\u304f)\u3057\u3066\u307b\u3057\u3044\u3053\u3068\u304c\u3042\u308b\u306e \u60f3\u8981\u548c\u4f60\u6709\u4e2a\u7ea6\u5b9a \u58f0(\u3053\u3048)\u304c\u67af(\u304b)\u308c\u3066\u540d\u524d(\u306a\u307e\u3048)\u304c\u547c(\u3088)\u3079\u306a\u304f\u306a\u308b \u76f4\u5230\u58f0\u97f3\u6c99\u54d1\u5730\u65e0\u6cd5\u547c\u5524\u5f7c\u6b64\u59d3\u540d \u305d\u306e\u65e5(\u3072)\u307e\u3067\u5fd8(\u308f\u3059)\u308c\u306a\u3044\u3067 \u90a3\u5929\u4e4b\u524d\u90fd\u4e0d\u8981\u5fd8\u8bb0 \u5149(\u3072\u304b\u308a)\u3092\u63a2(\u3055\u304c)\u3059\u3088\u3046\u306a\u7720(\u306d\u3080)\u308c\u306a\u3044\u591c(\u3088\u308b)\u306f \u4eff\u4f5b\u5728\u5bfb\u627e\u5149\u660e\u822c\u65e0\u6cd5\u5165\u7720\u7684\u591c\u665a \u671d(\u3042\u3055)\u307e\u3067\u624b(\u3066)\u3092\u63e1(\u306b\u304e)\u3063\u3066\u3044\u3066\u307b\u3057\u3044 \u5e0c\u671b\u4f60\u80fd\u4e00\u76f4\u63e1\u7740\u6211\u7684\u624b\u76f4\u5230\u6e05\u6668 \u6ca2\u5c71(\u305f\u304f\u3055\u3093)\u306e\u611b(\u3042\u3044)\u3067\u6ea2(\u3042\u3075)\u308c\u305f\u306a\u3089 \u5982\u679c\u7231\u591a\u5230\u6ee1\u6ea2\u800c\u51fa \u660e(\u3042)\u3051\u306a\u3044\u591c(\u3088\u308b)\u306e\u5922(\u3086\u3081)\u3092\u898b(\u307f)\u305b\u3066\u307b\u3057\u3044 \u5e0c\u671b\u4f60\u80fd\u8ba9\u6211\u505a\u4e00\u4e2a\u6c38\u4e0d\u7834\u6653\u7684\u7f8e\u68a6 \u5929\u79e4(\u3066\u3093\u3073\u3093)\u306f\u3044\u3064\u3082\u50be(\u304b\u305f\u3080)\u304f\u3051\u3069 \u5929\u5e73\u5c3d\u7ba1\u603b\u662f\u503e\u5411\u4e00\u7aef \u4eca\u591c(\u3053\u3093\u3084)\u3060\u3051\u306f\u540c(\u304a\u306a)\u3058\u3067\u3044\u305f\u3044 \u54ea\u6015\u53ea\u5728\u4eca\u665a\u4e5f\u60f3\u8ba9\u5b83\u8b8a\u5f97\u4e00\u6837 \u4e8c\u4eba(\u3075\u305f\u308a)\u3067\u9032(\u3059\u3059)\u307f\u59cb(\u306f\u3058)\u3081\u305f\u3053\u306e\u5217\u8eca(\u308c\u3063\u3057\u3083)\u306e \u4e24\u4e2a\u4eba\u4e00\u8d77\u5f00\u59cb\u4e58\u5750\u7684\u8fd9\u8d9f\u5217\u8f66 \u5207\u7b26(\u304d\u3063\u3077)\u306f\u6700\u5f8c(\u3055\u3044\u3054)\u307e\u3067\u5931(\u306a)\u304f\u3055\u306a\u3044\u3067\u306d \u76f4\u5230\u6700\u540e\u90fd\u8bf7\u4e0d\u8981\u5c06\u8f66\u7968\u5f04\u4e22\u54e6 \u3082\u3057\u3082\u884c(\u3086)\u304d\u5148(\u3055\u304d)\u3092\u898b(\u307f)\u5931(\u3046\u3057\u306a)\u3063\u305f\u306a\u3089 \u5982\u679c\u8ff7\u5931\u4e86\u76ee\u7684\u5730\u7684\u8bdd \u305d\u306e\u5834\u6240(\u3070\u3057\u3087)\u3067\u307e\u305f\u59cb(\u306f\u3058)\u3081\u3088\u3046 \u5c31\u5728\u90a3\u4e2a\u5730\u65b9\u91cd\u65b0\u5f00\u59cb\u5427 \u982c(\u307b\u307b)\u3092\u6fe1(\u306c)\u3089\u3059\u3088\u3046\u306a\u7720(\u306d\u3080)\u308c\u306a\u3044\u591c(\u3088\u308b)\u306f \u5728\u6cea\u6e7f\u8138\u988a\u7684\u4e0d\u7720\u4e4b\u591c \u5fc3\u5730(\u3053\u3053\u3061)\u3044\u3044\u5de6\u80a9(\u3072\u3060\u308a\u304b\u305f)\u3092\u8cb8(\u304b)\u3057\u3066\u307b\u3057\u3044 \u8bf7\u501f\u7ed9\u6211\u4f60\u90a3\u8212\u9002\u7684\u5de6\u80a9 \u6ca2\u5c71(\u305f\u304f\u3055\u3093)\u306e\u611b(\u3042\u3044)\u3092\u77e5(\u3057)\u308c\u305f\u306e\u306a\u3089 \u5982\u679c\u77e5\u9053\u4e86\u6211\u6eff\u5fc3\u7684\u7231\u610f \u53e3\u7d05(\u304f\u3061\u3079\u306b)\u3092\u6eb6(\u3068)\u304b\u3059\u3088\u3046\u306a\u30ad\u30b9(\u304d\u3059)\u3092\u3057\u3066 \u5c31\u7ed9\u6211\u6765\u4e2a\u80fd\u5c06\u53e3\u7ea2\u90fd\u878d\u5316\u7684\u543b\u5427 \u305d\u306e\u3042\u3068\u306f\u9f3b\u5148(\u306f\u306a\u3055\u304d)\u3067\u304f\u3059\u3063\u3068\u7b11(\u308f\u3089)\u3063\u3066 \u7136\u540e\u8138\u4e0a\u5e26\u7740\u7b11\u610f \u7d42(\u304a)\u308f\u308a\u306f\u306a\u3044\u3068\u8a00(\u3044)\u3063\u3066\u62b1(\u3060)\u304d\u3057\u3081\u3066 \u8bf4\u8fd8\u6ca1\u7ed3\u675f\u518d\u7d27\u62b1\u4f4f\u6211 \u541b(\u304d\u307f)\u306e\u77ed\u6240(\u305f\u3093\u3057\u3087)\u3084\u79c1(\u308f\u305f\u3057)\u306e\u9577\u6240(\u3061\u3087\u3046\u3057\u3087)\u304c\u5909(\u304b)\u308f\u3063\u3066\u3057\u307e\u3063\u3066\u3082 \u5373\u4f7f\u4f60\u7684\u7f3a\u70b9\u548c\u6211\u7684\u4f18\u70b9\u90fd\u6539\u53d8\u4e86 \u4ee3(\u304b)\u308f\u308a\u306f\u5c45(\u3044)\u306a\u3044\u3088 \u304d\u3063\u3068 \u4e5f\u6ca1\u6709\u80fd\u591f\u4ee3\u66ff\u7684\u5b58\u5728 \u601d(\u304a\u3082)\u3044\u51fa(\u3067)\u304c\u793a(\u3057\u3081)\u3059\u3088 \u307e\u305f\u624b(\u3066)\u3092\u53d6(\u3068)\u308d\u3046 \u50cf\u8bb0\u5fc6\u4e2d\u4e00\u6837\u518d\u6b21\u7275\u7740\u624b\u5427 \u661f\u5c51(\u307b\u3057\u304f\u305a)\u306e\u3088\u3046\u306a\u3053\u306e\u4e16\u754c(\u305b\u304b\u3044)\u3067 \u5728\u8fd9\u5982\u661f\u5c18\u822c\u7684\u4e16\u754c\u91cc \u7167(\u3066)\u3089\u3055\u308c\u305f\u5149(\u3072\u304b\u308a)\u306e\u5148(\u3055\u304d)\u306b\u3044\u305f\u3093\u3060 \u5728\u88ab\u7167\u8000\u7684\u5149\u8292\u4e4b\u524d \u541b(\u304d\u307f)\u306e\u307e\u307e\u305d\u306e\u307e\u307e\u304c\u7f8e(\u3046\u3064\u304f)\u3057\u3044\u304b\u3089 \u4f60\u662f\u5982\u6b64\u7684\u7f8e\u4e3d \u305d\u308c\u3067\u3044\u3044 \u305d\u308c\u3060\u3051\u3067\u3044\u3044 \u8fd9\u6837\u5c31\u597d~\u8fd9\u6837\u5c31\u8db3\u5920\u4e86 \u6ca2\u5c71(\u305f\u304f\u3055\u3093)\u306e\u611b(\u3042\u3044)\u3067\u6ea2(\u3042\u3075)\u308c\u305f\u306a\u3089 \u5982\u679c\u7231\u591a\u5230\u6ee1\u6ea2\u800c\u51fa \u660e(\u3042)\u3051\u306a\u3044\u591c(\u3088\u308b)\u306e\u5922(\u3086\u3081)\u3092\u898b(\u307f)\u305b\u3066\u307b\u3057\u3044 \u5e0c\u671b\u4f60\u80fd\u8ba9\u6211\u505a\u4e00\u4e2a\u6c38\u4e0d\u7834\u6653\u7684\u7f8e\u68a6 \u5929\u79e4(\u3066\u3093\u3073\u3093)\u306f\u304d\u3063\u3068\u307e\u305f\u50be(\u304b\u305f\u3080)\u304f\u3051\u3069 \u5c3d\u7ba1\u5929\u5e73\u8fd8\u4f1a\u518d\u5ea6\u503e\u659c \u305a\u3063\u3068\u305a\u3063\u3068\u541b(\u304d\u307f)\u3068\u4e00\u7dd2(\u3044\u3063\u3057\u3087)\u306b\u3044\u305f\u3044~ \u3044\u305f\u3044~ \u8fd8\u662f\u60f3\u8981\u6c38\u8fdc~\u6c38\u8fdc\u548c\u4f60\u5728\u4e00\u8d77","title":"\u30ed\u30de\u30f3\u30b9\u306e\u7d04\u675f/\u6d6a\u6f2b\u4e4b\u7ea6"}]}